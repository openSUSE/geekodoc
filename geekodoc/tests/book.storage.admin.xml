<?xml version="1.0"?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="book_storage_admin.xml" version="5.0" xml:lang="en" xml:id="book.storage.admin">
 <info>
  <title>Administration and Deployment Guide</title><productname>SUSE Enterprise Storage</productname><productname role="abbrev">SES</productname>
  <productnumber>4</productnumber>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
  <legalnotice xml:base="copyright_suse_storage_cc.xml" version="5.0">
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Copyright ©
<?dbtimestamp format="Y"?>

  SUSE LLC
 </para>
 <para>
  Copyright © 2010-2014, Inktank Storage, Inc. and contributors.

 </para>
 <para>
  The text of and illustrations in this document are licensed by Inktank
  Storage under a Creative Commons Attribution-Share Alike 4.0
  International ("CC-BY-SA"). An explanation of CC-BY-SA is available at
  <link xlink:href="http://creativecommons.org/licenses/by-sa/4.0/legalcode"/>.
  In accordance with CC-BY-SA, if you distribute this document or an
  adaptation of it, you must provide the URL for the original version.
 </para>
 <para>
  This document is an adaption of original works found at
  <link xlink:href="http://ceph.com/docs/master/"/> (2015-01-30).
 </para>

 <para>
  Red Hat, Red Hat Enterprise Linux, the Shadowman logo, JBoss, MetaMatrix,
  Fedora, the Infinity Logo, and RHCE are trademarks of Red Hat, Inc.,
  registered in the United States and other countries. Linux® is the
  registered trademark of Linus Torvalds in the United States and other
  countries. Java® is a registered trademark of Oracle and/or its
  affiliates. XFS® is a trademark of Silicon Graphics International
  Corp. or its subsidiaries in the United States and/or other countries.
  MySQL® is a registered trademark of MySQL AB in the United States, the
  European Union and other countries. All other trademarks are the property
  of their respective owners.
 </para>
 <para>
  For SUSE or Novell trademarks, see the Novell Trademark and Service Mark
  list
  <link xlink:href="http://www.novell.com/company/legal/trademarks/tmlist.html"/>.
  Linux* is a registered trademark of Linus Torvalds. All other third party
  trademarks are the property of their respective owners. A trademark symbol
  (®, ™ etc.) denotes a SUSE or Novell trademark; an asterisk (*)
  denotes a third party trademark.
 </para>
 <para>
  All information found in this book has been compiled with utmost attention
  to detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, the authors, nor the translators shall be held liable for
  possible errors or the consequences thereof.
 </para>
</legalnotice>
 </info>
 <preface xml:base="admin_intro.xml" version="5.0" xml:id="pre.cloud.admin">
 <title>About This Guide</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  SUSE Enterprise Storage is an extension to SUSE Linux Enterprise. It combines the capabilities from the
  Ceph (<link xlink:href="http://ceph.com/"/>) storage project with the
  enterprise engineering and support of SUSE. SUSE Enterprise Storage provides IT
  organizations with the ability to deploy a distributed storage architecture
  that can support a number of use cases using commodity hardware platforms.
 </para>
 <para>
  This guide helps you understand the concept of the SUSE Enterprise Storage with the
  main focus on managing and administrating the Ceph infrastructure. It also
  demonstrates how to use Ceph with other related solutions, such as <phrase>OpenStack</phrase>
  or KVM.
 </para>
 <para>
  Many chapters in this manual contain links to additional documentation
  resources. These include additional documentation that is available on the
  system as well as documentation available on the Internet.
 </para>
 <para>
  For an overview of the documentation available for your product and the
  latest documentation updates, refer to
  <link xlink:href="http://www.suse.com/documentation"/>.
 </para>
 <sect1 xml:base="common_intro_available_doc_i.xml" version="5.0">
 <title>Available Documentation</title>

 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>

 <para>

  The following manuals are available for this product:
 </para>

 <variablelist>
  <varlistentry>
   <term><xref linkend="book.storage.admin" role="internalbook"/>
   </term>
   <listitem>
    <para>
     Guides you through Ceph installation steps and cluster management tasks,
     including description of basic Ceph cluster structure and related terminology.
     The guide also introduces steps to integrate Ceph with virtualization solutions
     such as <systemitem class="library">libvirt</systemitem>, Xen, or KVM, and ways to access objects stored in the cluster via
     iSCSI and RADOS gateways.
    </para>
    <para>
     The <emphasis>Best Practice</emphasis> chapter (see <xref linkend="cha.storage.bestpractice" role="internalbook"/>)
     includes selected practical topics sorted by categories, so that you can easily
     find a solution or more information to a specific problem.
    </para>
   </listitem>
  </varlistentry>
 </variablelist>

 <para>
  HTML versions of the product manuals can be found in the installed system
  under <filename>/usr/share/doc/manual</filename>. Additionally, you can
  access the product-specific manuals as well as upstream documentation from
  the <guimenu>Help</guimenu> links in the graphical Web interfaces. Find
  the latest documentation updates at
  <link xlink:href="http://www.suse.com/documentation"/> where you can
  download the manuals for your product in multiple formats.
 </para>
</sect1>
 <sect1 xml:base="common_intro_feedback_i.xml" version="5.0">
 <title>Feedback</title>

 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>

 <para>
  Several feedback channels are available:
 </para>

 <variablelist>
  
  <varlistentry os="sles;sled;slepos">
   <term>Bugs and Enhancement Requests</term>
   <listitem>
    <para>
     For services and support options available for your product, refer to
     <link xlink:href="http://www.suse.com/support/"/>.
    </para>
    <para>
     To report bugs for a product component, log in to the Novell Customer Center from
     <link xlink:href="http://www.suse.com/support/"/> and select
     <menuchoice> <guimenu>My Support</guimenu> <guimenu>Service
     Request</guimenu> </menuchoice>.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>User Comments</term>
   <listitem>
    <para>
     We want to hear your comments about and suggestions for this manual and
     the other documentation included with this product. Use the User
     Comments feature at the bottom of each page in the online documentation
     or go to
     <link xlink:href="http://www.suse.com/documentation/feedback.html"/>
     and enter your comments there.

    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>Mail</term>
   <listitem>
    <para>
     For feedback on the documentation of this product, you can also send a
     mail to <literal>doc-team@suse.de</literal>. Make sure to include the
     document title, the product version, and the publication date of the
     documentation. To report errors or suggest enhancements, provide a
     concise description of the problem and refer to the respective section
     number and page (or URL).
    </para>
   </listitem>
  </varlistentry>
 </variablelist>
</sect1>
 <sect1 xml:base="common_intro_typografie_i.xml" version="5.0">


 <title>Documentation Conventions</title>

 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>

 <para>
  The following typographical conventions are used in this manual:
 </para>

 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <filename>/etc/passwd</filename>: directory names and file names
   </para>
  </listitem>
  <listitem>
   <para>
    <replaceable>placeholder</replaceable>: replace
    <replaceable>placeholder</replaceable> with the actual value
   </para>
  </listitem>
  <listitem>
   <para>
    <envar>PATH</envar>: the environment variable PATH
   </para>
  </listitem>
  <listitem>
   <para>
    <command>ls</command>, <option>--help</option>: commands, options, and
    parameters
   </para>
  </listitem>
  <listitem>
   <para>
    <systemitem class="username">user</systemitem>: users or groups
   </para>
  </listitem>
  <listitem>
   <para>
    <keycap function="alt"/>, <keycombo> <keycap function="alt"/>
    <keycap>F1</keycap> </keycombo>: a key to press or a key combination;
    keys are shown in uppercase as on a keyboard
   </para>
  </listitem>
  <listitem>
   <para>
    <guimenu>File</guimenu>, <menuchoice> <guimenu>File</guimenu>
    <guimenu>Save As</guimenu> </menuchoice>: menu items, buttons
   </para>
  </listitem>

  <listitem>
   <para>
    <emphasis>Dancing Penguins</emphasis> (Chapter
    <emphasis>Penguins</emphasis>, ↑Another Manual): This is a
    reference to a chapter in another manual.
   </para>
  </listitem>
 </itemizedlist>
</sect1>
 <sect1 xml:base="common_intro_making_i.xml" version="5.0">
 <title>About the Making of This Manual</title>

 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>

 <para>
  This book is written in Novdoc, a subset of DocBook (see
  <link xlink:href="http://www.docbook.org"/>). The XML source files were
  validated by <command>xmllint</command>, processed by
  <command>xsltproc</command>, and converted into XSL-FO using a customized
  version of Norman Walsh's stylesheets. The final PDF can be formatted
  through FOP from Apache or through XEP from RenderX. The authoring and
  publishing tools used to produce this manual are available in the package
  <systemitem class="resource">daps</systemitem>. The DocBook Authoring and
  Publishing Suite (DAPS) is developed as open source software. For more
  information, see <link xlink:href="http://daps.sf.net/"/>.
 </para>
</sect1>
</preface>
 <part xml:id="part.ses">
  <title>SUSE Enterprise Storage</title>
  <chapter xml:base="admin_about.xml" version="5.0" xml:id="cha.storage.about">
 <title>About SUSE Enterprise Storage</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="storage.intro">
  <title>Introduction</title>

  <para>
   SUSE Enterprise Storage is a distributed storage designed for scalability, reliability and
   performance based on the Ceph technology. As opposed to conventional
   systems which have allocation tables to store and fetch data, Ceph uses a
   pseudo-random data distribution function to store data, which reduces the
   number of look-ups required in storage. Data is stored on intelligent object
   storage devices (OSDs) by using daemons, which automates data management
   tasks such as data distribution, data replication, failure detection and
   recovery. Ceph is both self-healing and self-managing which results in
   reduction of administrative and budget overhead.
  </para>

  <para>
   The Ceph storage cluster uses two mandatory types of nodes—monitors
   and OSD daemons:
  </para>

  <variablelist>
   <varlistentry>
    <term>Monitor</term>
    <listitem>
     <para>
      Monitoring nodes maintain information about cluster health state, a map
      of other monitoring nodes and a CRUSH map.
     </para>
     <para>
      Monitor nodes also keep history of changes performed to the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD Daemon</term>
    <listitem>
     <para>
      An OSD daemon stores data and manages the data replication and
      rebalancing processes. Each OSD daemon handles one or more OSDs, which
      can be physical disks/partitions or logical volumes.
     </para>
     <para>
      OSD daemons also communicate with monitor nodes and provide them with the
      state of the other OSD daemons.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The Ceph storage cluster can use the following optional node types:
  </para>

  <variablelist>
   <varlistentry>
    <term>Metadata Server (MDS)</term>
    <listitem>
     <para>
      The metadata servers store metadata for the Ceph file system. By using
      MDS you can execute basic file system commands such as
      <command>ls</command> without overloading the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><phrase>RADOS Gateway</phrase></term>
    <listitem>
     <para>
      <phrase>RADOS Gateway</phrase> is an HTTP REST gateway for the RADOS object store. You can use
      this node type also when using the Ceph file system.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <note>
   <title>Each Node Type on a Separate Server</title>
   <para>
    We strongly recommend to install only one node type on a single server.
   </para>
  </note>

  <para>
   The Ceph environment has the following features:
  </para>

  <variablelist>
   <varlistentry>
    <term>Controlled, Scalable, Decentralized Placement of replicated Data using CRUSH</term>
    <listitem>
     <para>
      The Ceph system uses a unique map called CRUSH (Controlled Replication
      Under Scalable Hashing) to assign data to OSDs in an efficient manner.
      Data assignment offsets are generated as opposed to being looked up in
      tables. This does away with disk look-ups which come with conventional
      allocation table based systems, reducing the communication between the
      storage and the client. The client armed with the CRUSH map and the
      metadata such as object name and byte offset knows where it can find the
      data or which OSD it needs to place the data.
     </para>
     <para>
      CRUSH maintains a hierarchy of devices and the replica placement policy.
      As new devices are added, data from existing nodes is moved to the new
      device to improve distribution with regard to workload and resilience. As
      a part of the replica placement policy, it can add weights to the devices
      so some devices are more favored as opposed to others. This could be used
      to give more weights to Solid State Devices (SSDs) and lower weights to
      conventional rotational hard disks to get overall better performance.
     </para>
     <para>
      CRUSH is designed to optimally distribute data to make use of available
      devices efficiently. CRUSH supports different ways of data distribution
      such as the following:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        n-way replication (mirroring)
       </para>
      </listitem>
      <listitem>
       <para>
        RAID parity schemes
       </para>
      </listitem>
      <listitem>
       <para>
        Erasure Coding
       </para>
      </listitem>
      <listitem>
       <para>
        Hybrid approaches such as RAID-10
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Reliable Autonomic Distributed Object Storage (RADOS)</term>
    <listitem>
     <para>
      The intelligence in the OSD Daemons allows tasks such as data replication
      and migration for self-management and self-healing automatically. By
      default, data written to Ceph storage is replicated within the OSDs.
      The level and type of replication is configurable. In case of failures,
      the CRUSH map is updated and data is written to new (replicated) OSDs.
     </para>
     <para>
      The intelligence of OSD Daemons enables to handle data replication, data
      migration, failure detection and recovery. These tasks are automatically
      and autonomously managed. This also allows the creation of various pools
      for different sorts of I/O.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Replicated Monitor Servers</term>
    <listitem>
     <para>
      The monitor servers keep track of all the devices in the system. They
      manage the CRUSH map which is used to determine where the data needs to
      be placed. In case of failures of any of the OSDs, the CRUSH map is
      re-generated and re-distributed to the rest of the system. At a given
      time, it is recommended that a system contains multiple monitor servers
      to add redundancy and improve resilience.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
   	<term>
   	Configuration and managment framework for your cluster - DeepSea  
   	</term>
   	<listitem>
   		<para>
   		DeepSea is a collection of Salt states, runners and modules for deploying and managing Ceph.
   		</para>
   	</listitem>
   </varlistentry>
  </variablelist>

  <para>
   Currently the Ceph storage cluster can provide the following services:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>Ceph object storage</emphasis>
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Ceph file system</emphasis>
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>RADOS block device</emphasis>
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="storage.moreinfo">
  <title>Additional Information</title>

  <para>
   Ceph as a community project has its own extensive online documentation.
   For topics not found in this manual refer to
   <link xlink:href="http://ceph.com/docs/master/"/>.
  </para>
 </sect1>
</chapter>
  <chapter xml:base="admin_sysreq.xml" version="5.0" xml:id="cha.ceph.sysreq">
 <title>System Requirements</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sysreq.osd">
  <title>Minimal Recommendations per Storage Node</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     2 GB of RAM per each Terabyte of OSD (2‰ of total raw capacity).
    </para>
   </listitem>
   <listitem>
    <para>
     1.5 GHz of a CPU core per OSD.
    </para>
   </listitem>
   <listitem>
    <para>
     Bonded or redundant 10GbE networks.
    </para>
   </listitem>
   <listitem>
    <para>
     OSD disks in JBOD configurations.
    </para>
   </listitem>
   <listitem>
    <para>
     OSD disks should be exclusively used by SUSE Enterprise Storage.
    </para>
   </listitem>
   <listitem>
    <para>
     Dedicated disk/SSD for the operating system, preferably in a RAID1
     configuration.
    </para>
   </listitem>
   <listitem>
    <para>
     Additional 4 GB of RAM if cache tiering is used.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq.mon">
  <title>Minimal Recommendations per Monitor Node</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     3 SUSE Enterprise Storage monitor nodes recommended.
    </para>
   </listitem>
   <listitem>
    <para>
     2 GB of RAM per monitor.
    </para>
   </listitem>
   <listitem>
    <para>
     SSD or fast hard disk in a RAID1 configuration
    </para>
   </listitem>
   <listitem>
    <para>
     On installations with fewer than seven nodes, these can be hosted on the
     system disk of the OSD nodes.
    </para>
   </listitem>
   <listitem>
    <para>
     Nodes should be bare metal, not virtualized, for performance reasons.
    </para>
   </listitem>
   <listitem>
    <para>
     Mixing OSD, monitor, or <phrase>RADOS Gateway</phrase> nodes with the actual workload is not
     supported. No other load generating services other than OSDs, monitors or
     <phrase>RADOS Gateway</phrase>s daemons are supported on the same host.
    </para>
   </listitem>
   <listitem>
    <para>
     Configurations may vary from, and frequently exceed, these recommendations
     depending on individual sizing and performance needs.
    </para>
   </listitem>
   <listitem>
    <para>
     Bonded network interfaces for redundancy.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq.rgw">
  <title>Minimal Recommendations for <phrase>RADOS Gateway</phrase> Nodes</title>

  <para>
   <phrase>RADOS Gateway</phrase> nodes should have 6-8 CPU cores and 32 GB of RAM (64 GB recommended).
  </para>
 </sect1>
 <sect1 xml:id="sysreq.iscsi">
  <title>Minimal Recommendations for iSCSI Nodes</title>

  <para>
   iSCSI nodes should have 6-8 CPU cores and 16 GB of RAM.
  </para>
 </sect1>
 <sect1 xml:id="sysreq.naming">
  <title>Naming Limitations</title>

  <para>
   Ceph does not generally support non-ASCII characters in configuration
   files, pool names, user names and so forth. When configuring a Ceph
   cluster we recommend using only simple alphanumeric characters (A-Z, a-z,
   0-9) and minimal punctuation ('.', '-', '_') in all Ceph
   object/configuration names.
  </para>
 </sect1>
</chapter>
 </part>
 <part xml:id="part.deploy">
  <title>Cluster Deployment and Upgrade</title>
  <chapter xml:base="admin_ceph_installation.xml" version="5.0" xml:id="cha.ceph.install">
 <title>Introduction</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter outlines procedures to deploy the Ceph cluster. Currently we
  support the following methods of deployment:
 </para>
 <itemizedlist>
 <listitem>
 	<para>
 	<xref linkend="ceph.install.saltstack" role="internalbook"/>
 	</para>
 </listitem>
  <listitem>
   <para>
    <xref linkend="ceph.install.ceph-deploy" role="internalbook"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="ceph.install.crowbar" role="internalbook"/>
   </para>
  </listitem>
 </itemizedlist>
 <important>
  <title>Do Not Mix Installation Methods</title>
  <para>
   You cannot mix the supported installation methods. For example if you decide
   to deploy the cluster with Crowbar, you cannot later make changes to its
   settings with <command>ceph-deploy</command> and vice versa.
  </para>
 </important>          
  			
</chapter>
  <chapter xml:base="admin_install_salt.xml" version="5.0" xml:id="ceph.install.saltstack">
 <title>Deploying with DeepSea and Salt</title>
 <para>
  Salt along with DeepSea is a <emphasis>stack</emphasis> of components
  that help you deploy and manage server infrastructure. It is very scalable,
  fast, and relatively easy to get running. Read the following considerations
  before you start deploying the cluster with Salt:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis>Salt master</emphasis> is the host that controls the whole cluster
    deployment. Dedicate all the host resources to the Salt master services. Do
    not install Ceph on the host where you want to run Salt master.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Salt minions</emphasis> are the nodes controlled by Salt master. In
    the Ceph environment, Salt minion is typically an OSD or monitor.
   </para>
  </listitem>
  <listitem>
   <para>
    Salt minions need to correctly resolve the Salt master's host name over the
    network. By default, they look for the <systemitem>salt</systemitem> host
    name. Therefore, we recommend to set the Salt master's host name to
    <systemitem>salt</systemitem>.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="deepsea.description">
  <title>Introduction to DeepSea</title>

  <para>
   The goal of DeepSea is to save the administrator time and confidently
   perform complex operations on a Ceph cluster. This idea has driven a few
   choices. Before presenting those choices, some observations are necessary.
  </para>

  <para>
   All software has configuration. Sometimes the default is sufficient. This is
   not the case with Ceph. Ceph is flexible almost to a fault. Reducing
   this complexity would force administrators into preconceived configurations.
   Several of the existing Ceph solutions for an installation create a
   demonstration cluster of three nodes. However, the most interesting features
   of Ceph require more.
  </para>

  <para>
   One aspect of configuration management tools is accessing the data such as
   addresses and device names of the individual servers. For a distributed
   storage system such as Ceph, that aggregate can run into the hundreds.
   Collecting the information and entering the data manually into a
   configuration management tool is prohibitive and error prone.
  </para>

  <para>
   The steps necessary to provision the servers, collect the configuration,
   configure and deploy Ceph are mostly the same. However, this does not
   address managing the separate functions. For day to day operations, the
   ability to trivially add hardware to a given function and remove it
   gracefully is a requirement.
  </para>

    <para>
     With these observations in mind, DeepSea addresses them with the following
   strategy. DeepSea Consolidates the administrators decisions in a single location. The
     decisions revolve around cluster assignment, role assignment and profile
     assignment. And DeepSea collects each set of tasks into a simple goal. Each goal is a Stage:
    </para>
    
     <itemizedlist>
      <listitem>
       <para>
        <emphasis role="bold">Stage 0</emphasis>—the
        <emphasis role="bold">provisioning</emphasis>— this stage is
        optional as many sites provides their own provisioning of servers. If
        you do not have your provisioning tool, you should run this stage.
        During this stage all required updates are applied and your system may
        be rebooted.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 1</emphasis>—the
        <emphasis role="bold">discovery</emphasis>— here you detect all
        hardware in your cluster and collect necessary information for the
        Ceph configuration. For details about configuration refer to
        <xref linkend="deepsea.pillar.salt.configuration" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 2</emphasis>—the
        <emphasis role="bold">configuration</emphasis>— you need to
        prepare configuration data in a particular format.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 3</emphasis>—the
        <emphasis role="bold">deployment</emphasis>— creates a basic
        Ceph cluster with OSD and monitors.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 4</emphasis>—the
        <emphasis role="bold">services</emphasis>— additional features of
        Ceph like iSCSI, RadosGW and CephFS can be installed in this stage.
        Each is optional.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 5</emphasis>—the removal stage. This
        stage is not mandatory and during the initial setup it is usually not
        needed. In this stage the roles of minions and also the cluster
        configuration are removed. This stage is usually need when you need to
        remove a storage node from your cluster, for details refer to
        <xref linkend="salt.node.removing" role="internalbook"/>.
       </para>
      </listitem>
     </itemizedlist>
  
   

  <sect2 xml:id="deepsea.organisation.locations">
   <title>Organization and Important Locations</title>
   <para>
    Salt has several standard locations and several naming conventions used
    on your master node:
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>/srv/pillar</filename>
     </term>
     <listitem>
      <para>
       The directory stores configuration data for your cluster minions.
       <emphasis>Pillar</emphasis> is an interface for providing global
       configuration values to all your cluster minions.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/</filename>
     </term>
     <listitem>
      <para>
       The directory stores Salt state files (also called
       <emphasis>sls</emphasis> files). State files are formatted description
       of states in which the cluster should be. For more refer to the
       <link xlink:href="https://docs.saltstack.com/en/latest/topics/tutorials/starting_states.html">Salt
       documentation</link>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/module/runners</filename>
     </term>
     <listitem>
      <para>
       The directory stores python scripts known as runners. Runners are
       executed on the master node.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/_modules</filename>
     </term>
     <listitem>
      <para>
       The directory stores python scripts that are called modules. The modules
       are applied to all minions in your cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/pillar/ceph</filename>
     </term>
     <listitem>
      <para>
       The directory is used by DeepSea. Collected configuration data are
       stored there.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/ceph</filename>
     </term>
     <listitem>
      <para>
       Is a directory used by DeepSea. The directory stores aggregated
       configuration data that are ready to use by various salt commands. The
       directory stores sls files that can be in different format, but each
       subdirectory contains sls files in the same format. For example,
       <filename>/srv/salt/ceph/stage</filename> contains orchestration files
       that are executed by the <command>salt-run state.orchestrate</command>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.install.stack">
  <title>Deploying with DeepSea and Salt</title>

  <para>
   The cluster deployment process by using Salt has several phases. First,
   you need to prepare all nodes of the cluster and then you deploy and
   configure Ceph.
  </para>

  <para>
   The following procedure describes the cluster preparation in detail.
  </para>

  <procedure>
   <step>
    <para>
     Install and register SUSE Linux Enterprise Server 12 SP2 together with SUSE Enterprise Storage 4 extension on each
     node of the cluster.
    </para>
   </step>
   <step>
    <para>
     Install DeepSea on the node which you will used as the Salt master:
    </para>
<screen><prompt>root@master &gt; </prompt>zypper in deepsea</screen>
    <para>
     The command installs also the <literal>salt-master</literal> package.
    </para>
   </step>
   <step>
    <para>
     Install the package <literal>salt-minion</literal> on all cluster nodes
     including the Salt master node.
    </para>
<screen><prompt>root # </prompt>zypper in salt-minion</screen>
   </step>
   <step>
   <para>
   Configure all minions  to connect to the master. If your Salt master is not reachable by the DNS name <literal>salt</literal>, edit the file <filename>/etc/salt/minion</filename> or create a new file <filename>/etc/salt/minion.d/master.conf</filename> with the following:
   </para>
   <screen>master:<replaceable>&lt;DNS name of you Salt master&gt;</replaceable></screen>
   </step>
   <step>
   <para>
   If you performed changes to files mentioned in the previous step, restart the Salt service on all minions:
   </para>
   <screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Check that the Salt State file:
     <filename>/srv/pillar/ceph/master_minion.sls</filename> points to your
     Salt master. If you used the default host name for your Salt master -
     <emphasis>salt</emphasis> in the <emphasis>ses</emphasis> domain, then the
     file looks as follows:
    </para>
<screen>master_minion: salt.ses</screen>
   </step>
   <step>
   <para>
   Restart the Salt service on the master node:
   </para>
   <screen><prompt>root@master &gt; </prompt>systemctl restart salt-master.service</screen>
   </step>
   <step>
    <para>
     Accept all salt keys on the Salt master:
    </para>
<screen><prompt>root@master &gt; </prompt>salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verify that the keys have been accepted:
    </para>
<screen><prompt>root@master &gt; </prompt>salt-key --list-all</screen>
   </step>
   
   <step>
    <para>
     Ensure that you have access to Ceph Jewel repositories.
    </para>
   </step>
  </procedure>

  <para>
   Now you deploy and configure Ceph
  </para>

  <note>
   <title>Salt Command Conventions</title>
   <para>
    There are two possibilities how to run <command>salt-run
    state.orch</command> - one is with <literal>stage.&lt;stage
    number&gt;</literal>, the other is with a name of the stage. Both notations
    have the same impact and it is fully up to your preferences which command
    will you use.
   </para>
  </note>

  <para>
   Unless specified otherwise, all steps are mandatory.
  </para>

  <procedure>
   <step>
    <para>
     Now optionally provision your cluster. You cam omit this step if you have
     your own provisioning server.
    </para>
<screen><prompt>root@master &gt; </prompt>salt-run state.orch ceph.stage.0</screen>
    <para>
     or
    </para>
<screen><prompt>root@master &gt; </prompt>salt-run state.orch ceph.stage.prep</screen>
   </step>
   <step>
    <para>
     The discovery stage collects data from all minions and creates
     configuration fragments that are stored in the directory
     <filename>/srv/pillar/ceph/proposals</filename>. The data are stored in
     the YAML format in sls or yml files.
    </para>
<screen><prompt>root@master &gt; </prompt>salt-run state.orch ceph.stage.1</screen>
    <para>
     or
    </para>
<screen><prompt>root@master &gt; </prompt>salt-run state.orch ceph.stage.discovery</screen>
   </step>
   <step>
    <para>
     After the previous command finishes successfully, create a
     <filename>policy.cfg</filename> file in
     <filename>/srv/pillar/ceph/proposals</filename>. For details refer to
     <xref linkend="policy.configuration" role="internalbook"/>.
    </para>
   </step>
   <step>
    <para>
     The configuration stage parses the <filename>policy.cfg</filename> file
     and merges the included files into their final form. Cluster and role
     related contents are placed in
     <filename>/srv/pillar/ceph/cluster</filename>, while Ceph specific
     content is placed in <filename>/srv/pillar/ceph/stack/default</filename>.
    </para>
    <para>
     Run the following command <emphasis role="bold">twice</emphasis> to
     trigger the configuration stage:
    </para>
<screen><prompt>root@master &gt; </prompt>salt-run state.orch ceph.stage.2</screen>
    <para>
     or
    </para>
<screen><prompt>root@master &gt; </prompt>salt-run state.orch ceph.stage.configure</screen>
    <para>
     The configuration step may take several seconds. After the command
     finishes, you can view the pillar data for all minions by running:
    </para>
<screen><prompt>root@master &gt; </prompt>salt '*' pillar.items</screen>
    <note>
     <title>Overwriting Defaults</title>
     <para>
      As soon as the command finishes, you can view the default configuration
      and change it to suit your needs. For details refer to
      <xref linkend="custom.configuration" role="internalbook"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     Now you run the deploy stage. In this stage the pillar is validated and
     monitors and ODS daemons are created on the storage nodes. Run the
     following to start the stage:
    </para>
<screen><prompt>root@master &gt; </prompt>salt-run state.orch ceph.stage.3</screen>
    <para>
     or
    </para>
<screen><prompt>root@master &gt; </prompt>salt-run state.orch ceph.stage.deploy
  		</screen>
    <para>
     The command may take several minutes. If it fails, you have to fix the
     issue (rerunning of previous stages may be required). After the command
     succeeds, run the following to check the status:
    </para>
<screen>ceph -s</screen>
   </step>
   <step>
    <para>
     The last step of the Ceph cluster deployment is the
     <emphasis>services</emphasis> stage. Here you instantiate any of the
     currently supported services: iSCSI, CephFS, <phrase>RADOS Gateway</phrase> and openATTIC. In this
     stage necessary pool, authorizing keyrings and starting services are
     created. To start the stage, run the following:
    </para>
<screen><prompt>root@master &gt; </prompt>salt-run state.orch ceph.stage.4</screen>
    <para>
     or
    </para>
<screen><prompt>root@master &gt; </prompt>salt-run state.orch ceph.stage.services</screen>
    <para>
     Depending on the setup, the command may run several minutes.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deepsea.pillar.salt.configuration">
  <title>Configuration</title>

  <sect2 xml:id="policy.configuration">
   <title>The <filename>policy.cfg</filename> File</title>
   <para>
    The <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>
    configuration file is used to determine functions of individual cluster
    nodes (which node acts as OSD, which is a monitoring node, etc.). The file
    then includes configuration for individual nodes.
   </para>
   <para>
    Currently the only way how to configure policy is by manually editing the
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> configuration
    file. The file is divided into four sections:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <xref linkend="policy.cluster.assignment" xrefstyle="select: title" role="internalbook"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="policy.role.assignment" xrefstyle="select: title" role="internalbook"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="policy.common.configuration" xrefstyle="select: title" role="internalbook"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="policy.profile.assignment" xrefstyle="select: title" role="internalbook"/>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The order of the sections is arbitrary, but the content of included lines
    overwrites matching keys from the contents of previous lines.
   </para>
   <sect3 xml:id="policy.cluster.assignment">
    <title>Cluster Assignment</title>
    <para>
     In the <emphasis role="bold">cluster</emphasis> section you select minions
     for your cluster. You can select all minions, or you can blacklist or
     whitelist minions. Examples for a cluster called
     <emphasis role="bold">ceph</emphasis> follows.
    </para>
    <para>
     To include <emphasis role="bold">all</emphasis> minions, add the following
     lines:
    </para>
<screen>cluster-ceph/cluster/*.sls</screen>
    <para>
     To <emphasis role="bold">whilelist</emphasis> a particular minion:
    </para>
<screen>cluster-ceph/cluster/abc.domain.sls
  			</screen>
    <para>
     or a group of minions—you can shell glob matching:
    </para>
<screen>cluster-ceph/cluster/mon*.sls</screen>
    <para>
     To <emphasis role="bold">blacklist</emphasis> a minion/s, set the minion/s
     to <literal>unassigned</literal>:
    </para>
<screen>cluster-unassigned/cluster/client*.sls</screen>
   </sect3>
   <sect3 xml:id="policy.role.assignment">
    <title>Role Assignment</title>
    <para>
     In the <emphasis role="bold"/> section you need to assign roles
     to your cluster nodes. The general pattern is the following:
     </para>
     <screen>role-<replaceable>&lt;role name&gt;</replaceable>/<replaceable>&lt;path&gt;</replaceable>/<replaceable>&lt;included files&gt;</replaceable></screen>
     <para>
      Where the items have the following meaning and values:
      </para>
      <itemizedlist>
      <listitem>
      	<para>
      	<replaceable>&lt;role name&gt;</replaceable>
     is any of the following: <emphasis>master, admin, mon, mds, igw</emphasis>
     or <emphasis>rgw</emphasis>. Detailed description see below.
      	</para>
      </listitem>
      <listitem>
      	<para>
      	<replaceable>&lt;path&gt;</replaceable> is a relative path to sls or yml files. Usually in case of sls files it is <filename>cluster</filename>, while yml files are located at <filename>stack/default/ceph/minions</filename>.
      	</para>
      </listitem>
      <listitem>
      	<para>
      	<replaceable>&lt;included files&gt;</replaceable> are the Salt state files or YAML configuration files. Shell globing can be used for more specific matching.
      	</para>
      	
      </listitem>
      </itemizedlist>
      
    <para>
    An example for each role follows:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>master</emphasis> - the node has admin keyrings to all Ceph
       clusters. Currently, only a single Ceph cluster is supported. The <emphasis>master</emphasis> role is mandatory, always add a similar line like the following: 
      </para>
      <screen>role-master/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>admin</emphasis> - the minion will have an admin keyring. You define the role as follows:
      </para>
      <screen>role-admin/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>mon</emphasis> - the minion will provide the monitoring
       service to the Ceph cluster. This role requires addresses of the
       assigned minions, thus you need to include the files from the
       <filename>stack</filename> directory on top of the sls files:
      </para>
<screen>role-mon/stack/default/ceph/minions/mon*.yml
role-mon/cluster/*.sls</screen>
<para>
The example assigns the monitoring role to a group of minions.
</para>
     </listitem>
     <listitem>
      <para>
       <emphasis>mds</emphasis> - the minion will provide the metadata service to
       support CephFS.
      </para>
      <screen>role-mds/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>igw</emphasis> - the minion will act as a iSCSI gateway. This
       role requires addresses of the assigned minions, thus you need to
       also include the files from the <filename>stack</filename> directory:
      </para>
<screen>role-igw/stack/default/ceph/minions/xyz.domain.yml
role-igw/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>rgw</emphasis> - the minion will act as a <phrase>RADOS Gateway</phrase>:
      </para>
      <screen>role-rgw/cluster/*.sls</screen>
     </listitem>
    </itemizedlist>
    
    <note>
     <title>Multiple Roles of Cluster Nodes</title>
     <para>
      You can assign several roles to a single node. For instance, you can
      assign to two monitor nodes also the mds role:
     </para>
<screen>role-mds/cluster/mon[12]*.sls</screen>
    </note>
   </sect3>
   <sect3 xml:id="policy.common.configuration">
    <title>Common Configuration</title>
    <para>
     The common configuration section includes configuration files generated
     during the <emphasis>discovery (stage 1)</emphasis>. These configuration
     files store parameters like <literal>fsid</literal> or
     <literal>public_network</literal>. To include the required Ceph common
     configuration, add the following lines:
    </para>
<screen>config/stack/default/global.yml
config/stack/default/ceph/cluster.yml</screen>

   </sect3>
   <sect3 xml:id="policy.profile.assignment">
    <title>Profile Assignment</title>
    <para>
     In Ceph, a single storage role would be insufficient to describe the
     many disk configurations available with the same hardware. Therefore,
     stage 1 will generate multiple profiles when possible for the same storage
     node. The administrator adds the cluster and stack related lines similar
     to the mon and igw roles.
    </para>
    <para>
     The profile names begin with <emphasis>profile</emphasis> and end with a
     single digit. The format is the following:
    </para>
<screen>profile-<replaceable>&lt;label&gt;</replaceable>-<replaceable>&lt;single digit&gt;</replaceable><replaceable>&lt;path to sls or yml files&gt;</replaceable></screen>
    <para>
     where the items have the following meaning and values:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>&lt;label&gt;</replaceable> is dynamically generated based on quantity,
       model and size of the media, e.g. 2Disk2GB.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>&lt;single digit&gt;</replaceable> - defines the type of profile and
       reflects the count of medias attached to the minion. When
       <literal>1</literal> is specified, the media is treated like an
       individual OSD. When you specify <literal>2</literal> the node is with
       solid state media drive (SSD or NVMe) and the solid state media is
       considered as separate journals. Depending on the number of models and
       ratio of drives, additional profiles may be created by incrementing the
       digit.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>&lt;path to sls or yml files&gt;</replaceable> - replace it with a proper
       path to cluster sls files or to stack yml configuration files.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Now check the content of yml files in the
     <filename>stack/default/ceph/minions</filename> for the specific
     configuration. Then configure the profiles according to the following
     examples:
    </para>
    <para>
     A minion with a single disk called <emphasis>3HP5588GB</emphasis>:
    </para>
<screen>profile-3HP5588-1/cluster/*.sls
profile-3HP5588-1/stack/default/ceph/minions/*.yml</screen>
    <para>
     A minion with two disks <emphasis>2Intel745GB</emphasis> and
     <emphasis>6INTEL372GB</emphasis>.
    </para>
<screen>profile-2Intel745GB-6INTEL372GB-2/cluster/*.sls
profile-2Intel745GB-6INTEL372GB-2/stack/default/ceph/minions/*.yml</screen>
    <para>
     You can add as many lines as needed to define each a profile for each
     storage node:
    </para>
<screen>profile-24HP5588-1/cluster/cold*.sls
profile-24HP5588-1/stack/default/ceph/minions/cold*.yml
profile-18HP5588-6INTEL372GB-2/cluster/data*.sls
profile-18HP5588-6INTEL372GB-2/stack/default/ceph/minions/data*.yml</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="custom.configuration">
   <title>Customizing the Default Configuration</title>
   <para>
    You can change the default configuration generated in the stage 2. The
    pillar is updated after the stage 2, thus you can view the current settings
    by running:
   </para>
<screen><prompt>root@master &gt; </prompt>salt '*' pillar.items</screen>
   <para>
    The output of default configuration for a single minion is usually similar
    to the following:
   </para>
<screen>----------
    available_roles:
        - admin
        - mon
        - storage
        - mds
        - igw
        - rgw
        - client-cephfs
        - client-radosgw
        - client-iscsi
        - mds-nfs
        - rgw-nfs
        - master
    cluster:
        ceph
    cluster_network:
        172.16.22.0/24
    fsid:
        e08ec63c-8268-3f04-bcdb-614921e94342
    master_minion:
        admin.ceph
    mon_host:
        - 172.16.21.13
        - 172.16.21.11
        - 172.16.21.12
    mon_initial_members:
        - mon3
        - mon1
        - mon2
    public_address:
        172.16.21.11
    public_network:
        172.16.21.0/24
    roles:
        - admin
        - mon
        - mds
    time_server:
        admin.ceph
    time_service:
        ntp</screen>
   <para>
    The above mentioned settings are distributed into several configuration
    files. The directory structure with these files is defined in the
    <filename>/srv/pillar/ceph/stack/stack.cfg</filename> directory. The
    following files usually describe your cluster:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <filename>/srv/pillar/ceph/stack/global.yml</filename> - the file affects
      all minions in the Salt cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>/cluster.yml</filename>
      - the file affects all minions in the Ceph cluster called
      <literal>ceph</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>/roles/<replaceable>role</replaceable>.yml</filename>
      - affects all minions that are assigned the specific role in the
      <literal>ceph</literal> cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>minions/<replaceable>minion
      ID</replaceable>/yml</filename> - affects the individual minion.
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <title>Overwriting Directories with Default Values</title>
    <para>
     There is a parallel directory tree that stores default configuration setup
     in <filename>/srv/pillar/ceph/stack/default</filename>. Do not change
     values here, as they are overwritten.
    </para>
   </note>
   <para>
    The typical procedure of changing the collected configuration is the
    following:
   </para>
   <procedure>
    <step>
     <para>
      Find the location of the configuration item you need to change. For
      example, if you need to change cluster related thing like cluster
      network, edit the file
      <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename>.
     </para>
    </step>
    <step>
     <para>
      Save the file.
     </para>
    </step>
    <step>
     <para>
      Verify the changes by running:
     </para>
<screen><prompt>root@master &gt; </prompt>salt '*' saltutil.pillar_refresh</screen>
     <para>
      and then
     </para>
<screen><prompt>root@master &gt; </prompt>salt '*' pillar.items</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
  <chapter xml:base="admin_install_ceph-deploy.xml" version="5.0" xml:id="ceph.install.ceph-deploy">
 <title>Deploying with <command>ceph-deploy</command></title>
 <para>
  <command>ceph-deploy</command> is a command line utility to simplify the way
  you deploy Ceph cluster in small scale setups.
 </para>
 <sect1 xml:id="ceph.install.ceph-deploy.layout">
  <title>Ceph Layout</title>

  <para>
   For testing purposes, a minimal Ceph cluster can be made to run on a
   single node. However, in a production setup we recommend using at least four
   nodes: one admin node and three cluster nodes, each running one monitor
   daemon and some number of object storage daemons (OSDs).
  </para>

  <figure>
   <title>Minimal Ceph Setup</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_minimal.png" width="60%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_minimal.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <tip>
   <para>
    Although Ceph nodes can be virtual machines, real hardware is strongly
    recommended for the production environment.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ceph.install.ceph-deploy.network">
  <title>Network Recommendations</title>

  <para>
   The network environment where you intend to run Ceph should ideally be a
   bonded set of at least two network interfaces that is logically split into a
   public part and trusted internal part using VLANs. The bonding mode is
   recommended to be 802.3ad when possible to provide maximum bandwidth and
   resiliency.
  </para>

  <para>
   The public VLAN serves for providing the service to the customers, the
   internal part provides for the authenticated Ceph network communication.
   The main reason is that although Ceph authentication and protection
   against attacks once secret keys are in place, the messages used to
   configure these keys may be transferred open and are vulnerable.
  </para>

  <tip>
   <title>Nodes Configured via DHCP</title>
   <para>
    If your storage nodes are configured via DHCP, the default timeouts may not
    be sufficient for the network to be configured correctly before the various
    Ceph daemons start. If this happens, the Ceph MONs and OSDs will not
    start correctly (running <command>systemctl status ceph\*</command> will
    result in "unable to bind" errors), and Calamari may be unable to display
    graphs. To avoid this issue, we recommend increasing the DHCP client
    timeout to at least 30 seconds on each node in your storage cluster. This
    can be done by changing the following settings on each node:
   </para>
   <para>
    In <filename>/etc/sysconfig/network/dhcp</filename> set
   </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
   <para>
    In <filename>/etc/sysconfig/network/config</filename> set
   </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
  </tip>
 </sect1>
 <sect1 xml:id="ceph.install.ceph-deploy.eachnode">
  <title>Preparing Each Ceph Node</title>

  <para>
   Before deploying the Ceph cluster, apply the following steps for each
   Ceph node as <systemitem class="username">root</systemitem>:
  </para>

  <procedure>
   <step>
    <para>
     Install SUSE Linux Enterprise 12 SP2 and add the SUSE Enterprise Storage extension.
    </para>
    <figure>
     <title>SUSE Enterprise Storage Extension Selection</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="install_extension.png" width="70%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="install_extension.png" width="70%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
     On the <guimenu>Installation Settings</guimenu> screen, click
     <guimenu>Software</guimenu>. On the <guimenu>Software Selection and System
     Tasks</guimenu> screen, there are several tasks related to SUSE Enterprise Storage. For
     OSDs, monitors, or the admin server, be sure to choose SUSE Enterprise Storage server
     packages and confirm with <guimenu>OK</guimenu>.
    </para>
    <figure>
     <title>SUSE Enterprise Storage Related Installation Tasks</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="install_soft_sel.png" width="70%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="install_soft_sel.png" width="70%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
     For more information on the extension installation, see
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_add-ons.html"/>.
    </para>
   </step>
   <step>
    <para>
     Check the firewall status
    </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
    <para>
     and if it is on, either turn it off with
    </para>
<screen>sudo /sbin/SuSEfirewall2 off</screen>
    <para>
     or, if you want to keep it on, enable the appropriate set of ports. You
     can find detailed information in
     <xref linkend="storage.bp.net.firewall" role="internalbook"/>.
    </para>
   </step>
   <step>
    <para>
     Make sure that network settings are correct: each Ceph node needs to
     route to all other Ceph nodes, and each Ceph node needs to resolve all
     other Ceph nodes by their short host names (without the domain suffix).
     If these two conditions are not met, Ceph fails.
    </para>
    <tip>
     <title>Calamari Node</title>
     <para>
      If you plan to deploy the Calamari monitoring and management environment
      (refer to <xref linkend="ceph.install.calamari" role="internalbook"/> for more information),
      each Ceph node needs to reach the Calamari node as well.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Install and set up NTP—the time synchronization tool. We strongly
     recommend using NTP within the Ceph cluster. The reason is that Ceph
     daemons pass critical messages to each other, which must be processed
     before daemons reach a timeout threshold. If the clocks in Ceph monitors
     are not synchronized, it can lead to a number of anomalies, such as
     daemons ignoring received messages.
    </para>
    <para>
     Even though clock drift may still be noticeable with NTP, it is not yet
     harmful.
    </para>
    <para>
     To install NTP, run the following:
    </para>
<screen>sudo zypper in ntp yast2-ntp-client</screen>
    <para>
     To configure NTP, go to <menuchoice><guimenu>YaST</guimenu>
     <guimenu>Network Services</guimenu> <guimenu>NTP
     Configuration</guimenu></menuchoice>. Make sure to enable the NTP service
     (<command>systemctl enable ntpd.service &amp;&amp; systemctl start
     ntpd.service</command>). Find more detailed information on NTP in the
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html">
     SLES Administration Guide</link>.
    </para>
   </step>
   <step>
    <para>
     Install SSH server. Ceph uses SSH to log in to all cluster nodes. Make
     sure SSH is installed (<command>zypper in openssh</command>) and enabled
     (<command>systemctl enable sshd.service &amp;&amp; systemctl start
     sshd.service</command>).
    </para>
   </step>
   <step>
    <para>
     Add a <systemitem class="username">cephadm</systemitem> user account, and set password for it. The admin node
     will log in to Ceph nodes as this particular <systemitem class="username">cephadm</systemitem> user .
    </para>
<screen>sudo useradd -m cephadm &amp;&amp; passwd cephadm</screen>
   </step>
   <step>
    <para>
     The admin node needs to have passwordless SSH access to all Ceph nodes.
     When <command>ceph-deploy</command> logs in to a Ceph node as a
     <systemitem class="username">cephadm</systemitem> user, this user must have passwordless <command>sudo</command>
     privileges.
    </para>
    <para>
     Edit the <filename>/etc/sudoers</filename> file (with
     <command>visudo</command>) and add the following line to add the
     <command>sudo</command> command for the <systemitem class="username">cephadm</systemitem> user:
    </para>
<screen>cephadm ALL = (root) NOPASSWD:ALL</screen>
    <tip>
     <title>Disable <literal>requiretty</literal></title>
     <para>
      You may receive an error while trying to execute
      <command>ceph-deploy</command> commands. If <literal>requiretty</literal>
      is set by default, disable it by executing <command>sudo visudo</command>
      and locate the <literal>Defaults requiretty</literal> setting. Change it
      to<literal> Defaults:cephadm !requiretty</literal> to ensure
      that <command>ceph-deploy</command> can connect using the <systemitem class="username">cephadm</systemitem> user
      and execute commands with <command>sudo</command>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     On the admin node, become the <systemitem class="username">cephadm</systemitem> user, and enable passwordless SSH
     access to all other Ceph nodes:
    </para>
<screen>su - cephadm
<prompt>cephadm &gt; </prompt>ssh-keygen</screen>
    <para>
     You will be asked several questions. Leave the values at their defaults,
     and the passphrase empty.
    </para>
    <para>
     Copy the key to each Ceph node:
    </para>
<screen>ssh-copy-id cephadm@<replaceable>node1</replaceable>
ssh-copy-id cephadm@<replaceable>node2</replaceable>
ssh-copy-id cephadm@<replaceable>node3</replaceable></screen>
    <tip>
     <title>Running <command>ceph-deploy</command> from a Different User Account Than <systemitem class="username">cephadm</systemitem></title>
     <para>
      It is possible to run the <command>ceph-deploy</command> command even if
      you are logged in as a different user than <systemitem class="username">cephadm</systemitem>. For this purpose,
      you need to set up an SSH alias in your
      <filename>~/.ssh/config</filename> file:
     </para>
<screen>[...]
Host ceph-node1
  Hostname ceph-node1
  User cephadm</screen>
     <para>
      After this change, <command>ssh ceph-node1</command> automatically uses
      the <systemitem class="username">cephadm</systemitem> user to log in.
     </para>
    </tip>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.install.ceph-deploy.purge">
  <title>Cleaning Previous Ceph Environment</title>

  <para>
   If at any point during the Ceph deployment you run into trouble and need
   to start over, or you want to make sure that any previous Ceph
   configuration is removed, execute the following commands as <systemitem class="username">cephadm</systemitem> user
   to purge the previous Ceph configuration.
  </para>

  <warning>
   <para>
    Be aware that <emphasis>purging</emphasis> previous Ceph installation
    destroys stored data and access settings.
   </para>
  </warning>

<screen><prompt>cephadm &gt; </prompt>ceph-deploy purge <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable>
<prompt>cephadm &gt; </prompt>ceph-deploy purgedata <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable>
<prompt>cephadm &gt; </prompt>ceph-deploy forgetkeys</screen>
 </sect1>
 <sect1 xml:id="ceph.install.ceph-deploy.cephdeploy">
  <title>Running <command>ceph-deploy</command></title>

  <para>
   After you prepared each Ceph node as described in
   <xref linkend="ceph.install.ceph-deploy.eachnode" role="internalbook"/>, you are ready to deploy
   Ceph from the admin node with <command>ceph-deploy</command>. Note that
   <command>ceph-deploy</command> will not successfully install an OSD on disks
   that have been previously used, unless you first 'zap' them. Be aware that
   'zapping' erases the entire disk content:
  </para>

<screen><prompt>cephadm &gt; </prompt>ceph-deploy disk zap <replaceable>node:vdb</replaceable></screen>

  <procedure>
   <step>
    <para>
     Install <command>ceph</command> and <command>ceph-deploy</command>:
    </para>
<screen>sudo zypper in ceph ceph-deploy</screen>
   </step>
   <step>
    <para>
     Disable IPv6. Open <filename>/etc/sysctl.conf</filename>, edit the
     following lines, and reboot the admin node:
    </para>
<screen>net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1</screen>
   </step>
   <step>
    <para>
     Because it is not recommended to run <command>ceph-deploy</command> as
     <systemitem class="username">root</systemitem>, become the <systemitem class="username">cephadm</systemitem> user:
    </para>
<screen>su - cephadm</screen>
   </step>
   <step>
    <para>
     Run <command>ceph-deploy</command> to install Ceph on each node:
    </para>
<screen><prompt>cephadm &gt; </prompt>ceph-deploy install <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable></screen>
    <tip>
     <para>
      <command>ceph-deploy</command> creates important files in the directory
      where you run it from. It is best to run <command>ceph-deploy</command>
      in an empty directory.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Set up the monitor nodes. Create keys and local configuration. The keys
     are used to authenticate and protect the communication between Ceph
     nodes.
    </para>
<screen><prompt>cephadm &gt; </prompt>ceph-deploy new <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable></screen>
    <para>
     During this step, <command>ceph-deploy</command> creates local
     configuration files. It is recommended to inspect the configuration files
     in the current directory.
    </para>
    <tip>
     <title>Monitor Nodes on Different Subnets</title>
     <para>
      If the monitor nodes are not in the same subnet, you need to modify the
      <filename>ceph.conf</filename> in the current directory. For example, if
      the nodes have IP addresses
     </para>
<screen>10.121.9.186
10.121.10.186
10.121.11.186</screen>
     <para>
      add the following line to the global section of
      <filename>ceph.conf</filename>:
     </para>
<screen>public network = 10.121.0.0/16</screen>
     <para>
      Since you are likely to experience problems with IPv6 networking,
      consider modifying the IPv6 mon_host settings, as in the following
      example:
     </para>
<screen>mon_host = [2620:...10:121:9:186,2620:...10:121:10:186,2620:...10:121:11:186]</screen>
     <para>
      into its IPv4 equivalent:
     </para>
<screen>mon_host = 10.121.9.186, 10.121.10.186, 10.121.11.186</screen>
    </tip>
   </step>
   <step>
    <para>
     Create the initial monitor service on already created monitor nodes:
    </para>
<screen><prompt>cephadm &gt; </prompt>ceph-deploy mon create-initial</screen>
   </step>
   <step>
    <para>
     Any node from which you need to run Ceph command line tools needs a copy
     of the admin keyring. To copy the admin keyring to a node or set of nodes,
     run
    </para>
<screen><prompt>cephadm &gt; </prompt>ceph-deploy admin <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>node3</replaceable></screen>
    <important>
     <para>
      Because the <literal>client.admin</literal>'s keyring file is readable by
      <systemitem class="username">root</systemitem> only, you need to use <command>sudo</command> when running the
      <command>ceph</command> command.
     </para>
    </important>
   </step>
   <step>
    <para>
     Check the firewall status
    </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
    <para>
     and if it is off, check its configuration and turn it on with
    </para>
<screen>sudo /sbin/SuSEfirewall2 on</screen>
    <para>
     You can find detailed information in
     <xref linkend="storage.bp.net.firewall" role="internalbook"/>.
    </para>
   </step>
   <step>
    <para>
     Create OSD daemons. Although you can use a directory as a storage, we
     recommend to create a separate disk dedicated to a Ceph node. To find
     out the name of the disk device, run
    </para>
<screen>cat /proc/partitions
major minor  #blocks  name

 254        0   12582912 vda
 254        1    1532928 vda1
 254        2   11048960 vda2
  11        0    2831360 sr0
 254       16    4194304 vdb</screen>
    <para>
     In our case the <systemitem>vdb</systemitem> disk has no partitions, so it
     is most likely our newly created disk.
    </para>
    <para>
     Now set up the disk for Ceph:
    </para>
<screen><prompt>cephadm &gt; </prompt>ceph-deploy osd prepare <replaceable>node:vdb</replaceable></screen>
    <tip>
     <title>Using Existing Partitions</title>
     <para>
      If you need to create OSDs on already existing partitions, you need to
      set their GUIDs correctly. See
      <xref linkend="bp.osd_on_exisitng_partitions" role="internalbook"/> for more details.
     </para>
    </tip>
    <tip>
     <para>
      If the disk was already used before, add the <option>--zap</option>
      option.
     </para>
<screen><prompt>cephadm &gt; </prompt>ceph-deploy osd prepare --zap <replaceable>node:vdb</replaceable></screen>
     <para>
      Be aware that 'zapping' erases the entire disk content.
     </para>
    </tip>
    <note>

     <title>Default File System for OSDs</title>
     <para>
      The default and only supported file system for OSDs is
      <literal>xfs</literal>.
     </para>
    </note>
    <para>
     Optionally, activate the OSD:
    </para>
<screen><prompt>cephadm &gt; </prompt>ceph-deploy osd activate <replaceable>node:vdb1</replaceable></screen>
    <tip>
     <para>
      To join the functionality of <command>ceph-deploy osd prepare</command>
      and <command>ceph-deploy osd activate</command>, use <command>ceph-deploy
      osd create</command>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     To test the status of the cluster, run
    </para>
<screen>sudo ceph -k ceph.client.admin.keyring health</screen>
   </step>
  </procedure>

  <tip>
   <title>Non-default Cluster Name</title>
   <para>
    If you need to install the cluster with <command>ceph-deploy</command>
    using a name other than the default <literal>cluster</literal> name, you
    need to initially specify it with <option>--cluster</option>, and then
    specify it in each <command>ceph-deploy</command> command related to that
    cluster:
   </para>
<screen>ceph-deploy --cluster my_cluster new [...]
ceph-deploy --ceph-conf my_cluster.conf mon create-initial
ceph-deploy --ceph-conf my_cluster.conf osd prepare [...]
ceph-deploy --ceph-conf my_cluster.conf osd activate [...]</screen>
   <para>
    Note that using a name other than default cluster name is not supported by
    SUSE.
   </para>
  </tip>
 </sect1>
</chapter>
  <chapter xml:base="admin_install_crowbar.xml" version="5.0" xml:id="ceph.install.crowbar">
 <title>Deploying with Crowbar</title>
 <para>
  Crowbar (<link xlink:href="http://crowbar.github.io/"/>) is a framework to
  build complete deployments. It helps you transform groups of bare-metal nodes
  into an operational cluster within relatively short time.
 </para>
 <para>
  The deployment process consists of two basic steps: first you need to install
  and set up the Crowbar admin server, then use it to deploy the available
  OSD/monitor nodes.
 </para>
 <sect1 xml:id="ceph.install.crowbar.admin_server">
  <title>Installing and Setting Up the Crowbar Admin Server</title>

  <para>
   Crowbar admin server is a stand-alone host with SUSE Linux Enterprise Server 12 SP2 installed, operating
   in the same network as the Ceph OSD/MON nodes to be deployed. You need to
   configure the Crowbar admin server so that it provides software repositories
   required for Ceph deployment via TFTP protocol and PXE network boot.
  </para>

  <procedure>
   <step>
    <para>
     Install and register SUSE Linux Enterprise Server 12 SP2 on the Crowbar admin server. Optionally, you
     can install and register the SUSE Enterprise Storage 4 extension at the same time. For
     more information on SUSE Linux Enterprise Server installation, see
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_inst.html"/>.
     For more information on the extensions installation, see
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_add-ons.html"/>.
    </para>
    <tip>
     <para>
      Crowbar admin server does not require any graphical interface. To save the
      system resources and disk space, it is enough to install the
      <guimenu>Base System</guimenu>, <guimenu>Minimal System</guimenu> and, if
      you chose to install the SUSE Enterprise Storage 4 extension, <guimenu>SUSE Enterprise
      Storage Crowbar</guimenu> patterns from the <guimenu>Software Selection
      and System Tasks</guimenu> window. If you plan to synchronize
      repositories (see
      <xref linkend="ceph.install.crowbar.admin_server.repos" role="internalbook"/>) with SMT,
      add the <guimenu>Subscription Management Tool</guimenu> pattern as well.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Configure network settings for the Crowbar admin server. The server needs
     to have a static IP address assigned, and the full host name including the
     domain name specified (for example
     <literal>crowbar-admin.example.com</literal>). Check with
     <command>hostname -f</command> if the host name resolves correctly. The
     local network where you deploy the cluster needs to have the DHCP server
     disabled as the Crowbar admin server runs its own.
    </para>
    <tip>
     <para>
      Crowbar admin server default IP address is 192.168.124.10. If it is
      possible to keep that IP in your network environment, you can save some
      time on reconfiguring the Crowbar network settings.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Configure NTP to keep the server's time synchronized. See
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html"/>
     for more information on the NTP protocol.
    </para>
   </step>
   <step>
    <para>
     Make sure that SSH is enabled and started on the server.
    </para>
   </step>
   <step>
    <para>
     Install and register the SUSE Enterprise Storage 4 extension if you did not install it
     in step 1. For more information on extension installation, see
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_add-ons.html"/>.
     Then install the <guimenu>SUSE Enterprise Storage Crowbar</guimenu>
     pattern in YaST. If you prefer the command line, run <command>sudo
     zypper in -t pattern ses_admin</command>.
    </para>
   </step>
   <step>
    <para>
     Mount software repositories required for Ceph nodes deployment with
     Crowbar. See <xref linkend="ceph.install.crowbar.admin_server.repos" role="internalbook"/> for
     more detailed information.
    </para>
   </step>
   <step>
    <para>
     If you need to further customize the Crowbar admin server settings, refer
     to the <emphasis>Crowbar Setup</emphasis> chapter of the current
     <emphasis>SUSE OpenStack Cloud Deployment Guide</emphasis> at
     <link xlink:href="https://www.suse.com/documentation"/>.
    </para>
   </step>
   <step>
    <para>
     Run the following commands to complete the Crowbar admin server setup. The
     <command>install-ses-admin</command> script outputs a lot of information to
     the <filename>/var/log/crowbar/install.log</filename> log file which can be
     examined in the case of failure. Run it in the
     <systemitem>screen</systemitem> environment for safety reasons, as the
     network will be reconfigured during its run and interrupts may occur.
    </para>
<screen>sudo systemctl start crowbar-init
sudo crowbarctl database create
screen install-ses-admin</screen>
    <para>
     Be patient as the script takes several minutes to finish.
    </para>
   </step>
   <step>
    <para>
     After the script successfully finishes, you can view the Crowbar admin
     server Web UI by pointing your Web browser to the Crowbar admin server IP
     address (http://192.168.124.10 by default).
    </para>
   </step>
  </procedure>

  <sect2 xml:id="ceph.install.crowbar.admin_server.repos">
   <title>Prepare Software Repositories</title>
   <para>
    Crowbar admin server needs to provide several software repositories so that
    the Ceph nodes can install required packages from them on PXE boot. These
    repositories need to be mounted/synchronized under
    <filename>/srv/tftpboot/suse-12.2</filename>. The following description is
    based on the AMD64/Intel 64 architecture.
   </para>
   <tip>
    <title>Synchronizing Repositories</title>
    <para>
     There are several ways to provide the content in the repository
     directories. You can, for example, run your local SMT instance,
     synchronize the repositories, and then export them via NFS and mount them
     on the Crowbar admin server.
    </para>
   </tip>
   <variablelist>
    <varlistentry>
     <term>/srv/tftpboot/suse-12.2/x86_64/install</term>
     <listitem>
      <para>
       This directory needs to contain the contents of the SUSE Linux Enterprise Server 12 SP2 DVD disc
       #1. Ceph nodes need it for the base SUSE Linux Enterprise Server 12 SP2 installation. You can
       either mount the downloaded .iso image as a loop device, or copy its
       content with <command>rsync</command>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>/srv/tftpboot/suse-12.2/x86_64/repos/SLES12-SP2-Pool</term>
     <listitem>
      <para>
       Base software repository for SUSE Linux Enterprise Server 12 SP2.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>/srv/tftpboot/suse-12.2/x86_64/repos/SLES12-SP2-Updates</term>
     <listitem>
      <para>
       Repository containing updates for SUSE Linux Enterprise Server 12 SP2.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>/srv/tftpboot/suse-12.2/x86_64/repos/SUSE-Enterprise-Storage-4-Pool</term>
     <listitem>
      <para>
       Base software repository for SES 4.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>/srv/tftpboot/suse-12.2/x86_64/repos/SUSE-Enterprise-Storage-4-Updates</term>
     <listitem>
      <para>
       Repository containing updates for SES 4.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.install.crowbar.deploy_nodes">
  <title>Deploying the Ceph Nodes</title>

  <para>
   The Crowbar Web interface runs on the Administration Server. It provides an overview of the most
   important deployment details, including a view on the nodes and which roles
   are deployed on which nodes, and on the barclamp proposals that can be edited
   and deployed. In addition, the Crowbar Web interface shows details about the networks
   and switches in your cluster. It also provides graphical access to some
   tools with which you can manage your repositories, back up or restore the
   Administration Server, export the Chef configuration, or generate a
   <literal>supportconfig</literal> TAR archive with the most important log
   files.
  </para>

  <sect2 xml:id="sec.depl.crow.login">
   <title>Logging In</title>
   <para>
    The Crowbar Web interface uses the HTTP protocol and port <literal>80</literal>.
   </para>
   <procedure xml:id="pro.depl.crow.login">
    <title>Logging In to the Crowbar Web Interface</title>
    <step>
     <para>
      On any machine, start a Web browser and make sure that JavaScript and
      cookies are enabled.
     </para>
    </step>
    <step>
     <para>
      As URL, enter the IP address of the Administration Server, for example:
     </para>
<screen>http://192.168.124.10/</screen>
    </step>
    <step>
     <para>
      Log in as user <systemitem class="username">crowbar</systemitem>. If you
      have not changed the password, it is <literal>crowbar</literal> by
      default.
     </para>
    </step>
   </procedure>

   <procedure xml:id="pro.depl.crow.password">
    <title>Changing the Password for the Crowbar Web Interface</title>
    <step>
     <para>
      After being logged in to the Crowbar Web interface, select <menuchoice>
      <guimenu>Barclamp</guimenu> <guimenu>Crowbar</guimenu> </menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Select the <literal>Crowbar</literal> barclamp entry and
      <guimenu>Edit</guimenu> the proposal.
     </para>
    </step>
    <step>
     <para>
      In the <guimenu>Attributes</guimenu> section, click
      <guimenu>Raw</guimenu> to edit the configuration file.
     </para>
    </step>
    <step>
     <para>
      Search for the following entry:
     </para>
<screen>"crowbar": {
     "password": "crowbar"</screen>
    </step>
    <step>
     <para>
      Change the password.
     </para>
    </step>
    <step>
     <para>
      Confirm your change by clicking <guimenu>Save</guimenu> and
      <guimenu>Apply</guimenu>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.depl.inst.nodes.install">
   <title>Node Installation</title>
   <para>
    The Ceph nodes represent the actual cluster infrastructure. Node
    installation and service deployment is done automatically from the
    Administration Server. Before deploying the Ceph service, SUSE Linux Enterprise Server 12 SP2 will be installed
    on all nodes.
   </para>
   <para>
    To install a node, you need to boot it first using PXE. It will be booted
    with an image that enables the Administration Server to discover the node and make it
    available for installation. When you have allocated the node, it will boot
    using PXE again and the automatic installation will start.
   </para>
   <procedure>
    <step>
     <para>
      Boot all nodes that you want to deploy using PXE. The nodes will boot
      into the <quote>SLEShammer</quote> image, which performs the initial
      hardware discovery.
     </para>
     <important>
      <title>Limit the Number of Concurrent Boots using PXE</title>
      <para>
       Booting many nodes using PXE at the same time will cause heavy load on
       the TFTP server, because all nodes will request the boot image at the
       same time. It is recommended to boot the nodes time-delayed.
      </para>
     </important>
    </step>
    <step>
     <para>
      Open a browser and point it to the Crowbar Web interface on the Administration Server,
      for example <literal>http://192.168.124.10/</literal>. Log in as user
      <systemitem class="username">crowbar</systemitem>. The password is
      <literal>crowbar</literal> by default, if you have not changed it.
     </para>
     <para>
      Click <menuchoice> <guimenu>Nodes</guimenu> <guimenu>Dashboard</guimenu>
      </menuchoice> to open the <guimenu>Node Dashboard</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Each node that has successfully booted will be listed as being in state
      <literal>Discovered</literal>, indicated by a yellow bullet. The nodes
      will be listed with their MAC address as a name. Wait until all nodes are
      listed as being <literal>Discovered</literal> before proceeding. In case
      a node does not report as being <literal>Discovered</literal>, it may
      need to be rebooted manually.
     </para>
     <figure>
      <title>Discovered Nodes</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_node_dashboard_initial_nodes.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_node_dashboard_initial_nodes.png" width="75%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Although this step is optional, it is recommended to properly group your
      nodes at this stage, since it lets you clearly arrange all nodes.
      Grouping the nodes by role would be one option, for example monitor nodes
      and OSD nodes.
     </para>
     <substeps performance="required">
      <step>
       <para>
        Enter the name of a new group into the <guimenu>New Group</guimenu>
        text box and click <guimenu>Add Group</guimenu>.
       </para>
      </step>
      <step>
       <para>
        Drag and drop a node onto the title of the newly created group. Repeat
        this step for each node you want to put into the group.
       </para>
       <figure>
        <title>Grouping Nodes</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="depl_node_dashboard_groups_initial.png" width="100%" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="depl_node_dashboard_groups_initial.png" width="75%" format="PNG"/>
         </imageobject>
        </mediaobject>
       </figure>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      To allocate all nodes, click <menuchoice><guimenu>Nodes</guimenu>
      <guimenu>Bulk Edit</guimenu></menuchoice>. To allocate a single node,
      click the name of a node, then click <guimenu>Edit</guimenu>.
     </para>
     <figure>
      <title>Editing a Single Node</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_node_edit.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_node_edit.png" width="75%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <important>
      <title>Limit the Number of Concurrent Node Deployments</title>
      <para>
       Deploying many nodes in bulk mode will cause heavy load on the
       Administration Server. The subsequent concurrent Chef client runs triggered by the
       nodes will require a lot of RAM on the Administration Server.
      </para>
      <para>
       Therefore it is recommended to limit the number of concurrent
       <quote>Allocations</quote> in bulk mode. The maximum number depends on
       the amount of RAM on the Administration Server—limiting concurrent deployments
       to five up to ten is recommended.
      </para>
     </important>
    </step>
    <step>
     <para>
      In single node editing mode, you can also specify the <guimenu>Filesystem
      Type</guimenu> for the node. By default, it is set to
      <literal>ext4</literal> for all nodes. It is recommended to keep this
      default.
     </para>
    </step>
    <step>
     <para>
      Provide a meaningful <guimenu>Alias</guimenu>, <guimenu>Public
      Name</guimenu> and a <guimenu>Description</guimenu> for each node and
      check the <guimenu>Allocate</guimenu> box. You can also specify the
      <guimenu>Intended Role</guimenu> for the node. This optional setting is
      used to make reasonable proposals for the barclamps.
     </para>
     <para>
      By default <guimenu>Target Platform</guimenu> is set to <guimenu>SLES 12
      SP2</guimenu>.
     </para>
     <tip>
      <title>Alias Names</title>
      <para>
       Providing an alias name will change the default node names (MAC address)
       to the name you provided, making it easier to identify the node.
       Furthermore, this alias will also be used as a DNS
       <literal>CNAME</literal> for the node in the admin network. As a result,
       you can access the node via this alias when, for example, logging in via
       SSH.
      </para>
     </tip>
     <figure>
      <title>Bulk Editing Nodes</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_node_bulk_edit_allocate.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_node_bulk_edit_allocate.png" width="75%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      When you have filled in the data for all nodes, click
      <guimenu>Save</guimenu>. The nodes will reboot and commence the
      AutoYaST-based SUSE Linux Enterprise Server installation (or installation of other target platforms,
      if selected) via a second boot using PXE. Click <menuchoice>
      <guimenu>Nodes</guimenu> <guimenu>Dashboard</guimenu> </menuchoice> to
      return to the <guimenu>Node Dashboard</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Nodes that are being installed are listed with the status
      <literal>Installing</literal> (yellow/green bullet). When the
      installation of a node has finished, it is listed as being
      <literal>Ready</literal>, indicated by a green bullet. Wait until all
      nodes are listed as being <literal>Ready</literal> before proceeding.
     </para>
     <figure>
      <title>All Nodes Have Been Installed</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_node_dashboard_groups_installed.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_node_dashboard_groups_installed.png" width="75%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.depl.ceph.barclamps">
   <title>Barclamps</title>
   <para>
    The Ceph service is automatically installed on the nodes by using
    so-called barclamps—a set of recipes, templates, and installation
    instructions. A barclamp is configured via a so-called proposal. A proposal
    contains the configuration of the service(s) associated with the barclamp
    and a list of machines onto which the barclamp should be deployed.
   </para>
   <para>
    All existing barclamps can be accessed from the Crowbar Web interface by
    clicking <guimenu>Barclamps</guimenu>. To create or edit barclamp proposals
    and deploy them, proceed as follows:
   </para>
   <procedure>
    <step>
     <para>
      Open a browser and point it to the Crowbar Web interface available on the
      Administration Server, for example <literal>http://192.168.124.10/</literal>. Log in
      as user <systemitem class="username">crowbar</systemitem>. The password
      is <literal>crowbar</literal> by default, if you have not changed it.
     </para>
     <para>
      Click <guimenu>Barclamps</guimenu> to open the <guimenu>All
      Barclamps</guimenu> menu. Alternatively you may filter the list to
      <guimenu>Crowbar</guimenu> or <guimenu>SUSE Enterprise Storage</guimenu>
      barclamps by choosing the respective option from
      <guimenu>Barclamps</guimenu>. The <guimenu>Crowbar</guimenu> barclamps
      contain general recipes for setting up and configuring all nodes, while
      the <guimenu>SUSE Enterprise Storage</guimenu> barclamps are dedicated to
      Ceph service deployment and configuration.
     </para>
    </step>
    <step>
     <para>
      You can either <guimenu>Create</guimenu> a proposal or
      <guimenu>Edit</guimenu> an existing one.
     </para>
     <para>
      Most Ceph barclamps consist of two sections: the
      <guimenu>Attributes</guimenu> section lets you change the configuration,
      and the <guimenu>Node Deployment</guimenu> section lets you choose onto
      which nodes to deploy the barclamp.
     </para>
    </step>
    <step>
     <para>
      To edit the <guimenu>Attributes</guimenu> section, change the values via
      the Web form. Alternatively you can directly edit the configuration file
      by clicking <guimenu>Raw</guimenu>.
     </para>
     <warning>
      <title>Raw Mode</title>
      <para>
       If you switch between <guimenu>Raw</guimenu> mode and Web form
       (<guimenu>Custom</guimenu> mode), make sure to <guimenu>Save</guimenu>
       your changes before switching, otherwise they will be lost.
      </para>
     </warning>
    </step>
    <step>
     <para>
      To assign nodes to a role, use the <guimenu>Deployment</guimenu> section
      of the barclamp. It shows the <guimenu>Available Nodes</guimenu> that you
      can assign to the roles belonging to the barclamp.
     </para>
     <para>
      One or more nodes are usually automatically pre-selected for available
      roles. If this pre-selection does not meet your requirements, click the
      <guimenu>Remove</guimenu> icon next to the role to remove the assignment.
      Assign a node or cluster of your choice by selecting the respective entry
      from the list of <guimenu>Available Nodes</guimenu>, <guimenu>Available
      Clusters</guimenu>, or <guimenu>Available Clusters with Remote
      Nodes</guimenu>. Drag it to the desired role and drop it onto the
      <emphasis>role name</emphasis>. Do <emphasis>not</emphasis> drop a node
      or cluster onto the text box—this is used to filter the list of
      available nodes or clusters!
     </para>
    </step>
    <step>
     <para>
      To save and deploy your edits, click <guimenu>Apply</guimenu>. To save
      your changes without deploying them, click <guimenu>Save</guimenu>. To
      remove the complete proposal, click <guimenu>Delete</guimenu>. A proposal
      that already has been deployed can only be deleted manually, see
      <xref linkend="sec.depl.ceph.barclamps.delete" role="internalbook"/> for details.
     </para>
     <para>
      If you deploy a proposal onto a node where a previous one is still
      active, the new proposal will overwrite the old one.
     </para>
     <note>
      <title>Wait Until a Proposal has been Deployed</title>
      <para>
       Deploying a proposal might take some time (up to several minutes). It is
       strongly recommended to always wait until you see the note
       <quote>Successfully applied the proposal</quote> before proceeding on to
       the next proposal.
      </para>
     </note>
    </step>
   </procedure>
   <sect3 xml:id="sec.depl.ceph.barclamps.delete">
    <title>Delete a Proposal That Already Has Been Deployed</title>
    <para>
     To delete a proposal that already has been deployed, you first need to
     <guimenu>Deactivate</guimenu> it in the Crowbar Web interface. Deactivating
     a proposal removes the chef role from the nodes, so the routine that
     installed and set up the services is not executed anymore. After a
     proposal has been deactivated, you can <guimenu>Delete</guimenu> it in the
     Crowbar Web interface to remove the barclamp configuration data from the
     server.
    </para>
    <para>
     Deactivating and deleting a barclamp that already had been deployed does
     <emphasis>not</emphasis> remove packages installed when the barclamp was
     deployed. Nor does it stop any services that were started during the
     barclamp deployment. To undo the deployment on the affected node, you need
     to stop (<command>systemctl stop
     <replaceable>service</replaceable></command>) the respective services and
     disable (<command>systemctl disable
     <replaceable>service</replaceable></command>) them. Uninstalling packages
     should not be necessary.
    </para>
   </sect3>
   <sect3 xml:id="sec.depl.ceph.barclamps.queues">
    <title>Queuing/Dequeuing Proposals</title>
    <para>
     When a proposal is applied to one or more nodes that are nor yet available
     for deployment (for example because they are rebooting or have not been
     fully installed, yet), the proposal will be put in a queue. A message like
    </para>
<screen>Successfully queued the proposal until the following become ready: d52-54-00-6c-25-44</screen>
    <para>
     will be shown when having applied the proposal. A new button
     <guimenu>Dequeue</guimenu> will also become available. Use it to cancel
     the deployment of the proposal by removing it from the queue.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.depl.ceph.ceph">
   <title>Deploying Ceph</title>
   <para>
    For Ceph at least four nodes are required. If deploying the optional
    Calamari server for Ceph management and monitoring, an additional node is
    required.
   </para>
   <para>
    The Ceph barclamp has the following configuration options:
   </para>
   <variablelist>
    <varlistentry>
     <term><guimenu>Disk Selection Method</guimenu>
     </term>
     <listitem>
      <para>
       Choose whether to only use the first available disk or all available
       disks. <quote>Available disks</quote> are all disks currently not used
       by the system. Note that one disk (usually
       <filename>/dev/sda</filename>) of every block storage node is already
       used for the operating system and is not available for Ceph.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>Number of Replicas of an Object</guimenu>
     </term>
     <listitem>
      <para>
       For data security, stored objects are not only stored once, but
       redundantly. Specify the number of copies that should be stored for each
       object with this setting. The number includes the object itself. If you
       for example want the object plus two copies, specify 3.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>SSL Support for RadosGW</guimenu>
     </term>
     <listitem>
      <para>
       Choose whether to encrypt public communication
       (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If choosing
       <guimenu>HTTPS</guimenu>, you need to specify the locations for the
       certificate key pair files.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>Calamari Credentials</guimenu>
     </term>
     <listitem>
      <para>
       Calamari is a Web front-end for managing and analyzing the Ceph
       cluster. Provide administrator credentials (user name, password, e-mail
       address) in this section. When Ceph has bee deployed you can log in to
       Calamari with these credentials. Deploying Calamari is
       optional—leave these text boxes empty when not deploying Calamari.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <figure>
    <title>The Ceph Barclamp</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_barclamp_ceph.png" width="100%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_barclamp_ceph.png" width="75%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    The Ceph service consists of the following different roles:
   </para>
   <variablelist>
    <varlistentry>
     <term><guimenu>ceph-osd</guimenu>
     </term>
     <listitem>
      <para>
       The virtual block storage service. Install this role on all dedicated
       Ceph Storage Nodes (at least three), but not on any other node.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>ceph-mon</guimenu>
     </term>
     <listitem>
      <para>
       Cluster monitor daemon for the Ceph distributed file system.
       <guimenu>ceph-mon</guimenu> needs to be installed on three or five
       dedicated nodes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>ceph-calamari</guimenu>
     </term>
     <listitem>
      <para>
       Sets up the Calamari Web interface which lets you manage the Ceph
       cluster. Deploying it is optional. The Web interface can be accessed via
       http://<replaceable>IP-ADDRESS</replaceable>/ (where
       <replaceable>IP-ADDRESS</replaceable> is the address of the machine
       where <guimenu>ceph-calamari</guimenu> is deployed on).
       <guimenu>ceph-calamari</guimenu> needs to be installed on a dedicated
       node—it is <emphasis>not</emphasis> possible to install it on a
       nodes running other services.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>ceph-radosgw</guimenu>
     </term>
     <listitem>
      <para>
       The HTTP REST gateway for Ceph. Install it on a dedicated node.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>ceph-mds</guimenu>
     </term>
     <listitem>
      <para>
       The metadata server daemon for the CephFS. Install it on a dedicated
       node. For more information on CephFS refer to
       <xref linkend="cha.ceph.cephfs" role="internalbook"/>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <figure>
    <title>The Ceph Barclamp: Node Deployment Example</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_barclamp_ceph_node_deployment.png" width="100%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_barclamp_ceph_node_deployment.png" width="75%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>
 </sect1>
</chapter>
  <chapter xml:base="admin_ceph_upgrade.xml" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade SUSE Enterprise Storage from the previous
  release(s) to the current one.
 </para>
 <sect1 xml:id="ceph.upgrade.general">
  <title>General Upgrade Procedure</title>

  <para>
   Before upgrading the Ceph cluster, you need to have both the underlying
   SUSE Linux Enterprise Server and SUSE Enterprise Storage correctly registered against SCC or SMT. You can upgrade
   daemons in your cluster while the cluster is online and in service. Certain
   types of daemons depend upon others. For example Ceph <phrase>RADOS Gateway</phrase>s depend upon
   Ceph monitors and Ceph OSD daemons. We recommend upgrading in this
   order:
  </para>

  <orderedlist spacing="normal">
   <listitem>
    <para>
     Admin node (if you deployed the cluster using the admin node).
    </para>
   </listitem>
   <listitem>
    <para>
     Ceph monitors.
    </para>
   </listitem>
   <listitem>
    <para>
     Ceph OSD daemons.
    </para>
   </listitem>
   <listitem>
    <para>
     Ceph <phrase>RADOS Gateway</phrase>s.
    </para>
   </listitem>
  </orderedlist>

  <tip>
   <para>
    We recommend upgrading all the daemons of a specific type—for example
    all monitor daemons or all OSD daemons—one by one to ensure that they
    are all on the same release. We also recommend that you upgrade all the
    daemons in your cluster before you try to exercise new functionality in a
    release.
   </para>
   <para>
    After all the daemons of a specific type are upgraded, check their status.
   </para>
   <para>
    Ensure each monitor has rejoined the quorum after all monitors are
    upgraded:
   </para>
<screen>ceph mon stat</screen>
   <para>
    Ensure each Ceph OSD daemon has rejoined the cluster after all OSDs are
    upgraded:
   </para>
<screen>ceph osd stat</screen>
  </tip>
 </sect1>
 <sect1 xml:id="ceph.upgrade.to4">
  <title>Upgrade from SUSE Enterprise Storage 2.1/3 to 4</title>

  <para>
   This section includes general tasks when upgrading from SUSE Enterprise Storage 2.1/3 to 4.
  </para>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    packages versions on all the Ceph nodes you want to upgrade before you
    can start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP1
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 2.1 or 3
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   To upgrade the SUSE Enterprise Storage 2.1 or 3 cluster to version 4, follow these steps
   on each cluster node:
  </para>

  <procedure>
   <step>
    <warning>
     <title>Do Not Run <command>zypper dup</command> or Reboot the Node</title>
     <para>
      After you prepare for the upgrade to SUSE Linux Enterprise Server 12 SP2 as suggested later in
      this step, do <emphasis>not</emphasis> run <command>zypper dup</command>
      or reboot the node as its Ceph related services may not start
      correctly.
     </para>
    </warning>
    <para>
     Upgrade the current SUSE Linux Enterprise Server to version 12 SP2. Refer to
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_update_sle.html"/>
     for more information on supported upgrade methods.
    </para>
   </step>
   <step>
    <para>
     List all the active services with <command>zypper ls</command>.
    </para>
<screen>zypper ls
#| Alias                                      | Name | Enabled | Refresh | Type
-+--------------------------------------------+------+---------+---------+------
1| SUSE_Enterprise_Storage_3_x86_64           | ...  | Yes     | Yes     | ris
2| SUSE_Linux_Enterprise_Server_12_SP2_x86_64 | ...  | Yes     | Yes     | ris
[...]</screen>
    <para>
     Verify that services related to SUSE Linux Enterprise Server 12 SP2 are present and enabled.
    </para>
   </step>
   <step>
    <para>
     Remove the current SUSE Enterprise Storage service. You can do it as follows:
    </para>
<screen>sudo zypper rs <replaceable>ID</replaceable></screen>
   </step>
   <step>
    <para>
     Activate SUSE Enterprise Storage 4 service. You can use <command>yast2 add-on</command>.
    </para>
   </step>
   <step>
    <para>
     Refresh new software repositories:
    </para>
<screen>sudo zypper ref</screen>
   </step>
   <step>
    <para>
     Install the upgrade helper package:
    </para>
<screen>sudo zypper in ses-upgrade-helper</screen>
   </step>
   <step>
    <para>
     Run the upgrade script:
    </para>
<screen>sudo upgrade-ses.sh</screen>
    <para>
     The script does the distribution upgrade of the node. After reboot, the
     node comes up with SUSE Linux Enterprise Server 12 SP2 and SUSE Enterprise Storage 4 running.
    </para>
   </step>
   <step>
   	<para>
   	Check that the folder <filename>/var/log/ceph</filename> is owned by the <literal>ceph</literal>. If not, change it:
   	</para>
   	<screen>sudo chown ceph.ceph /var/log/ceph</screen>
   </step>
  </procedure>

  <sect2 xml:id="ceph.upgrade.2.1to3.iscsi_up">
   <title>iSCSI Gateways Upgrade</title>
   <para>
    For iSCSI gateways, consider the following points during upgrades:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Check that the <systemitem>lrbd</systemitem> service is enabled so that
      the iSCSI gateway configuration is applied on reboot.
     </para>
    </listitem>
    <listitem>
     <para>
      Upgrade the iSCSI gateway nodes after the monitor and OSD nodes.
     </para>
     <tip>
      <para>
       If an iSCSI gateway includes OSD or monitor processes on the same node,
       then upgrade and restart these processes before the system is rebooted
       into the new kernel.
      </para>
     </tip>
    </listitem>
    <listitem>
     <para>
      Check that the Ceph cluster health is <literal>HEALTH_OK</literal> when
      proceeding with the iSCSI gateway upgrade.
     </para>
    </listitem>
    <listitem>
     <para>
      iSCSI initiators (clients) that require access to storage throughout the
      upgrade need to be configured with multi-path I/O (MPIO).
     </para>
     <itemizedlist mark="opencircle">
      <listitem>
       <para>
        Before rebooting or taking an iSCSI gateway node offline, manually
        disable the corresponding initiator MPIO device paths on the client.
       </para>
      </listitem>
      <listitem>
       <para>
        Once the gateway is back online, enable the client MPIO device path(s).
       </para>
      </listitem>
      <listitem>
       <para>
        For all gateway nodes exposing a given iSCSI target, care should be
        taken to ensure that no more than one iSCSI gateway node is offline
        (rebooted for kernel update) at any moment during the upgrade.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
   <sect3 xml:id="ceph.upgrade.2.1to3.iscsi">
    <title>Updated Behavior for iSCSI Gateways</title>
    <itemizedlist>
     <listitem>
      <para>
       The <option>rbd_name</option> is a backward compatibility option for
       setting the backstore name to only use the name of the image. Starting
       from SUSE Enterprise Storage 3, the default uses
      </para>
<screen><replaceable>pool_name</replaceable>-<replaceable>image_name</replaceable></screen>
      <para>
       Do not use this option for new installations.
      </para>
<screen>"pools": [
    {
        "pool": "rbd",
        "gateways": [
            {
                "host": "igw1",
                "tpg": [
                    {
                        "image": "archive",
                        "rbd_name": "simple"
                    }
                ]
            }
        ]
    }
]</screen>
     </listitem>
     <listitem>
      <para>
       Likewise, <option>wwn_generate</option> will use the original scheme of
       target and image name for setting the <option>vpn_unit_serial</option>.
       The current default uses
      </para>
<screen><replaceable>pool_name</replaceable>-<replaceable>target</replaceable>-<replaceable>image_name</replaceable></screen>
      <para>
       Do not use this option for new installations.
      </para>
<screen>"targets": [
  {
    "host": "igw1",
    "target": "iqn.2003-01.org.linux-iscsi.generic.x86:sn.abcdefghijk",
    "wwn_generate": "original"
  }</screen>
      <para>
       For more information, see
       <filename>/usr/share/doc/lrbd/README.migration</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       The <option>rgw_region_root_pool</option> is deprecated for federated
       <phrase>RADOS Gateway</phrase> deployments in SUSE Enterprise Storage 3. Replace it with the new
       <option>rgw_zonegroup_root_pool</option> option in
       <filename>ceph.conf</filename>
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.upgrade.2to4">
  <title>Upgrade from SUSE Enterprise Storage 2.1 to 4</title>
  <para>
  This section includes tasks specific to upgrading SUSE Enterprise Storage 2.1 to 4.
 </para>
 <sect2 xml:id="ceph.upgrade.2to4.jewel">
  <title>Set <option>require_jewel_osds</option> osdmap Flag</title>
  <para>
   When the last OSD is upgraded from SUSE Enterprise Storage 2.1 to 4, the monitor nodes will
   detect that all OSDs are running <emphasis>jewel</emphasis> and complain that
   the <option>require_jewel_osds osdmap</option> flag is not set.  You need to
   set this flag manually to acknowledge that, once the cluster has been
   upgraded to <emphasis>jewel</emphasis> it cannot be downgraded to
   <emphasis>hammer</emphasis>. Set the flag by running the following command:
  </para>
   <screen>sudo ceph osd set require_jewel_osds</screen>
   <para>
    After the command completes, the warning disappears.
   </para>
   <para>
    On fresh installs of SUSE Enterprise Storage 4, this flag is set automatically when the
    Ceph monitors create the initial osdmap, so no end-user action is needed in this
    case.
   </para>
 </sect2>
 </sect1>
</chapter>
 </part>
 <part xml:id="part.operate">
  <title>Operating a Cluster</title>
  <chapter xml:base="admin_operating_intro.xml" version="5.0" xml:id="cha.ceph.operating">
 <title>Introduction</title>
 <para>
  In this part of the manual you will learn how to start or stop Ceph
  services, how to monitor a cluster's state, how to use and modify CRUSH maps,
  and how to manage storage pools.
 </para>
 <para>
  It also includes advanced topics, for example how to manage users and
  authentication in general, how to manage pool and RADOS device snapshots, how
  to set up erasure coded pools, or how to increase the cluster performance
  with cache tiering.
 </para>
</chapter>
  <chapter xml:base="admin_operating_services.xml" version="5.0" xml:id="ceph.operating.services">
 <title>Operating Ceph Services</title>
 <para>
  Ceph related services are operated with the <command>systemctl</command>
  command. The operation takes place on the node you are currently logged in
  to. You need to have <systemitem class="username">root</systemitem> privileges to be able to operate on Ceph
  services.
 </para>
 <sect1 xml:id="ceph.operating.services.targets">
  <title>Starting, Stopping, and Restarting Services using Targets</title>

  <para>
   To facilitate starting, stopping, and restarting all the services of a
   particular type (for example all Ceph services, or all MONs, or all OSDs)
   on a node, Ceph provides the following <systemitem class="daemon">systemd</systemitem> unit files:
  </para>

<screen>ceph.target
ceph-osd.target
ceph-mon.target
ceph-mds.target
ceph-radosgw.target
ceph-rbd-mirror.target</screen>

  <para>
   To start/stop/restart all Ceph services on the node, run:
  </para>

<screen>systemctl stop ceph.target
systemctl start ceph.target
systemctl restart ceph.target</screen>

  <para>
   To start/stop/restart all OSDs on the node, run:
  </para>

<screen>systemctl stop ceph-osd.target
systemctl start ceph-osd.target
systemctl restart ceph-osd.target</screen>

  <para>
   Commands for the other targets are analogous.
  </para>
 </sect1>
 <sect1 xml:id="ceph.operating.services.individual">
  <title>Starting, Stopping, and Restarting Individual Services</title>

  <para>
   You can operate individual services using the following parametrized
   <systemitem class="daemon">systemd</systemitem> unit files:
  </para>

<screen>ceph-osd@.service
ceph-mon@.service
ceph-mds@.service
ceph-radosgw@.service
ceph-rbd-mirror@.service</screen>

  <para>
   To use these commands, you first need to identify the name of the service
   you want to operate. See
   <xref linkend="ceph.operating.services.finding_names" role="internalbook"/> to learn more about
   services identification.
  </para>

  <para>
   To start/stop/restart the <literal>osd.1</literal> service, run:
  </para>

<screen>systemctl stop ceph-osd@1.service
systemctl start ceph-osd@1.service
systemctl restart ceph-osd@1.service</screen>

  <para>
   Commands for the other service types are analogous.
  </para>
 </sect1>
 <sect1 xml:id="ceph.operating.services.finding_names">
  <title>Identifying Individual Services</title>

  <para>
   You can find out the names/numbers of a particular type of service by
   running <command>systemctl</command> and filtering the results with the
   <command>grep</command> command. For example:
  </para>

<screen>systemctl | grep -i 'ceph-osd.*service'
systemctl | grep -i 'ceph-mon.*service'
[...]</screen>
 </sect1>
 <sect1 xml:id="ceph.operating.services.status">
  <title>Service Status</title>

  <para>
   You can query <systemitem class="daemon">systemd</systemitem> for the status of services. For example:
  </para>

<screen>systemctl status ceph-osd@1.service
systemctl status ceph-mon@vanguard2.service</screen>

  <para>
   If you do not know the exact name/number of the service, see
   <xref linkend="ceph.operating.services.finding_names" role="internalbook"/>.
  </para>
 </sect1>
</chapter>
  <chapter xml:base="admin_operating_monitor.xml" version="5.0" xml:id="ceph.monitor">
 <title>Determining Cluster State</title>
 <para>
  Once you have a running cluster, you may use the <command>ceph</command> tool
  to monitor your cluster. Determining cluster state typically involves
  checking OSD status, monitor status, placement group status and metadata
  server status.
 </para>
 <tip>
  <title>Interactive Mode</title>
  <para>
   To run the <command>ceph</command> tool in an interactive mode, type
   <command>ceph</command> at the command line with no arguments. The
   interactive mode is more convenient if you are going to enter more
   <command>ceph</command> commands in a row. For example:
  </para>
<screen>ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</screen>
 </tip>
 <sect1 xml:id="monitor.health">
  <title>Checking Cluster Health</title>

  <para>
   After you start your cluster, and before you start reading and/or writing
   data, check your cluster’s health first. You can check on the health of
   your Ceph cluster with the following:
  </para>

<screen>ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>

  <para>
   If you specified non-default locations for your configuration or keyring,
   you may specify their locations:
  </para>

<screen>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>

  <para>
   Upon starting the Ceph cluster, you will likely encounter a health warning
   such as <literal>HEALTH_WARN XXX num placement groups stale</literal>. Wait
   a few moments and check it again. When your cluster is ready, <command>ceph
   health</command> should return a message such as
   <literal>HEALTH_OK</literal>. At that point, it is okay to begin using the
   cluster.
  </para>
 </sect1>
 <sect1 xml:id="monitor.watch">
  <title>Watching a Cluster</title>

  <para>
   To watch the cluster’s ongoing events, open a new terminal and enter:
  </para>

<screen>ceph -w</screen>

  <para>
   Ceph will print each event. For example, a tiny Ceph cluster consisting
   of one monitor, and two OSDs may print the following:
  </para>

<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41338: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
             952 active+clean

2014-06-02 15:45:21.655871 osd.0 [INF] 17.71 deep-scrub ok
2014-06-02 15:45:47.880608 osd.1 [INF] 1.0 scrub ok
2014-06-02 15:45:48.865375 osd.1 [INF] 1.3 scrub ok
2014-06-02 15:45:50.866479 osd.1 [INF] 1.4 scrub ok
[...]
2014-06-02 15:45:55.720929 mon.0 [INF] pgmap v41343: 952 pgs: \
 1 active+clean+scrubbing+deep, 951 active+clean; 17130 MB data, 115 GB used, \
 167 GB / 297 GB avail</screen>

  <para>
   The output provides the following information:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Cluster ID
    </para>
   </listitem>
   <listitem>
    <para>
     Cluster health status
    </para>
   </listitem>
   <listitem>
    <para>
     The monitor map epoch and the status of the monitor quorum
    </para>
   </listitem>
   <listitem>
    <para>
     The OSD map epoch and the status of OSDs
    </para>
   </listitem>
   <listitem>
    <para>
     The placement group map version
    </para>
   </listitem>
   <listitem>
    <para>
     The number of placement groups and pools
    </para>
   </listitem>
   <listitem>
    <para>
     The <emphasis>notional</emphasis> amount of data stored and the number of
     objects stored; and,
    </para>
   </listitem>
   <listitem>
    <para>
     The total amount of data stored.
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>How Ceph Calculates Data Usage</title>
   <para>
    The <literal>used</literal> value reflects the actual amount of raw storage
    used. The <literal>xxx GB / xxx GB</literal> value means the amount
    available (the lesser number) of the overall storage capacity of the
    cluster. The notional number reflects the size of the stored data before it
    is replicated, cloned or snapshotted. Therefore, the amount of data
    actually stored typically exceeds the notional amount stored, because
    Ceph creates replicas of the data and may also use storage capacity for
    cloning and snapshotting.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="monitor.stats">
  <title>Checking a Cluster’s Usage Stats</title>

  <para>
   To check a cluster’s data usage and data distribution among pools, you can
   use the <command>df</command> option. It is similar to Linux
   <command>df</command>. Execute the following:
  </para>

<screen>ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED
    27570M     27304M         266M          0.97
POOLS:
    NAME             ID     USED     %USED     MAX AVAIL     OBJECTS
    data             0       120         0         5064M           4
    metadata         1         0         0         5064M           0
    rbd              2         0         0         5064M           0
    hot-storage      4       134         0         4033M           2
    cold-storage     5      227k         0         5064M           1
    pool1            6         0         0         5064M           0</screen>

  <para>
   The <literal>GLOBAL</literal> section of the output provides an overview of
   the amount of storage your cluster uses for your data.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>SIZE</literal>: The overall storage capacity of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>AVAIL</literal>: The amount of free space available in the
     cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>RAW USED</literal>: The amount of raw storage used.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% RAW USED</literal>: The percentage of raw storage used. Use
     this number in conjunction with the <literal>full ratio</literal> and
     <literal>near full ratio</literal> to ensure that you are not reaching
     your cluster’s capacity. See
     <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/mon-config-ref#storage-capacit">Storage
     Capacity</link> for additional details.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The <literal>POOLS</literal> section of the output provides a list of pools
   and the notional usage of each pool. The output from this section
   <emphasis>does not</emphasis> reflect replicas, clones or snapshots. For
   example, if you store an object with 1MB of data, the notional usage will be
   1MB, but the actual usage may be 2MB or more depending on the number of
   replicas, clones and snapshots.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>NAME</literal>: The name of the pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ID</literal>: The pool ID.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: The notional amount of data stored in kilobytes,
     unless the number appends M for megabytes or G for gigabytes.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>%USED</literal>: The notional percentage of storage used per
     pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>OBJECTS</literal>: The notional number of objects stored per
     pool.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    The numbers in the POOLS section are notional. They are not inclusive of
    the number of replicas, snapshots or clones. As a result, the sum of the
    USED and %USED amounts will not add up to the RAW USED and %RAW USED
    amounts in the %GLOBAL section of the output.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitor.status">
  <title>Checking a Cluster’s Status</title>

  <para>
   To check a cluster’s status, execute the following:
  </para>

<screen>ceph status</screen>

  <para>
   or
  </para>

<screen>ceph -s</screen>

  <para>
   In interactive mode, type <command>status</command> and press
   <keycap function="enter"/>.
  </para>

<screen>ceph&gt; status</screen>

  <para>
   Ceph will print the cluster status. For example, a tiny Ceph cluster
   consisting of one monitor and two OSDs may print the following:
  </para>

<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
 </sect1>
 <sect1 xml:id="monitor.osdstatus">
  <title>Checking OSD Status</title>

  <para>
   You can check OSDs to ensure they are up and on by executing:
  </para>

<screen>ceph osd stat</screen>

  <para>
   or
  </para>

<screen>ceph osd dump</screen>

  <para>
   You can also view OSDs according to their position in the CRUSH map.
  </para>

<screen>ceph osd tree</screen>

  <para>
   Ceph will print out a CRUSH tree with a host, its OSDs, whether they are
   up and their weight.
  </para>

<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
 </sect1>
 <sect1 xml:id="monitor.monstatus">
  <title>Checking Monitor Status</title>

  <para>
   If your cluster has multiple monitors (likely), you should check the monitor
   quorum status after you start the cluster before reading and/or writing
   data. A quorum must be present when multiple monitors are running. You
   should also check monitor status periodically to ensure that they are
   running.
  </para>

  <para>
   To display the monitor map, execute the following:
  </para>

<screen>ceph mon stat</screen>

  <para>
   or
  </para>

<screen>ceph mon dump</screen>

  <para>
   To check the quorum status for the monitor cluster, execute the following:
  </para>

<screen>ceph quorum_status</screen>

  <para>
   Ceph will return the quorum status. For example, a Ceph cluster
   consisting of three monitors may return the following:
  </para>

<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6791\/0"}
           ]
    }
}</screen>
 </sect1>

 <sect1 xml:id="monitor.pgroupstatus">
  <title>Checking Placement Group States</title>

  <para>
   Placement groups map objects to OSDs. When you monitor your placement
   groups, you will want them to be <literal>active</literal> and
   <literal>clean</literal>. For a detailed discussion, refer to
   <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/monitoring-osd-pg">Monitoring
   OSDs and Placement Groups.</link>
  </para>
 </sect1>
 <sect1 xml:id="monitor.adminsocket">
  <title>Using the Admin Socket</title>

  <para>
   The Ceph admin socket allows you to query a daemon via a socket interface.
   By default, Ceph sockets reside under <filename>/var/run/ceph</filename>.
   To access a daemon via the admin socket, log in to the host running the
   daemon and use the following command:
  </para>

<screen>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>

  <para>
   To view the available admin socket commands, execute the following command:
  </para>

<screen>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>

  <para>
   The admin socket command enables you to show and set your configuration at
   runtime. Refer to <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/ceph-conf#ceph-runtime-config">Viewing
   a Configuration at Runtime</link>for details.
  </para>

  <para>
   Additionally, you can set configuration values at runtime directly (the
   admin socket bypasses the monitor, unlike <command>ceph tell</command>
   <replaceable>daemon-type</replaceable>.<replaceable>id</replaceable>
   injectargs, which relies on the monitor but does not require you to log in
   directly to the host in question).
  </para>
 </sect1>
</chapter>
  <chapter xml:base="admin_cephx.xml" version="5.0" xml:id="cha.storage.cephx">
 <title>Authentication with <systemitem class="service">cephx</systemitem></title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  To identify clients and protect against man-in-the-middle attacks, Ceph
  provides its <systemitem class="service">cephx</systemitem> authentication system. <emphasis>Clients</emphasis> in
  this context are either human users—such as the admin user—or
  Ceph-related services/daemons, for example OSDs, monitors, or <phrase>RADOS Gateway</phrase>s.
 </para>
 <note>
  <para>
   The <systemitem class="service">cephx</systemitem> protocol does not address data encryption in transport, such as
   TLS/SSL.
  </para>
 </note>
 <sect1 xml:id="storage.cephx.arch">


  <title>Authentication Architecture</title>

  <para>
   <systemitem class="service">cephx</systemitem> uses shared secret keys for authentication, meaning both the client
   and the monitor cluster have a copy of the client’s secret key. The
   authentication protocol enables both parties to prove to each other that
   they have a copy of the key without actually revealing it. This provides
   mutual authentication, which means the cluster is sure the user possesses
   the secret key, and the user is sure that the cluster has a copy of the
   secret key as well.
  </para>

  <para>
   A key scalability feature of Ceph is to avoid a centralized interface to
   the Ceph object store. This means that Ceph clients can interact with
   OSDs directly. To protect data, Ceph provides its <systemitem class="service">cephx</systemitem> authentication
   system, which authenticates Ceph clients.
  </para>

  <para>
   Each monitor can authenticate clients and distribute keys, so there is no
   single point of failure or bottleneck when using <systemitem class="service">cephx</systemitem>. The monitor
   returns an authentication data structure that contains a session key for use
   in obtaining Ceph services. This session key is itself encrypted with the
   client’s permanent secret key, so that only the client can request
   services from the Ceph monitors. The client then uses the session key to
   request its desired services from the monitor, and the monitor provides the
   client with a ticket that will authenticate the client to the OSDs that
   actually handle data. Ceph monitors and OSDs share a secret, so the client
   can use the ticket provided by the monitor with any OSD or metadata server
   in the cluster. <systemitem class="service">cephx</systemitem> tickets expire, so an attacker cannot use an expired
   ticket or session key obtained wrongfully. This form of authentication will
   prevent attackers with access to the communications medium from either
   creating bogus messages under another client’s identity or altering
   another client’s legitimate messages, as long as the client secret key is
   not revealed before it expires.
  </para>

  <para>
   To use <systemitem class="service">cephx</systemitem>, an administrator must setup clients/users first. In the
   following diagram, the
   <systemitem class="username">client.admin</systemitem> user invokes
   <command>ceph auth get-or-create-key</command> from the command line to
   generate a user name and secret key. Ceph’s <command>auth</command>
   subsystem generates the user name and key, stores a copy with the monitor(s)
   and transmits the user’s secret back to the
   <systemitem class="username">client.admin</systemitem> user. This means that
   the client and the monitor share a secret key.
  </para>

  <figure>
   <title>Basic <systemitem class="service">cephx</systemitem> Authentication</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cephx_keyring.png" width="70%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cephx_keyring.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   To authenticate with the monitor, the client passes the user name to the
   monitor. The monitor generates a session key and encrypts it with the secret
   key associated with the user name and transmits the encrypted ticket back to
   the client. The client then decrypts the data with the shared secret key to
   retrieve the session key. The session key identifies the user for the
   current session. The client then requests a ticket related to the user,
   which is signed by the session key. The monitor generates a ticket, encrypts
   it with the user’s secret key and transmits it back to the client. The
   client decrypts the ticket and uses it to sign requests to OSDs and metadata
   servers throughout the cluster.
  </para>

  <figure>
   <title><systemitem class="service">cephx</systemitem> Authentication</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cephx_keyring2.png" width="70%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cephx_keyring2.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The <systemitem class="service">cephx</systemitem> protocol authenticates ongoing communications between the client
   machine and the Ceph servers. Each message sent between a client and a
   server after the initial authentication is signed using a ticket that the
   monitors, OSDs, and metadata servers can verify with their shared secret.
  </para>

  <figure>
   <title><systemitem class="service">cephx</systemitem> Authentication - MDS and OSD</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cephx_keyring3.png" width="70%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cephx_keyring3.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <important>
   <para>
    The protection offered by this authentication is between the Ceph client
    and the Ceph cluster hosts. The authentication is not extended beyond the
    Ceph client. If the user accesses the Ceph client from a remote host,
    Ceph authentication is not applied to the connection between the user’s
    host and the client host.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="storage.cephx.keymgmt">


  <title>Key Management</title>

  <para>
   This section describes Ceph client users and their authentication and
   authorization with the Ceph storage cluster. <emphasis>Users</emphasis>
   are either individuals or system actors such as applications, which use
   Ceph clients to interact with the Ceph storage cluster daemons.
  </para>

  <para>
   When Ceph runs with authentication and authorization enabled (enabled by
   default), you must specify a user name and a keyring containing the secret
   key of the specified user (usually via the command line). If you do not
   specify a user name, Ceph will use
   <systemitem class="username">client.admin</systemitem> as the default user
   name. If you do not specify a keyring, Ceph will look for a keyring via
   the keyring setting in the Ceph configuration file. For example, if you
   execute the <command>ceph health</command> command without specifying a user
   name or keyring, Ceph interprets the command like this:
  </para>

<screen>ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health</screen>

  <para>
   Alternatively, you may use the <literal>CEPH_ARGS</literal> environment
   variable to avoid re-entering the user name and secret.
  </para>

  <sect2 xml:id="storage.cephx.keymgmt.backgrnd">
   <title>Background Information</title>
   <para>
    Regardless of the type of Ceph client (for example, block device, object
    storage, file system, native API), Ceph stores all data as objects within
    <emphasis>pools</emphasis>. Ceph users need to have access to pools in
    order to read and write data. Additionally, Ceph users must have execute
    permissions to use Ceph's administrative commands. The following concepts
    will help you understand Ceph user management.
   </para>
   <sect3>
    <title>User</title>
    <para>
     A user is either an individual or a system actor such as an application.
     Creating users allows you to control who (or what) can access your Ceph
     storage cluster, its pools, and the data within pools.
    </para>
    <para>
     Ceph uses <emphasis>types</emphasis> of users. For the purposes of user
     management, the type will always be <literal>client</literal>. Ceph
     identifies users in period (.) delimited form, consisting of the user type
     and the user ID. For example, <literal>TYPE.ID</literal>,
     <literal>client.admin</literal>, or <literal>client.user1</literal>. The
     reason for user typing is that Ceph monitors, OSDs, and metadata servers
     also use the cephx protocol, but they are not clients. Distinguishing the
     user type helps to distinguish between client users and other users,
     streamlining access control, user monitoring, and traceability.
    </para>
    <note>
     <para>
      A Ceph storage cluster user is not the same as a Ceph object storage
      user or a Ceph file system user. The Ceph <phrase>RADOS Gateway</phrase> uses a Ceph storage
      cluster user to communicate between the gateway daemon and the storage
      cluster, but the gateway has its own user management functionality for
      end users. The Ceph file system uses POSIX semantics. The user space
      associated with it is not the same as a Ceph storage cluster user.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Authorization and Capabilities</title>
    <para>
     Ceph uses the term 'capabilities' (caps) to describe authorizing an
     authenticated user to exercise the functionality of the monitors, OSDs,
     and metadata servers. Capabilities can also restrict access to data within
     a pool or a namespace within a pool. A Ceph administrative user sets a
     user's capabilities when creating or updating a user.
    </para>
    <para>
     Capability syntax follows the form:
    </para>
<screen><replaceable>daemon-type</replaceable> 'allow <replaceable>capability</replaceable>' [...]</screen>
    <para>
     Following is a list of capabilities for each service type:
    </para>
    <variablelist>
     <varlistentry>
      <term>Monitor capabilities</term>
      <listitem>
       <para>
        include <literal>r</literal>, <literal>w</literal>,
        <literal>x</literal> and <literal>allow profile
        <replaceable>cap</replaceable></literal>.
       </para>
<screen>mon 'allow rwx'
mon 'allow profile osd'</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>OSD capabilities</term>
      <listitem>
       <para>
        include <literal>r</literal>, <literal>w</literal>,
        <literal>x</literal>, <literal>class-read</literal>,
        <literal>class-write</literal> and <literal>profile osd</literal>.
        Additionally, OSD capabilities also allow for pool and namespace
        settings.
       </para>
<screen>osd 'allow <replaceable>capability</replaceable>' [pool=<replaceable>poolname</replaceable>] [namespace=<replaceable>namespace-name</replaceable>]</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>MDS capability</term>
      <listitem>
       <para>
        simply requires <literal>allow</literal>, or blank.
       </para>
<screen>mds 'allow'</screen>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     The following entries describe each capability:
    </para>
    <variablelist>
     <varlistentry>
      <term>allow</term>
      <listitem>
       <para>
        Precedes access settings for a daemon. Implies <literal>rw</literal>
        for MDS only.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>r</term>
      <listitem>
       <para>
        Gives the user read access. Required with monitors to retrieve the
        CRUSH map.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>w</term>
      <listitem>
       <para>
        Gives the user write access to objects.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>x</term>
      <listitem>
       <para>
        Gives the user the capability to call class methods (both read and
        write) and to conduct <literal>auth</literal> operations on monitors.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>class-read</term>
      <listitem>
       <para>
        Gives the user the capability to call class read methods. Subset of
        <literal>x</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>class-write</term>
      <listitem>
       <para>
        Gives the user the capability to call class write methods. Subset of
        <literal>x</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>*</term>
      <listitem>
       <para>
        Gives the user read, write, and execute permissions for a particular
        daemon/pool, and the ability to execute admin commands.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>profile osd</term>
      <listitem>
       <para>
        Gives a user permissions to connect as an OSD to other OSDs or
        monitors. Conferred on OSDs to enable OSDs to handle replication
        heartbeat traffic and status reporting.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>profile mds</term>
      <listitem>
       <para>
        Gives a user permissions to connect as an MDS to other MDSs or
        monitors.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>profile bootstrap-osd</term>
      <listitem>
       <para>
        Gives a user permissions to bootstrap an OSD. Delegated to deployment
        tools such as <command>ceph-disk</command>,
        <command>ceph-deploy</command> so that they have permissions to add
        keys when bootstrapping an OSD.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>profile bootstrap-mds</term>
      <listitem>
       <para>
        Gives a user permissions to bootstrap a metadata server. Delegated to
        deployment tools such as ceph-deploy so they have permissions to add
        keys when bootstrapping a metadata server.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3>
    <title>Pools</title>
    <para>
     A pool is a logical partition where users store data. In Ceph
     deployments, it is common to create a pool as a logical partition for
     similar types of data. For example, when deploying Ceph as a back-end
     for <phrase>OpenStack</phrase>, a typical deployment would have pools for volumes, images,
     backups and virtual machines, and users such as
     <systemitem class="username">client.glance</systemitem> or
     <systemitem class="username">client.cinder</systemitem>.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="storage.cephx.keymgmt.usermgmt">
   <title>Managing Users</title>
   <para>
    User management functionality provides Ceph cluster administrators with
    the ability to create, update, and delete users directly in the Ceph
    cluster.
   </para>
   <para>
    When you create or delete users in the Ceph cluster, you may need to
    distribute keys to clients so that they can be added to keyrings. See
    <xref linkend="storage.cephx.keymgmt.keyringmgmt" role="internalbook"/> for details.
   </para>
   <sect3>
    <title>Listing Users</title>
    <para>
     To list the users in your cluster, execute the following:
    </para>
<screen>ceph auth list</screen>
    <para>
     Ceph will list all users in your cluster. For example, in a cluster with
     two nodes, <command>ceph auth list</command> output looks similar to this:
    </para>
<screen>installed auth entries:

osd.0
        key: AQCvCbtToC6MDhAATtuT70Sl+DymPCfDSsyV4w==
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.1
        key: AQC4CbtTCFJBChAAVq5spj0ff4eHZICxIOVZeA==
        caps: [mon] allow profile osd
        caps: [osd] allow *
client.admin
        key: AQBHCbtT6APDHhAA5W00cBchwkQjh3dkKsyPjw==
        caps: [mds] allow
        caps: [mon] allow *
        caps: [osd] allow *
client.bootstrap-mds
        key: AQBICbtTOK9uGBAAdbe5zcIGHZL3T/u2g6EBww==
        caps: [mon] allow profile bootstrap-mds
client.bootstrap-osd
        key: AQBHCbtT4GxqORAADE5u7RkpCN/oo4e5W0uBtw==
        caps: [mon] allow profile bootstrap-osd</screen>
    <note>
     <title>TYPE.ID Notation</title>
     <para>
      Note that the <literal>TYPE.ID</literal> notation for users applies such
      that <literal>osd.0</literal> specifies a user of type
      <literal>osd</literal> and its ID is <literal>0</literal>.
      <literal>client.admin</literal> is a user of type
      <literal>client</literal> and its ID is <literal>admin</literal>. Note
      also that each entry has a <literal>key:
      <replaceable>value</replaceable></literal> entry, and one or more
      <literal>caps:</literal> entries.
     </para>
     <para>
      You may use the <option>-o <replaceable>filename</replaceable></option>
      option with <command>ceph auth list</command> to save the output to a
      file.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Getting Information about Users</title>
    <para>
     To retrieve a specific user, key, and capabilities, execute the following:
    </para>
<screen>ceph auth get <replaceable>TYPE.ID</replaceable></screen>
    <para>
     For example:
    </para>
<screen>ceph auth get client.admin
exported keyring for client.admin
[client.admin]
	key = AQA19uZUqIwkHxAAFuUwvq0eJD4S173oFRxe0g==
	caps mds = "allow"
	caps mon = "allow *"
 caps osd = "allow *"</screen>
    <para>
     Developers may also execute the following:
    </para>
<screen>ceph auth export <replaceable>TYPE.ID</replaceable></screen>
    <para>
     The <command>auth export</command> command is identical to <command>auth
     get</command>, but also prints the internal authentication ID.
    </para>
   </sect3>
   <sect3 xml:id="storage.cephx.keymgmt.usermgmt.useradd">
    <title>Adding Users</title>
    <para>
     Adding a user creates a user name (<literal>TYPE.ID</literal>), a secret
     key, and any capabilities included in the command you use to create the
     user.
    </para>
    <para>
     A user's key enables the user to authenticate with the Ceph storage
     cluster. The user's capabilities authorize the user to read, write, or
     execute on Ceph monitors (mon), Ceph OSDs (osd), or Ceph metadata
     servers (mds).
    </para>
    <para>
     There are a few commands available to add a user:
    </para>
    <variablelist>
     <varlistentry>
      <term><command>ceph auth add</command>
      </term>
      <listitem>
       <para>
        This command is the canonical way to add a user. It will create the
        user, generate a key, and add any specified capabilities.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><command>ceph auth get-or-create</command>
      </term>
      <listitem>
       <para>
        This command is often the most convenient way to create a user, because
        it returns a keyfile format with the user name (in brackets) and the
        key. If the user already exists, this command simply returns the user
        name and key in the keyfile format. You may use the <option>-o
        <replaceable>filename</replaceable></option> option to save the output
        to a file.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><command>ceph auth get-or-create-key</command>
      </term>
      <listitem>
       <para>
        This command is a convenient way to create a user and return the user's
        key (only). This is useful for clients that need the key only (for
        example <systemitem class="library">libvirt</systemitem>). If the user already exists, this command simply
        returns the key. You may use the <option>-o
        <replaceable>filename</replaceable></option> option to save the output
        to a file.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     When creating client users, you may create a user with no capabilities. A
     user with no capabilities can authenticate but nothing more. Such client
     cannot retrieve the cluster map from the monitor. However, you can create
     a user with no capabilities if you want to defer adding capabilities later
     using the <command>ceph auth caps</command> command.
    </para>
    <para>
     A typical user has at least read capabilities on the Ceph monitor and
     read and write capabilities on Ceph OSDs. Additionally, a user's OSD
     permissions are often restricted to accessing a particular pool.
    </para>
<screen>ceph auth add client.john mon 'allow r' osd \
 'allow rw pool=liverpool'
ceph auth get-or-create client.paul mon 'allow r' osd \
 'allow rw pool=liverpool'
ceph auth get-or-create client.george mon 'allow r' osd \
 'allow rw pool=liverpool' -o george.keyring
ceph auth get-or-create-key client.ringo mon 'allow r' osd \
 'allow rw pool=liverpool' -o ringo.key</screen>
    <important>
     <para>
      If you provide a user with capabilities to OSDs, but you <emphasis>do
      not</emphasis> restrict access to particular pools, the user will have
      access to <emphasis>all</emphasis> pools in the cluster.
     </para>
    </important>
   </sect3>
   <sect3>
    <title>Modifying User Capabilities</title>
    <para>
     The <command>ceph auth caps</command> command allows you to specify a user
     and change the user's capabilities. Setting new capabilities will
     overwrite current ones. To view current capabilities run <command>ceph
     auth get
     <replaceable>USERTYPE</replaceable>.<replaceable>USERID</replaceable></command>.
     To add capabilities, you also need to specify the existing capabilities
     when using the following form:
    </para>
<screen>ceph auth caps <replaceable>USERTYPE</replaceable>.<replaceable>USERID</replaceable> <replaceable>daemon</replaceable> 'allow [r|w|x|*|...] \
     [pool=<replaceable>pool-name</replaceable>] [namespace=<replaceable>namespace-name</replaceable>]' [<replaceable>daemon</replaceable> 'allow [r|w|x|*|...] \
     [pool=<replaceable>pool-name</replaceable>] [namespace=<replaceable>namespace-name</replaceable>]']</screen>
    <para>
     For example:
    </para>
<screen>ceph auth get client.john
ceph auth caps client.john mon 'allow r' osd 'allow rw pool=prague'
ceph auth caps client.paul mon 'allow rw' osd 'allow rwx pool=prague'
ceph auth caps client.brian-manager mon 'allow *' osd 'allow *'</screen>
    <para>
     To remove a capability, you may reset the capability. If you want the user
     to have no access to a particular daemon that was previously set, specify
     an empty string:
    </para>
<screen>ceph auth caps client.ringo mon ' ' osd ' '</screen>
   </sect3>
   <sect3>
    <title>Deleting Users</title>
    <para>
     To delete a user, use <command>ceph auth del</command>:
    </para>
<screen>ceph auth del <replaceable>TYPE</replaceable>.<replaceable>ID</replaceable></screen>
    <para>
     where <replaceable>TYPE</replaceable> is one of <literal>client</literal>,
     <literal>osd</literal>, <literal>mon</literal>, or <literal>mds</literal>,
     and <replaceable>ID</replaceable> is the user name or ID of the daemon.
    </para>
   </sect3>
   <sect3>
    <title>Printing a User's Key</title>
    <para>
     To print a user’s authentication key to standard output, execute the
     following:
    </para>
<screen>ceph auth print-key <replaceable>TYPE</replaceable>.<replaceable>ID</replaceable></screen>
    <para>
     where <replaceable>TYPE</replaceable> is one of <literal>client</literal>,
     <literal>osd</literal>, <literal>mon</literal>, or <literal>mds</literal>,
     and <replaceable>ID</replaceable> is the user name or ID of the daemon.
    </para>
    <para>
     Printing a user's key is useful when you need to populate client software
     with a user's key (such as <systemitem class="library">libvirt</systemitem>), as in the following example:
    </para>
<screen>mount -t ceph host:/ mount_point \
-o name=client.user,secret=`ceph auth print-key client.user`</screen>
   </sect3>
   <sect3 xml:id="storage.cephx.keymgmt.usermgmt.userimp">
    <title>Importing Users</title>
    <para>
     To import one or more users, use <command>ceph auth import</command> and
     specify a keyring:
    </para>
<screen>sudo ceph auth import -i /etc/ceph/ceph.keyring</screen>
    <note>
     <para>
      The Ceph storage cluster will add new users, their keys and their
      capabilities and will update existing users, their keys and their
      capabilities.
     </para>
    </note>
   </sect3>
  </sect2>

  <sect2 xml:id="storage.cephx.keymgmt.keyringmgmt">
   <title>Keyring Management</title>
   <para>
    When you access Ceph via a Ceph client, the client will look for a
    local keyring. Ceph presets the keyring setting with the following four
    keyring names by default so you do not need to set them in your Ceph
    configuration file unless you want to override the defaults:
   </para>
<screen>/etc/ceph/<replaceable>cluster</replaceable>.<replaceable>name</replaceable>.keyring
/etc/ceph/<replaceable>cluster</replaceable>.keyring
/etc/ceph/keyring
/etc/ceph/keyring.bin</screen>
   <para>
    The <replaceable>cluster</replaceable> metavariable is your Ceph cluster
    name as defined by the name of the Ceph configuration file.
    <filename>ceph.conf</filename> means that the cluster name is
    <literal>ceph</literal>, thus <literal>ceph.keyring</literal>. The
    <replaceable>name</replaceable> metavariable is the user type and user ID,
    for example <literal>client.admin</literal>, thus
    <literal>ceph.client.admin.keyring</literal>.
   </para>
   <para>
    After you create a user (for example
    <systemitem class="username">client.ringo</systemitem>), you must get the
    key and add it to a keyring on a Ceph client so that the user can access
    the Ceph storage cluster.
   </para>
   <para>
    <xref linkend="storage.cephx.keymgmt" role="internalbook"/> details how to list, get, add,
    modify and delete users directly in the Ceph storage cluster. However,
    Ceph also provides the <command>ceph-authtool</command> utility to allow
    you to manage keyrings from a Ceph client.
   </para>
   <sect3>
    <title>Creating a Keyring</title>
    <para>
     When you use the procedures in <xref linkend="storage.cephx.keymgmt" role="internalbook"/> to
     create users, you need to provide user keys to the Ceph client(s) so
     that the client can retrieve the key for the specified user and
     authenticate with the Ceph storage cluster. Ceph clients access
     keyrings to look up a user name and retrieve the user's key:
    </para>
<screen>sudo ceph-authtool --create-keyring /path/to/keyring</screen>
    <para>
     When creating a keyring with multiple users, we recommend using the
     cluster name (for example <replaceable>cluster</replaceable>.keyring) for
     the keyring file name and saving it in the <filename>/etc/ceph</filename>
     directory so that the keyring configuration default setting will pick up
     the file name without requiring you to specify it in the local copy of
     your Ceph configuration file. For example, create
     <filename>ceph.keyring</filename> by executing the following:
    </para>
<screen>sudo ceph-authtool -C /etc/ceph/ceph.keyring</screen>
    <para>
     When creating a keyring with a single user, we recommend using the cluster
     name, the user type and the user name and saving it in the
     <filename>/etc/ceph</filename> directory. For example,
     <filename>ceph.client.admin.keyring</filename> for the
     <systemitem class="username">client.admin</systemitem> user.
    </para>
   </sect3>
   <sect3>
    <title>Adding a User to a Keyring</title>
    <para>
     When you add a user to the Ceph storage cluster (see
     <xref linkend="storage.cephx.keymgmt.usermgmt.useradd" role="internalbook"/>), you can
     retrieve the user, key and capabilities, and save the user to a keyring.
    </para>
    <para>
     If you only want to use one user per keyring, the <command>ceph auth
     get</command> command with the <option>-o</option> option will save the
     output in the keyring file format. For example, to create a keyring for
     the <systemitem class="username">client.admin</systemitem> user, execute
     the following:
    </para>
<screen>ceph auth get client.admin -o /etc/ceph/ceph.client.admin.keyring</screen>
    <para>
     When you want to import users to a keyring, you can use
     <command>ceph-authtool</command> to specify the destination keyring and
     the source keyring:
    </para>
<screen>sudo ceph-authtool /etc/ceph/ceph.keyring \
  --import-keyring /etc/ceph/ceph.client.admin.keyring</screen>
   </sect3>
   <sect3>
    <title>Creating a User</title>
    <para>
     Ceph provides the <command>ceph auth add</command> command to create a
     user directly in the Ceph storage cluster. However, you can also create
     a user, keys and capabilities directly on a Ceph client keyring. Then,
     you can import the user to the Ceph storage cluster:
    </para>
<screen>sudo ceph-authtool -n client.ringo --cap osd 'allow rwx' \
  --cap mon 'allow rwx' /etc/ceph/ceph.keyring</screen>
    <para>
     You can also create a keyring and add a new user to the keyring
     simultaneously:
    </para>
<screen>sudo ceph-authtool -C /etc/ceph/ceph.keyring -n client.ringo \
  --cap osd 'allow rwx' --cap mon 'allow rwx' --gen-key</screen>
    <para>
     In the previous scenarios, the new user
     <systemitem class="username">client.ringo</systemitem> is only in the
     keyring. To add the new user to the Ceph storage cluster, you must still
     add the new user to the cluster:
    </para>
<screen>sudo ceph auth add client.ringo -i /etc/ceph/ceph.keyring</screen>
   </sect3>
   <sect3>
    <title>Modifying Users</title>
    <para>
     To modify the capabilities of a user record in a keyring, specify the
     keyring and the user followed by the capabilities:
    </para>
<screen>sudo ceph-authtool /etc/ceph/ceph.keyring -n client.ringo \
  --cap osd 'allow rwx' --cap mon 'allow rwx'</screen>
    <para>
     To update the modified user within the Ceph cluster environment, you
     must import the changes from the keyring to the user entry in the Ceph
     cluster:
    </para>
<screen>ceph auth import -i /etc/ceph/ceph.keyring</screen>
    <para>
     See <xref linkend="storage.cephx.keymgmt.usermgmt.userimp" role="internalbook"/> for details
     on updating a Ceph storage cluster user from a keyring.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="storage.cephx.keymgmt.cmdline">
   <title>Command Line Usage</title>
   <para>
    The <command>ceph</command> command supports the following options related
    to the user name and secret manipulation:
   </para>
   <variablelist>
    <varlistentry>
     <term><option>--id</option> or <option>--user</option>
     </term>
     <listitem>
      <para>
       Ceph identifies users with a type and an ID
       (<replaceable>TYPE</replaceable>.<replaceable>ID</replaceable>, such as
       <systemitem class="username">client.admin</systemitem> or
       <systemitem class="username">client.user1</systemitem>). The
       <option>id</option>, <option>name</option> and <option>-n</option>
       options enable you to specify the ID portion of the user name (for
       example <systemitem class="username">admin</systemitem> or
       <systemitem class="username">user1</systemitem>). You can specify the
       user with the --id and omit the type. For example, to specify user
       client.foo enter the following:
      </para>
<screen>ceph --id foo --keyring /path/to/keyring health
ceph --user foo --keyring /path/to/keyring health</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--name</option> or <option>-n</option>
     </term>
     <listitem>
      <para>
       Ceph identifies users with a type and an ID
       (<replaceable>TYPE</replaceable>.<replaceable>ID</replaceable>, such as
       <systemitem class="username">client.admin</systemitem> or
       <systemitem class="username">client.user1</systemitem>). The
       <option>--name</option> and <option>-n</option> options enable you to
       specify the fully qualified user name. You must specify the user type
       (typically <literal>client</literal>) with the user ID:
      </para>
<screen>ceph --name client.foo --keyring /path/to/keyring health
ceph -n client.foo --keyring /path/to/keyring health</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--keyring</option>
     </term>
     <listitem>
      <para>
       The path to the keyring containing one or more user name and secret. The
       <option>--secret</option> option provides the same functionality, but it
       does not work with <phrase>RADOS Gateway</phrase>, which uses <option>--secret</option> for
       another purpose. You may retrieve a keyring with <command>ceph auth
       get-or-create</command> and store it locally. This is a preferred
       approach, because you can switch user names without switching the
       keyring path:
      </para>
<screen>sudo rbd map --id foo --keyring /path/to/keyring mypool/myimage</screen>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
  <chapter xml:base="admin_ceph_datamgm.xml" version="5.0" xml:id="cha.storage.datamgm">
 <title>Stored Data Management</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The CRUSH algorithm determines how to store and retrieve data by computing
  data storage locations. CRUSH empowers Ceph clients to communicate with
  OSDs directly rather than through a centralized server or broker. With an
  algorithmically determined method of storing and retrieving data, Ceph
  avoids a single point of failure, a performance bottleneck, and a physical
  limit to its scalability.
 </para>
 <para>
  CRUSH requires a map of your cluster, and uses the CRUSH map to
  pseudo-randomly store and retrieve data in OSDs with a uniform distribution
  of data across the cluster.
 </para>
 <para>
  CRUSH maps contain a list of OSDs, a list of ‘buckets’ for aggregating
  the devices into physical locations, and a list of rules that tell CRUSH how
  it should replicate data in a Ceph cluster’s pools. By reﬂecting the
  underlying physical organization of the installation, CRUSH can model—and
  thereby address—potential sources of correlated device failures. Typical
  sources include physical proximity, a shared power source, and a shared
  network. By encoding this information into the cluster map, CRUSH placement
  policies can separate object replicas across different failure domains while
  still maintaining the desired distribution. For example, to address the
  possibility of concurrent failures, it may be desirable to ensure that data
  replicas are on devices using different shelves, racks, power supplies,
  controllers, and/or physical locations.
 </para>
 <para>
  When you create a configuration file and deploy Ceph with
  <command>ceph-deploy</command>, Ceph generates a default CRUSH map for your
  configuration. The default CRUSH map is fine for your Ceph sandbox
  environment. However, when you deploy a large-scale data cluster, you should
  give significant consideration to developing a custom CRUSH map, because it
  will help you manage your Ceph cluster, improve performance and ensure data
  safety.
 </para>
 <para>
  For example, if an OSD goes down, a CRUSH map can help you locate the
  physical data center, room, row and rack of the host with the failed OSD in
  the event you need to use on-site support or replace hardware.
 </para>
 <para>
  Similarly, CRUSH may help you identify faults more quickly. For example, if
  all OSDs in a particular rack go down simultaneously, the fault may lie with
  a network switch or power to the rack or the network switch rather than the
  OSDs themselves.
 </para>
 <para>
  A custom CRUSH map can also help you identify the physical locations where
  Ceph stores redundant copies of data when the placement group(s) associated
  with a failed host are in a degraded state.
 </para>
 <para>
  There are three main sections to a CRUSH Map.
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <xref linkend="datamgm.devices" xrefstyle="select: title" role="internalbook"/> consist of any
    object storage device, that is, the hard disk corresponding to a
    <systemitem>ceph-osd</systemitem> daemon.
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm.buckets" xrefstyle="select: title" role="internalbook"/> consist of a
    hierarchical aggregation of storage locations (for example rows, racks,
    hosts, etc.) and their assigned weights.
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm.rules" xrefstyle="select: title" role="internalbook"/> consist of the
    manner of selecting buckets.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="datamgm.devices">
  <title>Devices</title>

  <para>
   To map placement groups to OSDs, a CRUSH Map requires a list of OSD devices
   (the name of the OSD daemon). The list of devices appears first in the
   CRUSH Map.
  </para>

<screen>#devices
device <replaceable>num</replaceable> <replaceable>osd.name</replaceable></screen>

  <para>
   For example:
  </para>

<screen>#devices
device 0 osd.0
device 1 osd.1
device 2 osd.2
device 3 osd.3</screen>

  <para>
   As a general rule, an OSD daemon maps to a single disk.
  </para>
 </sect1>
 <sect1 xml:id="datamgm.buckets">
  <title>Buckets</title>

  <para>
   CRUSH maps contain a list of OSDs, which can be organized into 'buckets' for
   aggregating the devices into physical locations.
  </para>

  <informaltable frame="none">
   <tgroup cols="3">
    <colspec colwidth="1in"/>
    <colspec colwidth="3in"/>
    <colspec colwidth="7in"/>
    <tbody>
     <row>
      <entry>
       <para>
        0
       </para>
      </entry>
      <entry>
       <para>
        OSD
       </para>
      </entry>
      <entry>
       <para>
        An OSD daemon (osd.1, osd.2, etc.).
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        1
       </para>
      </entry>
      <entry>
       <para>
        Host
       </para>
      </entry>
      <entry>
       <para>
        A host name containing one or more OSDs.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        2
       </para>
      </entry>
      <entry>
       <para>
        Chassis
       </para>
      </entry>
      <entry>
       <para>
        Chassis of which the rack is composed.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        3
       </para>
      </entry>
      <entry>
       <para>
        Rack
       </para>
      </entry>
      <entry>
       <para>
        A computer rack. The default is <literal>unknownrack</literal>.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        4
       </para>
      </entry>
      <entry>
       <para>
        Row
       </para>
      </entry>
      <entry>
       <para>
        A row in a series of racks.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        5
       </para>
      </entry>
      <entry>
       <para>
        Pdu
       </para>
      </entry>
      <entry>
       <para>
        Power distribution unit.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        6
       </para>
      </entry>
      <entry>
       <para>
        Pod
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        7
       </para>
      </entry>
      <entry>
       <para>
        Room
       </para>
      </entry>
      <entry>
       <para>
        A room containing racks and rows of hosts.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        8
       </para>
      </entry>
      <entry>
       <para>
        Data Center
       </para>
      </entry>
      <entry>
       <para>
        A physical data center containing rooms.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        9
       </para>
      </entry>
      <entry>
       <para>
        Region
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        10
       </para>
      </entry>
      <entry>
       <para>
        Root
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>

  <tip>
   <para>
    You can remove these types and create your own bucket types.
   </para>
  </tip>

  <para>
   Ceph’s deployment tools generate a CRUSH map that contains a bucket for
   each host, and a pool named 'default', which is useful for the default
   <literal>rbd</literal> pool. The remaining bucket types provide a means for
   storing information about the physical location of nodes/buckets, which
   makes cluster administration much easier when OSDs, hosts, or network
   hardware malfunction and the administrator needs access to physical
   hardware.
  </para>

  <para>
   A bucket has a type, a unique name (string), a unique ID expressed as a
   negative integer, a weight relative to the total capacity/capability of its
   item(s), the bucket algorithm ( <literal>straw</literal> by default), and
   the hash (<literal>0</literal> by default, reflecting CRUSH Hash
   <literal>rjenkins1</literal>). A bucket may have one or more items. The
   items may consist of other buckets or OSDs. Items may have a weight that
   reflects the relative weight of the item.
  </para>

<screen>[bucket-type] [bucket-name] {
  id [a unique negative numeric ID]
  weight [the relative capacity/capability of the item(s)]
  alg [the bucket type: uniform | list | tree | straw ]
  hash [the hash type: 0 by default]
  item [item-name] weight [weight]
}</screen>

  <para>
   The following example illustrates how you can use buckets to aggregate a
   pool and physical locations like a data center, a room, a rack and a row.
  </para>

<screen>host ceph-osd-server-1 {
        id -17
        alg straw
        hash 0
        item osd.0 weight 1.00
        item osd.1 weight 1.00
}

row rack-1-row-1 {
        id -16
        alg straw
        hash 0
        item ceph-osd-server-1 weight 2.00
}

rack rack-3 {
        id -15
        alg straw
        hash 0
        item rack-3-row-1 weight 2.00
        item rack-3-row-2 weight 2.00
        item rack-3-row-3 weight 2.00
        item rack-3-row-4 weight 2.00
        item rack-3-row-5 weight 2.00
}

rack rack-2 {
        id -14
        alg straw
        hash 0
        item rack-2-row-1 weight 2.00
        item rack-2-row-2 weight 2.00
        item rack-2-row-3 weight 2.00
        item rack-2-row-4 weight 2.00
        item rack-2-row-5 weight 2.00
}

rack rack-1 {
        id -13
        alg straw
        hash 0
        item rack-1-row-1 weight 2.00
        item rack-1-row-2 weight 2.00
        item rack-1-row-3 weight 2.00
        item rack-1-row-4 weight 2.00
        item rack-1-row-5 weight 2.00
}

room server-room-1 {
        id -12
        alg straw
        hash 0
        item rack-1 weight 10.00
        item rack-2 weight 10.00
        item rack-3 weight 10.00
}

datacenter dc-1 {
        id -11
        alg straw
        hash 0
        item server-room-1 weight 30.00
        item server-room-2 weight 30.00
}

pool data {
        id -10
        alg straw
        hash 0
        item dc-1 weight 60.00
        item dc-2 weight 60.00
}</screen>
 </sect1>
 <sect1 xml:id="datamgm.rules">
  <title>Rule Sets</title>

  <para>
   CRUSH maps support the notion of 'CRUSH rules', which are the rules that
   determine data placement for a pool. For large clusters, you will likely
   create many pools where each pool may have its own CRUSH ruleset and rules.
   The default CRUSH map has a rule for each pool, and one ruleset assigned to
   each of the default pools.
  </para>

  <note>
   <para>
    In most cases, you will not need to modify the default rules. When you
    create a new pool, its default ruleset is 0.
   </para>
  </note>

  <para>
   A rule takes the following form:
  </para>

<screen>rule <replaceable>rulename</replaceable> {

        ruleset <replaceable>ruleset</replaceable>
        type <replaceable>type</replaceable>
        min_size <replaceable>min-size</replaceable>
        max_size <replaceable>max-size</replaceable>
        step <replaceable>step</replaceable>

}</screen>

  <variablelist>
   <varlistentry>
    <term>ruleset</term>
    <listitem>
     <para>
      An integer. Classifies a rule as belonging to a set of rules. Activated
      by setting the ruleset in a pool. This option is required. Default is
      <literal>0</literal>.
     </para>
     <important>
      <para>
       You need to increase the ruleset number from the default 0 continuously,
       otherwise the related monitor may crash.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>type</term>
    <listitem>
     <para>
      A string. Describes a rule for either a hard disk (replicated) or a RAID.
      This option is required. Default is <literal>replicated</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>min_size</term>
    <listitem>
     <para>
      An integer. If a placement group makes fewer replicas than this number,
      CRUSH will NOT select this rule. This option is required. Default is
      <literal>2</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>max_size</term>
    <listitem>
     <para>
      An integer. If a placement group makes more replicas than this number,
      CRUSH will NOT select this rule. This option is required. Default is
      <literal>10</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step take <replaceable>bucket</replaceable>
    </term>
    <listitem>
     <para>
      Takes a bucket name, and begins iterating down the tree. This option is
      required.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step choose firstn <replaceable>num</replaceable> type <replaceable>bucket-type</replaceable>
    </term>
    <listitem>
     <para>
      Selects the number of buckets of the given type. Where N is the number of
      options available, if <replaceable>num</replaceable> &gt; 0 &amp;&amp;
      &lt; N, choose that many buckets; if <replaceable>num</replaceable> &lt;
      0, it means N - <replaceable>num</replaceable>; and, if
      <replaceable>num</replaceable> == 0, choose N buckets (all available).
      Follows <literal>step take</literal> or <literal>step choose</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step emit</term>
    <listitem>
     <para>
      Outputs the current value and empties the stack. Typically used at the
      end of a rule, but may also be used to form different trees in the same
      rule. Follows <literal>step choose</literal>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    To activate one or more rules with a common ruleset number to a pool, set
    the ruleset number to the pool.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="op.crush">
  <title>CRUSH Map Manipulation</title>

  <para>
   This section introduces ways to basic CRUSH Map manipulation, such as
   editing a CRUSH Map, changing CRUSH Map parameters, and
   adding/moving/removing an OSD.
  </para>

  <sect2>
   <title>Editing a CRUSH Map</title>
   <para>
    To edit an existing CRUSH map, do the following:
   </para>
   <procedure>
    <step>
     <para>
      Get a CRUSH Map. To get the CRUSH Map for your cluster, execute the
      following:
     </para>
<screen>ceph osd getcrushmap -o
<replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      Ceph will output (<option>-o</option>) a compiled CRUSH Map to the
      file name you specified. Since the CRUSH Map is in a compiled form, you
      must decompile it first before you can edit it.
     </para>
    </step>
    <step>
     <para>
      Decompile a CRUSH Map. To decompile a CRUSH Map, execute the following:
     </para>
<screen>crushtool -d <replaceable>compiled-crushmap-filename</replaceable> \
 -o <replaceable>decompiled-crushmap-filename</replaceable></screen>
     <para>
      Ceph will decompile (<option>-d</option>) the compiled CRUSH map and
      output (<option>-o</option>) it to the file name you specified.
     </para>
    </step>
    <step>
     <para>
      Edit at least one of Devices, Buckets and Rules parameters.
     </para>
    </step>
    <step>
     <para>
      Compile a CRUSH Map. To compile a CRUSH Map, execute the following:
     </para>
<screen>crushtool -c <replaceable>decompiled-crush-map-filename</replaceable> \
 -o <replaceable>compiled-crush-map-filename</replaceable></screen>
     <para>
      Ceph will store a compiled CRUSH map to the file name you specified.
     </para>
    </step>
    <step>
     <para>
      Set a CRUSH Map. To set the CRUSH Map for your cluster, execute the
      following:
     </para>
<screen>ceph osd setcrushmap -i <replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      Ceph will input the compiled CRUSH Map of the file name you specified
      as the CRUSH Map for the cluster.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="op.crush.addosd">
   <title>Add/Move an OSD</title>
   <para>
    To add or move an OSD in the CRUSH map of a running cluster, execute the
    following:
   </para>
<screen>ceph osd crush set <replaceable>id_or_name</replaceable> <replaceable>weight</replaceable> root=<replaceable>pool-name</replaceable>
<replaceable>bucket-type</replaceable>=<replaceable>bucket-name</replaceable> ...</screen>
   <variablelist>
    <varlistentry>
     <term>id</term>
     <listitem>
      <para>
       An integer. The numeric ID of the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       A string. The full name of the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>weight</term>
     <listitem>
      <para>
       A double. The CRUSH weight for the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pool</term>
     <listitem>
      <para>
       A key/value pair. By default, the CRUSH hierarchy contains the pool
       default as its root. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bucket-type</term>
     <listitem>
      <para>
       Key/value pairs. You may specify the OSD’s location in the CRUSH
       hierarchy.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The following example adds <literal>osd.0</literal> to the hierarchy, or
    moves the OSD from a previous location.
   </para>
<screen>ceph osd crush set osd.0 1.0 root=data datacenter=dc1 room=room1 \
row=foo rack=bar host=foo-bar-1</screen>
  </sect2>

  <sect2 xml:id="op.crush.osdweight">
   <title>Adjust an OSD’s CRUSH Weight</title>
   <para>
    To adjust an OSD’s crush weight in the CRUSH map of a running cluster,
    execute the following:
   </para>
<screen>ceph osd crush reweight <replaceable>name</replaceable> <replaceable>weight</replaceable></screen>
   <variablelist>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       A string. The full name of the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>weight</term>
     <listitem>
      <para>
       A double. The CRUSH weight for the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op.crush.osdremove">
   <title>Remove an OSD</title>
   <para>
    To remove an OSD from the CRUSH map of a running cluster, execute the
    following:
   </para>
<screen>ceph osd crush remove <replaceable>name</replaceable></screen>
   <variablelist>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       A string. The full name of the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op.crush.movebucket">
   <title>Move a Bucket</title>
   <para>
    To move a bucket to a different location or position in the CRUSH map
    hierarchy, execute the following:
   </para>
<screen>ceph osd crush move <replaceable>bucket-name</replaceable> <replaceable>bucket-type</replaceable>=<replaceable>bucket-name</replaceable>, ...</screen>
   <variablelist>
    <varlistentry>
     <term>bucket-name</term>
     <listitem>
      <para>
       A string. The name of the bucket to move/reposition. This option is
       required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bucket-type</term>
     <listitem>
      <para>
       Key/value pairs. You may specify the bucket’s location in the CRUSH
       hierarchy.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="op.mixed_ssd_hdd">
  <title>Mixed SSDs and HDDs on the Same Node</title>

  <para>
   It can be desirable to configure a Ceph cluster such that each node has a
   mix of SSDs and HDDs, with one storage pool on the fast SSDs and one storage
   pool on the slower HDDs. To do this, the CRUSH Map needs to be edited.
  </para>

  <para>
   The default CRUSH Map will have a simple hierarchy, where the default root
   contains hosts, and the hosts contain OSDs, for example:
  </para>

<screen><prompt>cephadm &gt; </prompt>ceph osd tree
ID  WEIGHT  TYPE NAME         UP/DOWN REWEIGHT
 -1 0.18494 root default
 -2 0.05548     host node1
  0 0.01849         osd.0          up 1.00000
  3 0.01849         osd.3          up 1.00000
  6 0.01849         osd.6          up 1.00000
 -3 0.05548     host node2
  1 0.01849         osd.1          up 1.00000
  4 0.01849         osd.4          up 1.00000
  7 0.01849         osd.7          up 1.00000
 -4 0.05548     host node3
  2 0.01849         osd.2          up 1.00000
  5 0.01849         osd.5          up 1.00000
  8 0.01849         osd.8          up 1.00000</screen>

  <para>
   This provides no distinction between disk types. In order to split the OSDs
   into SSDs and HDDs, we need to create a second hierarchy in the CRUSH Map:
  </para>

<screen><prompt>cephadm &gt; </prompt>ceph osd crush add-bucket ssd root</screen>

  <para>
   Having created the new root for SSDs, we need to add hosts to it. This means
   creating new host entries. But because the same host name cannot appear more
   than once in a CRUSH Map, this uses fake host names. These fake host names
   do not need to be resolvable by DNS. CRUSH does not care what the host names
   are, they only need to create the right hierarchies. The one thing that
   <emphasis>does</emphasis> need to be changed in order to support fake host
   names is that you must set
  </para>

<screen>osd crush update on start = false</screen>

  <para>
   in <filename>/etc/ceph/ceph.conf</filename>. Otherwise the OSDs you move
   will be reset later to their original location in the default root, and the
   cluster will not behave as expected.
  </para>

  <para>
   Once that setting is changed, add the new fake hosts to the SSD root:
  </para>

<screen><prompt>cephadm &gt; </prompt>ceph osd crush add-bucket node1-ssd host
<prompt>cephadm &gt; </prompt>ceph osd crush move node1-ssd root=ssd
<prompt>cephadm &gt; </prompt>ceph osd crush add-bucket node2-ssd host
<prompt>cephadm &gt; </prompt>ceph osd crush move node2-ssd root=ssd
<prompt>cephadm &gt; </prompt>ceph osd crush add-bucket node3-ssd host
<prompt>cephadm &gt; </prompt>ceph osd crush move node3-ssd root=ssd</screen>

  <para>
   Finally, for each SSD OSD, move the OSD to the SSD root. In this example, we
   assume that osd.0, osd.1 and osd.2 are physically hosted on SSDs:
  </para>

<screen><prompt>cephadm &gt; </prompt>ceph osd crush add osd.0 1 root=ssd
<prompt>cephadm &gt; </prompt>ceph osd crush set osd.0 1 root=ssd host=node1-ssd
<prompt>cephadm &gt; </prompt>ceph osd crush add osd.1 1 root=ssd
<prompt>cephadm &gt; </prompt>ceph osd crush set osd.1 1 root=ssd host=node2-ssd
<prompt>cephadm &gt; </prompt>ceph osd crush add osd.2 1 root=ssd
<prompt>cephadm &gt; </prompt>ceph osd crush set osd.2 1 root=ssd host=node3-ssd</screen>

  <para>
   The CRUSH hierarchy should now look like this:
  </para>

<screen><prompt>cephadm &gt; </prompt>ceph osd tree
ID WEIGHT  TYPE NAME                   UP/DOWN REWEIGHT PRIMARY-AFFINITY
-5 3.00000 root ssd
-6 1.00000     host node1-ssd
 0 1.00000         osd.0                    up  1.00000          1.00000
-7 1.00000     host node2-ssd
 1 1.00000         osd.1                    up  1.00000          1.00000
-8 1.00000     host node3-ssd
 2 1.00000         osd.2                    up  1.00000          1.00000
-1 0.11096 root default
-2 0.03699     host node1
 3 0.01849         osd.3                    up  1.00000          1.00000
 6 0.01849         osd.6                    up  1.00000          1.00000
-3 0.03699     host node2
 4 0.01849         osd.4                    up  1.00000          1.00000
 7 0.01849         osd.7                    up  1.00000          1.00000
-4 0.03699     host node3
 5 0.01849         osd.5                    up  1.00000          1.00000
 8 0.01849         osd.8                    up  1.00000          1.00000</screen>

  <para>
   Now, create a CRUSH rule that targets the SSD root:
  </para>

<screen><prompt>cephadm &gt; </prompt>ceph osd crush rule create-simple ssd_replicated_ruleset ssd host</screen>

  <para>
   The original default <option>replicated_ruleset</option> (with ID 0) will
   target the HDDs. The new <option>ssd_replicated_ruleset</option> (with ID 1)
   will target the SSDs.
  </para>

  <para>
   Any existing pools will still be using the HDDs, because they are in the
   default hierarchy in the CRUSH map. A new pool can be created to use SSDs
   only:
  </para>

<screen><prompt>cephadm &gt; </prompt>ceph osd pool create ssd-pool 64 64
<prompt>cephadm &gt; </prompt>ceph osd pool set ssd-pool crush_ruleset 1</screen>

  <para>
   The ID "1" in the above command needs to match the ID of the new CRUSH role
   which targets the SSDs.
  </para>
 </sect1>
</chapter>
  <chapter xml:base="admin_operating_pools.xml" version="5.0" xml:id="ceph.pools">
 <title>Managing Storage Pools</title>
 <para>
  When you first deploy a cluster without creating a pool, Ceph uses the
  default pools for storing data. A pool provides you with:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>Resilience</emphasis>: You can set how many OSDs are allowed to
    fail without losing data. For replicated pools, it is the desired number of
    copies/replicas of an object. A typical configuration stores an object and
    one additional copy (that is <emphasis>size=2</emphasis>), but you can
    determine the number of copies/replicas. For erasure coded pools, it is the
    number of coding chunks (that is <emphasis>m=2</emphasis> in the erasure
    code profile).
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Placement Groups</emphasis>: You can set the number of placement
    groups for the pool. A typical configuration uses approximately 100
    placement groups per OSD to provide optimal balancing without using up too
    many computing resources. When setting up multiple pools, be careful to
    ensure you set a reasonable number of placement groups for both the pool
    and the cluster as a whole.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>CRUSH Rules</emphasis>: When you store data in a pool, a CRUSH
    ruleset mapped to the pool enables CRUSH to identify a rule for the
    placement of the object and its replicas (or chunks for erasure coded
    pools) in your cluster. You can create a custom CRUSH rule for your pool.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Snapshots</emphasis>: When you create snapshots with
    <command>ceph osd pool mksnap</command>, you effectively take a snapshot of
    a particular pool.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Set Ownership</emphasis>: You can set a user ID as the owner of a
    pool.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  To organize data into pools, you can list, create, and remove pools. You can
  also view the usage statistics for each pool.
 </para>
 <sect1 xml:id="ceph.pools.operate">
  <title>Operating Pools</title>

  <para>
   This section introduces practical information to perform basic tasks with
   pools. You can find out how to list, create, and delete pools, as well as
   show pool statistics or manage snapshots of a pool.
  </para>

  <sect2>
   <title>List Pools</title>
   <para>
    To list your cluster’s pools, execute:
   </para>
<screen>ceph osd lspools
0 rbd, 1 photo_collection, 2 foo_pool,</screen>
  </sect2>

  <sect2 xml:id="ceph.pools.operate.add_pool">
   <title>Create a Pool</title>
   <para>
    To create a replicated pool, execute:
   </para>
<screen>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> <replaceable>pgp_type</replaceable> <replaceable>crush_ruleset_name</replaceable>, <replaceable>expected_num_objects</replaceable></screen>
   <para>
    To create an erasure pool, execute:
   </para>
<screen>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> <replaceable>pgp_type</replaceable> <replaceable>erasure_code_profile</replaceable> \
 <replaceable>crush_ruleset_name</replaceable>, <replaceable>expected_num_objects</replaceable></screen>
   <variablelist>
    <varlistentry>
     <term>pool_name</term>
     <listitem>
      <para>
       The name of the pool. It must be unique. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       The total number of placement groups for the pool. This option is
       required. Default value is 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       The total number of placement groups for placement purposes. This should
       be equal to the total number of placement groups, except for placement
       group splitting scenarios. This option is required. Default value is 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_type</term>
     <listitem>
      <para>
       The pool type which may either be <emphasis>replicated</emphasis> to
       recover from lost OSDs by keeping multiple copies of the objects or
       <emphasis>erasure</emphasis> to get a kind of generalized RAID5
       capability. The replicated pools require more raw storage but implement
       all Ceph operations. The erasure pools require less raw storage but
       only implement a subset of the available operations. Default is
       'replicated'.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset_name</term>
     <listitem>
      <para>
       The name of the crush ruleset for this pool. If the specified ruleset
       does not exist, the creation of replicated pool will fail with -ENOENT.
       But replicated pool will create a new erasure ruleset with specified
       name. The default value is 'erasure-code' for erasure pool. Picks up
       Ceph configuration variable
       <option>osd_pool_default_crush_replicated_ruleset</option> for
       replicated pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>erasure_code_profile=profile</term>
     <listitem>
      <para>
       For erasure pools only. Use the erasure code profile. It must be an
       existing profile as defined by <command>osd erasure-code-profile
       set</command>.
      </para>
      <para>
       When you create a pool, set the number of placement groups to a
       reasonable value (for example 100). Consider the total number of
       placement groups per OSD too. Placement groups are computationally
       expensive, so performance will degrade when you have many pools with
       many placement groups (for example 50 pools with 100 placement groups
       each). The point of diminishing returns depends upon the power of the
       OSD host.
      </para>
      <para>
       See
       <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/">Placement
       Groups</link> for details on calculating an appropriate number of
       placement groups for your pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>expected_num_objects</term>
     <listitem>
      <para>
       The expected number of objects for this pool. By setting this value, the
       PG folder splitting happens at the pool creation time. This avoids the
       latency impact with a runtime folder splitting.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Set Pool Quotas</title>
   <para>
    You can set pool quotas for the maximum number of bytes and/or the maximum
    number of objects per pool.
   </para>
<screen>ceph osd pool set-quota <replaceable>pool-name</replaceable> <replaceable>max_objects</replaceable> <replaceable>obj-count</replaceable> <replaceable>max_bytes</replaceable> <replaceable>bytes</replaceable></screen>
   <para>
    For example:
   </para>
<screen>ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    To remove a quota, set its value to 0.
   </para>
  </sect2>

  <sect2 xml:id="ceph.pools.operate.del_pool">
   <title>Delete a Pool</title>
   <para>
    To delete a pool, execute:
   </para>
<screen>ceph osd pool delete <replaceable>pool-name</replaceable> <replaceable>pool-name</replaceable> --yes-i-really-really-mean-it</screen>
   <para>
    If you created your own rulesets and rules for a pool you created, you
    should consider removing them when you no longer need your pool. If you
    created users with permissions strictly for a pool that no longer exists,
    you should consider deleting those users too.
   </para>
  </sect2>

  <sect2>
   <title>Rename a Pool</title>
   <para>
    To rename a pool, execute:
   </para>
<screen>ceph osd pool rename <replaceable>current-pool-name</replaceable> <replaceable>new-pool-name</replaceable></screen>
   <para>
    If you rename a pool and you have per-pool capabilities for an
    authenticated user, you must update the user’s capabilities with the new
    pool name.
   </para>
  </sect2>

  <sect2>
   <title>Show Pool Statistics</title>
   <para>
    To show a pool’s usage statistics, execute:
   </para>
<screen>rados df
pool name  category  KB  objects   lones  degraded  unfound  rd  rd KB  wr  wr KB
cold-storage    -   228   1         0      0          0       0   0      1   228
data            -    1    4         0      0          0       0   0      4    4
hot-storage     -    1    2         0      0          0       15  10     5   231
metadata        -    0    0         0      0          0       0   0      0    0
pool1           -    0    0         0      0          0       0   0      0    0
rbd             -    0    0         0      0          0       0   0      0    0
total used          266268          7
total avail       27966296
total space       28232564</screen>
  </sect2>

  <sect2 xml:id="ceph.pools.values">
   <title>Set Pool Values</title>
   <para>
    To set a value to a pool, execute:
   </para>
<screen>ceph osd pool set <replaceable>pool-name</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
   <para>
    You may set values for the following keys:
   </para>
   <variablelist>
    <varlistentry>
     <term>size</term>
     <listitem>
      <para>
       Sets the number of replicas for objects in the pool. See
       <xref linkend="ceph.pools.options.num_of_replicas" role="internalbook"/> for further
       details. Replicated pools only.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       Sets the minimum number of replicas required for I/O. See
       <xref linkend="ceph.pools.options.num_of_replicas" role="internalbook"/> for further
       details. Replicated pools only.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       The number of seconds to allow clients to replay acknowledged, but
       uncommitted requests.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       The number of placement groups for the pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       The effective number of placement groups to use when calculating data
       placement.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       The ruleset to use for mapping object placement in the cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       Set (1) or unset (0) the HASHPSPOOL flag on a given pool. Enabling this
       flag changes the algorithm to better distribute PGs to OSDs. After
       enabling this flag on a pool whose HASHPSPOOL flag was set to 0, the
       cluster starts backfilling to have a correct placement of all PGs again.
       Be aware that this can create quite substantial I/O load on a cluster,
       so good planning must be done on a highly loaded production clusters.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       Prevents the pool from being removed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       Prevents the pool's <option>pg_num</option> and <option>pgp_num</option>
       from being changed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       Prevents the pool's size from being changed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       Set/Unset the <literal>WRITE_FADVISE_DONTNEED</literal> flag on a given
       pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub,nodeep-scrub</term>
     <listitem>
      <para>
       Disables (deep)-scrubbing of the data for the specific pool to resolve
       temporary high I/O load.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       Enables hit set tracking for cache pools. See
       <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom
       Filter</link> for additional information. This option can have the
       following values: <literal>bloom</literal>,
       <literal>explicit_hash</literal>, <literal>explicit_object</literal>.
       Default is <literal>bloom</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       The number of hit sets to store for cache pools. The higher the number,
       the more RAM consumed by the <systemitem>ceph-osd</systemitem> daemon.
       Default is <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       The duration of a hit set period in seconds for cache pools. The higher
       the number, the more RAM consumed by the
       <systemitem>ceph-osd</systemitem> daemon.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       The false positive probability for the bloom hit set type. See
       <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom
       Filter</link> for additional information. Valid range is 0.0 - 1.0
       Default is <literal>0.05</literal>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       The percentage of the cache pool containing modified (dirty) objects
       before the cache tiering agent will flush them to the backing storage
       pool. Default is <literal>.4</literal>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       The percentage of the cache pool containing modified (dirty) objects
       before the cache tiering agent will flush them to the backing storage
       pool with a higher speed. Default is <literal>.6</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       The percentage of the cache pool containing unmodified (clean) objects
       before the cache tiering agent will evict them from the cache pool.
       Default is <literal>.8</literal>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       Ceph will begin flushing or evicting objects when the
       <option>max_bytes</option> threshold is triggered.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       Ceph will begin flushing or evicting objects when the
       <option>max_objects</option> threshold is triggered.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       Temperature decay rate between two successive
       <literal>hit_set</literal>s. Default is <literal>20</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_search_last_n</term>
     <listitem>
      <para>
       Count at most <literal>N</literal> appearances in
       <literal>hit_set</literal>s for temperature calculation. Default is
       <literal>1</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       The time (in seconds) before the cache tiering agent will flush an
       object from the cache pool to the storage pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       The time (in seconds) before the cache tiering agent will evict an
       object from the cache pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       If this flag is enabled on erasure coding pools, then the read request
       issues sub-reads to all shards, and waits until it receives enough
       shards to decode to serve the client. In the case of
       <emphasis>jerasure</emphasis> and <emphasis>isa</emphasis> erasure
       plug-ins, when the first <literal>K</literal> replies return, then the
       client’s request is served immediately using the data decoded from
       these replies. This helps to gain some resources for better performance.
       Currently, this flag is only supported for erasure coding pools. Default
       is <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       The minimum interval in seconds for pool scrubbing when the cluster load
       is low. The default <literal>0</literal> means that the
       <option>osd_scrub_min_interval</option> value from the Ceph
       configuration file is used.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       The maximum interval in seconds for pool scrubbing, regardless of the
       cluster load. The default <literal>0</literal> means that the
       <option>osd_scrub_max_interval</option> value from the Ceph
       configuration file is used.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       The interval in seconds for the pool <emphasis>deep</emphasis>
       scrubbing. The default <literal>0</literal> means that the
       <option>osd_deep_scrub</option> value from the Ceph configuration file
       is used.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Get Pool Values</title>
   <para>
    To get a value from a pool, execute:
   </para>
<screen>ceph osd pool get <replaceable>pool-name</replaceable> <replaceable>key</replaceable></screen>
   <para>
    You can get values for keys listed in <xref linkend="ceph.pools.values" role="internalbook"/>
    plus the following keys:
   </para>
   <variablelist>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       The number of placement groups for the pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       The effective number of placement groups to use when calculating data
       placement. Valid range is equal to or less than
       <literal>pg_num</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph.pools.options.num_of_replicas">
   <title>Set the Number of Object Replicas</title>
   <para>
    To set the number of object replicas on a replicated pool, execute the
    following:
   </para>
<screen>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <important>
    <para>
     The <replaceable>num-replicas</replaceable> includes the object itself. If
     you for example want the object and two copies of the object for a total
     of three instances of the object, specify 3.
    </para>
   </important>
   <para>
    For example:
   </para>
<screen>ceph osd pool set data size 3</screen>
   <para>
    You may execute this command for each pool.
   </para>
   <note>
    <para>
     An object might accept I/Os in degraded mode with fewer than <literal>pool
     size</literal> replicas. To set a minimum number of required replicas for
     I/O, you should use the <literal>min_size</literal> setting. For example:
    </para>
<screen>ceph osd pool set data min_size 2</screen>
    <para>
     This ensures that no object in the data pool will receive I/O with fewer
     than <literal>min_size</literal> replicas.
    </para>
   </note>
  </sect2>

  <sect2>
   <title>Get the Number of Object Replicas</title>
   <para>
    To get the number of object replicas, execute the following:
   </para>
<screen>ceph osd dump | grep 'replicated size'</screen>
   <para>
    Ceph will list the pools, with the <literal>replicated size</literal>
    attribute highlighted. By default, Ceph creates two replicas of an object
    (a total of three copies, or a size of 3).
   </para>
  </sect2>
 </sect1>
</chapter>
  <chapter xml:base="admin_operating_snapshots.xml" version="5.0" xml:id="cha.ceph.snapshots">
 <title>Snapshots</title>
 <para>
  A snapshot is a read-only copy of the state of an object—a pool or an
  image—at a particular point in time. This way you can retain a history
  of its state. There are two types of snapshots in Ceph—RBD snapshots
  and pool snapshots.
 </para>
 <sect1 xml:id="cha.ceph.snapshots.rbd">
  <title>RBD Snapshots</title>

  <para>
   RBD snapshot is a snapshot of a RADOS block device image. With snapshots you
   retain a history of the image’s state. Ceph also supports snapshot
   layering, which allows you to clone VM images quickly and easily. Ceph
   supports block device snapshots using the <command>rbd</command> command and
   many higher level interfaces, including QEMU,
   <systemitem>libvirt</systemitem>, OpenStack and CloudStack.
  </para>

  <note>
   <para>
    Stop input/output operations before snapshotting an image. If the image
    contains a file system, the file system must be in a consistent state
    <emphasis>before</emphasis> snapshotting.
   </para>
  </note>

  <sect2>
   <title>Cephx Notes</title>
   <para>
    When <systemitem>cephx</systemitem> is enabled (see
    <link xlink:href="http://ceph.com/docs/master/rados/configuration/auth-config-ref/"/>
    for more information), you must specify a user name or ID and a path to the
    keyring containing the corresponding key for the user. See
    <link xlink:href="http://ceph.com/docs/master/rados/operations/user-management/">User
    Management</link> for more details. You may also add the
    <systemitem>CEPH_ARGS</systemitem> environment variable to avoid re-entry
    of the following parameters.
   </para>
<screen>rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    For example:
   </para>
<screen>rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     Add the user and secret to the <systemitem>CEPH_ARGS</systemitem>
     environment variable so that you do not need to enter them each time.
    </para>
   </tip>
  </sect2>

  <sect2>
   <title>Snapshot Basics</title>
   <para>
    The following procedures demonstrate how to create, list, and remove
    snapshots using the <command>rbd</command> command on the command line.
   </para>
   <sect3>
    <title>Create Snapshot</title>
    <para>
     To create a snapshot with <command>rbd</command>, specify the <option>snap
     create</option> option, the pool name and the image name.
    </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>rbd --pool rbd snap create --snap snapshot1 image1
rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>List Snapshots</title>
    <para>
     To list snapshots of an image, specify the pool name and the image name.
    </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>rbd --pool rbd snap ls image1
rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3>
    <title>Rollback Snapshot</title>
    <para>
     To rollback to a snapshot with <command>rbd</command>, specify the
     <option>snap rollback</option> option, the pool name, the image name and
     the snap name.
    </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>rbd --pool pool1 snap rollback --snap snapshot1 image1
rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      Rolling back an image to a snapshot means overwriting the current version
      of the image with data from a snapshot. The time it takes to execute a
      rollback increases with the size of the image. It is <emphasis>faster to
      clone</emphasis> from a snapshot <emphasis>than to rollback</emphasis> an
      image to a snapshot, and it is the preferred method of returning to a
      pre-existing state.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Delete a Snapshot</title>
    <para>
     To delete a snapshot with <command>rbd</command>, specify the <option>snap
     rm</option> option, the pool name, the image name and the user name.
    </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>rbd --pool pool1 snap rm --snap snapshot1 image1
rbd snap rm pool1/imag1@snapshot1</screen>
    <note>
     <para>
      Ceph OSDs delete data asynchronously, so deleting a snapshot does not
      free up the disk space immediately.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Purge Snapshots</title>
    <para>
     To delete all snapshots for an image with <command>rbd</command>, specify
     the <option>snap purge</option> option and the image name.
    </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>rbd --pool pool1 snap purge image1
rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.snapshoti.layering">
   <title>Layering</title>
   <para>
    Ceph supports the ability to create many copy-on-write (COW) clones of a
    block device snapshot. Snapshot layering enables Ceph block device
    clients to create images very quickly. For example, you might create a
    block device image with a Linux VM written to it; then, snapshot the image,
    protect the snapshot, and create as many copy-on-write clones as you like.
    A snapshot is read-only, so cloning a snapshot simplifies
    semantics—making it possible to create clones rapidly.
   </para>
   <note>
    <para>
     The terms “parent” and “child” mentioned in the command line
     examples below mean a Ceph block device snapshot (parent), and the
     corresponding image cloned from the snapshot (child).
    </para>
   </note>
   <para>
    Each cloned image (child) stores a reference to its parent image, which
    enables the cloned image to open the parent snapshot and read it.
   </para>
   <para>
    A COW clone of a snapshot behaves exactly like any other Ceph block
    device image. You can read to, write from, clone, and resize cloned images.
    There are no special restrictions with cloned images. However, the
    copy-on-write clone of a snapshot refers to the snapshot, so you
    <emphasis>must</emphasis> protect the snapshot before you clone it.
   </para>
   <note>
    <para>
     Ceph only supports cloning for <emphasis>format 2</emphasis> images
     (that is created with <command>rbd create --image-format 2</command>).
    </para>
   </note>
   <sect3>
    <title>Getting Started with Layering</title>
    <para>
     Ceph block device layering is a simple process. You must have an image.
     You must create a snapshot of the image. You must protect the snapshot.
     Once you have performed these steps, you can begin cloning the snapshot.
    </para>
    <para>
     The cloned image has a reference to the parent snapshot, and includes the
     pool ID, image ID and snapshot ID. The inclusion of the pool ID means that
     you may clone snapshots from one pool to images in another pool.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <emphasis>Image Template</emphasis>: A common use case for block device
       layering is to create a master image and a snapshot that serves as a
       template for clones. For example, a user may create an image for a Linux
       distribution (for example SUSE Linux Enterprise Server), and create a snapshot for it.
       Periodically, the user may update the image and create a new snapshot
       (for example <command>zypper ref &amp;&amp; zypper patch</command>
       followed by <command>rbd snap create</command>). As the image matures,
       the user can clone any one of the snapshots.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Extended Template</emphasis>: A more advanced use case
       includes extending a template image that provides more information than
       a base image. For example, a user may clone an image (a VM template) and
       install other software (for example a database, a content management
       system, an analytics system, etc.) and then snapshot the extended image,
       which itself may be updated same as the base image.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Template Pool</emphasis>: One way to use block device layering
       is to create a pool that contains master images that act as templates,
       and snapshots of those templates. You may then extend read-only
       privileges to users so that they may clone the snapshots without the
       ability to write or execute within the pool.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Image Migration/Recovery</emphasis>: One way to use block
       device layering is to migrate or recover data from one pool into another
       pool.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>Protecting a Snapshot</title>
    <para>
     Clones access the parent snapshots. All clones would break if a user
     inadvertently deleted the parent snapshot. To prevent data loss, you need
     to protect the snapshot before you can clone it.
    </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>rbd --pool pool1 snap protect --image image1 --snap snapshot1
rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      You cannot delete a protected snapshot.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Cloning a Snapshot</title>
    <para>
     To clone a snapshot, you need to specify the parent pool, image and
     snapshot, the child pool and image name. You must protect the snapshot
     before you can clone it.
    </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
rbd clone
<replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
 <replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      You may clone a snapshot from one pool to an image in another pool. For
      example, you may maintain read-only images and snapshots as templates in
      one pool, and writable clones in another pool.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Unprotecting a Snapshot</title>
    <para>
     Before you can delete a snapshot, you must unprotect it first.
     Additionally, you may <emphasis>not</emphasis> delete snapshots that have
     references from clones. You must flatten each clone of a snapshot, before
     you can delete the snapshot.
    </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>Listing Children of a Snapshot</title>
    <para>
     To list the children of a snapshot, execute the following:
    </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>rbd --pool pool1 children --image image1 --snap snapshot1
rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>Flattening a Cloned Image</title>
    <para>
     Cloned images retain a reference to the parent snapshot. When you remove
     the reference from the child clone to the parent snapshot, you effectively
     “flatten” the image by copying the information from the snapshot to
     the clone. The time it takes to flatten a clone increases with the size of
     the snapshot. To delete a snapshot, you must flatten the child images
     first.
    </para>
<screen>rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>rbd --pool pool1 flatten --image image1
rbd flatten pool1/image1</screen>
    <note>
     <para>
      Since a flattened image contains all the information from the snapshot, a
      flattened image will take up more storage space than a layered clone.
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="cha.ceph.snapshots.pool">
  <title>Pool Snapshots</title>

  <para>
   Pool snapshots are snapshots of the state of the whole Ceph pool. With
   pool snapshots, you can retain the history of the pool's state. Depending on
   the pool's size, creating pool snapshots may require a lot of storage space.
   Always check the related storage for enough disk space before creating a
   snapshot of a pool.
  </para>

  <sect2>
   <title>Make a Snapshot of a Pool</title>
   <para>
    To make a snapshot of a pool, execute:
   </para>
<screen>ceph osd pool mksnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
   <para>
    For example:
   </para>
<screen>ceph osd pool mksnap pool1 snapshot1
created pool pool1 snap snapshot1</screen>
  </sect2>

  <sect2>
   <title>Remove a Snapshot of a Pool</title>
   <para>
    To remove a snapshot of a pool, execute:
   </para>
<screen>ceph osd pool rmsnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
  </sect2>
 </sect1>
</chapter>
  <chapter xml:base="admin_ceph_erasure.xml" version="5.0" xml:id="cha.ceph.erasure">
 <title>Erasure Coded Pools</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  A Ceph pool is associated to a type to sustain the loss of an OSD (that is
  a disk since most of the time there is one OSD per disk). The default choice
  when creating a pool is replicated, meaning every object is copied on
  multiple disks. The Erasure Code pool type can be used instead to save space.
 </para>
 <para>
  For background information on Erasure Code, see
  <link xlink:href="https://en.wikipedia.org/wiki/Erasure_code"/>.
 </para>
 <note>
  <para>
   You cannot access erasure coded pools with the rbd interface unless you have
   a cache tier configured. Refer to <xref linkend="ceph.tier.erasure" role="internalbook"/> for
   more details.
  </para>
 </note>
 <sect1>
  <title>Creating a Sample Erasure Coded Pool</title>

  <para>
   The simplest erasure coded pool is equivalent to RAID5 and requires at least
   three hosts:
  </para>

<screen><prompt role="user">&gt; </prompt>ceph osd pool create ecpool 12 12 erasure
pool 'ecpool' created
<prompt role="user">&gt; </prompt>echo ABCDEFGHI | rados --pool ecpool put NYAN -
<prompt role="user">&gt; </prompt>rados --pool ecpool get NYAN -
ABCDEFGHI</screen>

  <para>
   The <literal>12</literal> in the <command>pool create</command> command
   stands for the number of placement groups.
  </para>
 </sect1>
 <sect1>
  <title>Erasure Code Profiles</title>

  <para>
   Some terminology hints:
  </para>

  <variablelist>
   <varlistentry>
    <term>chunk</term>
    <listitem>
     <para>
      when the encoding function is called, it returns chunks of the same size:
      data chunks which can be concatenated to reconstruct the original object
      and coding chunks which can be used to rebuild a lost chunk.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>k</term>
    <listitem>
     <para>
      the number of data chunks, that is the number of chunks into which the
      original object is divided. For example if <literal>k = 2</literal> a
      10KB object will be divided into <literal>k</literal> objects of 5KB
      each.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>m</term>
    <listitem>
     <para>
      the number of coding chunks, that is the number of additional chunks
      computed by the encoding functions. If there are 2 coding chunks, it
      means 2 OSDs can be out without losing data.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The default erasure code profile sustains the loss of a single OSD. It is
   equivalent to a replicated pool of size two but requires 1.5TB instead of
   2TB to store 1TB of data. The default profile can be displayed with:
  </para>

<screen><prompt role="user">&gt; </prompt>ceph osd erasure-code-profile get default
directory=.libs
k=2
m=1
plugin=jerasure
ruleset-failure-domain=host
technique=reed_sol_van</screen>

  <para>
   Choosing the right profile is important because it cannot be modified after
   the pool is created: a new pool with a different profile needs to be created
   and all objects from the previous pool moved to the new.
  </para>

  <para>
   The most important parameters of the profile are <literal>k</literal>,
   <literal>m</literal> and <literal>ruleset-failure-domain</literal> because
   they define the storage overhead and the data durability. For example, if
   the desired architecture must sustain the loss of two racks with a storage
   overhead of 40% overhead, the following profile can be defined:
  </para>

<screen><prompt role="user">&gt; </prompt>ceph osd erasure-code-profile set <replaceable>myprofile</replaceable> \
   k=3 \
   m=2 \
   ruleset-failure-domain=rack
<prompt role="user">&gt; </prompt>ceph osd pool create ecpool 12 12 erasure <replaceable>myprofile</replaceable>
<prompt role="user">&gt; </prompt>echo ABCDEFGHI | rados --pool ecpool put NYAN -
<prompt role="user">&gt; </prompt>rados --pool ecpool get NYAN -
ABCDEFGHI</screen>

  <para>
   The NYAN object will be divided in three (<literal>k=3</literal>) and two
   additional chunks will be created (<literal>m=2</literal>). The value of
   <literal>m</literal> defines how many OSDs can be lost simultaneously
   without losing any data. The <literal>ruleset-failure-domain=rack</literal>
   will create a CRUSH ruleset that ensures no two chunks are stored in the
   same rack.
  </para>

  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_erasure_obj.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_erasure_obj.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </informalfigure>

  <para>
   For more information about the erasure code profiles, see
   <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/erasure-code-profile"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph.tier.erasure">
  <title>Erasure Coded Pool And Cache Tiering</title>

  <para>
   Erasure coded pools require more resources than replicated pools and lack
   some functionalities such as partial writes. To overcome these limitations,
   it is recommended to set a cache tier before the erasure coded pool.
  </para>

  <para>
   For example, if the <quote>hot-storage</quote> pool is made of fast storage:
  </para>

<screen><prompt role="user">&gt; </prompt>ceph osd tier add ecpool hot-storage
<prompt role="user">&gt; </prompt>ceph osd tier cache-mode hot-storage writeback
<prompt role="user">&gt; </prompt>ceph osd tier set-overlay ecpool hot-storage</screen>

  <para>
   This will place the <quote>hot-storage</quote> pool as tier of ecpool in
   write-back mode so that every write and read to the ecpool is actually using
   the hot-storage and benefits from its flexibility and speed.
  </para>

  <para>
   It is not possible to create an RBD image on an erasure coded pool because
   it requires partial writes. It is however possible to create an RBD image on
   an erasure coded pool when a replicated pool tier set a cache tier:
  </para>

<screen><prompt role="user">&gt; </prompt>rbd --pool ecpool create --size 10 myvolume</screen>

  <para>
   For more information about cache tiering, see
   <xref linkend="cha.ceph.tiered" role="internalbook"/>.
  </para>
 </sect1>
</chapter>
  <chapter xml:base="admin_ceph_tiered_storage.xml" version="5.0" xml:id="cha.ceph.tiered">

 <title>Cache Tiering</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  A <emphasis>cache tier</emphasis> is an additional storage layer implemented
  between the client and the standard storage. It is designed to speed up the
  access to pools stored on slow hard disks and erasure coded pools.
 </para>
 <para>
  Typically cache tiering involves creating a pool of relatively fast/expensive
  storage devices (for example SSD drives) configured to act as a cache tier,
  and a backing pool of slower and cheaper devices configured to act as a
  storage tier.
 </para>
 <sect1>
  <title>Tiered Storage Terminology</title>

  <para>
   Cache tiering recognizes two types of pools: a <emphasis>cache
   pool</emphasis> and a <emphasis>storage pool</emphasis>.
  </para>

  <tip>
   <para>
    For general information on pools, see <xref linkend="ceph.pools" role="internalbook"/>.
   </para>
  </tip>

  <variablelist>
   <varlistentry>
    <term>storage pool</term>
    <listitem>
     <para>
      Either a standard replicated pool that stores several copies of an object
      in the Ceph storage cluster, or an erasure coded pool (see
      <xref linkend="cha.ceph.erasure" role="internalbook"/>).
     </para>
     <para>
      The storage pool is sometimes referred to as a 'backing' or 'cold'
      storage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>cache pool</term>
    <listitem>
     <para>
      A standard replicated pool stored on a relatively small but fast storage
      device with their own ruleset in a crush map.
     </para>
     <para>
      The cache pool is also referred to as a 'hot' storage.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.ceph.tiered.caution">
  <title>Points to Consider</title>

  <para>
   Cache tiering may <emphasis>degrade</emphasis> the cluster performance for
   specific workloads. The following points show some of its aspects you need
   to consider:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>Workload dependent</emphasis>: Whether a cache will improve
     performance is dependent on the workload. Because there is a cost
     associated with moving objects into or out of the cache, it can be more
     effective when most of the requests touch a small number of objects. The
     cache pool should be large enough to capture the working set for your
     workload to avoid thrashing.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Difficult to benchmark</emphasis>: Most performance benchmarks
     may show low performance with cache tiering. The reason is that they
     request a big set of objects, and it takes a long time for the cache to
     'warm up'.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Possibly low performance</emphasis>: For workloads that are not
     suitable for cache tiering, performance is often slower than a normal
     replicated pool without cache tiering enabled.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis><systemitem>librados</systemitem> object enumeration</emphasis>:
     If your application is using <systemitem>librados</systemitem> directly
     and relies on object enumeration, cache tiering may not work as expected.
     (This is not a problem for <phrase>RADOS Gateway</phrase>, RBD, or CephFS.)
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1>
  <title>When to Use Cache Tiering</title>

  <para>
   Consider using cache tiering in the following cases:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     You need to access erasure coded pools via RADOS block device (RBD).
    </para>
   </listitem>
   <listitem>
    <para>
     You need to access erasure coded pools via iSCSI as it inherits the
     limitations of RBD. For more information on iSCSI, refer to
     <xref linkend="cha.ceph.iscsi" role="internalbook"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     You have a limited number of high performance storage and a large
     collection of low performance storage, and need to access the stored data
     faster.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec.ceph.tiered.cachemodes">
  <title>Cache Modes</title>

  <para>
   The cache tiering agent handles the migration of data between the cache tier
   and the backing storage tier. Administrators have the ability to configure
   how this migration takes place. There are two main scenarios:
  </para>

  <variablelist>
   <varlistentry>
    <term>write-back mode</term>
    <listitem>
     <para>
      When administrators configure tiers with write-back mode, Ceph clients
      write data to the cache tier and receive an ACK from the cache tier. In
      time, the data written to the cache tier migrates to the storage tier and
      gets flushed from the cache tier. Conceptually, the cache tier is
      overlaid <quote>in front</quote> of the backing storage tier. When a
      Ceph client needs data that resides in the storage tier, the cache
      tiering agent migrates the data to the cache tier on read, then it is
      sent to the Ceph client. Thereafter, the Ceph client can perform I/O
      using the cache tier, until the data becomes inactive. This is ideal for
      mutable data such as photo or video editing, transactional data, etc.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>read-only mode</term>
    <listitem>
     <para>
      When administrators configure tiers with read-only mode, Ceph clients
      write data to the backing tier. On read, Ceph copies the requested
      objects from the backing tier to the cache tier. Stale objects get
      removed from the cache tier based on the defined policy. This approach is
      ideal for immutable data such as presenting pictures or videos on a
      social network, DNA data, X-ray imaging, etc., because reading data from
      a cache pool that might contain out-of-date data provides weak
      consistency. Do not use read-only mode for mutable data.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ses.tiered.storage">
  <title>Setting Up an Example Tiered Storage</title>

  <para>
   This section illustrates how to set up a fast SSD cache tier (hot-storage)
   in front of a standard hard disk (cold-storage).
  </para>

  <tip>
   <para>
    The following example is for illustration purposes only and includes a
    setup with one root and one rule for the SSD part residing on a single
    Ceph node.
   </para>
   <para>
    In the production environment, the cluster setup typically includes more
    root and rule entries for the hot storage, and also mixed nodes with both
    SSDs and SATA disks.
   </para>
  </tip>

  <procedure>
   <step>
    <para>
     Prepare a host machine with fast drives, such as SSDs. This cluster node
     will act as a fast cache tier.
    </para>
   </step>
   <step>
    <para>
     Turn the machine into a Ceph node. Install the software and configure
     the host machine as described in
     <xref linkend="ceph.install.ceph-deploy.eachnode" role="internalbook"/>. Let us assume that
     its name is <replaceable>node-4</replaceable>.
    </para>
   </step>
   <step>
    <para>
     You need to create 4 OSDs nodes. For this purpose run
     <command>ceph-deploy</command> from the admin server (refer to
     <xref linkend="ceph.install.ceph-deploy.cephdeploy" role="internalbook"/>). Remember to
     replace <replaceable>node-4</replaceable> with the actual node name and
     device with the actual device name:
    </para>
<screen><prompt>cephadm &gt; </prompt>for d in a b c d; do
  ceph-deploy osd create node-4:device${d}
done</screen>
    <para>
     This may result in an entry like this in the CRUSH map:
    </para>
<screen>[...]
host node-4 {
        id -5  # do not change unnecessarily
        # weight 0.012
        alg straw
        hash 0  # rjenkins1
        item osd.6 weight 0.003
        item osd.7 weight 0.003
        item osd.8 weight 0.003
        item osd.9 weight 0.003
}
[...]</screen>
   </step>
   <step>
    <para>
     Edit the CRUSH map for the hot-storage pool mapped to the OSDs backed by
     the fast SSD drives. Define a second hierarchy with a root node for the
     SSDs (as <quote>root ssd</quote>). Additionally, change the weight and a
     CRUSH rule for the SSDs. For more information on CRUSH map, see
     <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/crush-map/"/>.
    </para>
    <para>
     Edit the CRUSH map directly with command line tools such as
     <command>getcrushmap</command> and <command>crushtool</command>:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Retrieve the current map and save it as <filename>c.map</filename>:
      </para>
<screen>sudo ceph osd getcrushmap -o c.map</screen>
     </step>
     <step>
      <para>
       Decompile <filename>c.map</filename> and save it as
       <filename>c.txt</filename>:
      </para>
<screen><prompt>cephadm &gt; </prompt>crushtool -d c.map -o c.txt</screen>
     </step>
     <step>
      <para>
       Edit <filename>c.txt</filename>:
      </para>
<screen>[...]
host node-4 {
        id -5  # do not change unnecessarily
        # weight 4.000
        alg straw
        hash 0  # rjenkins1
        item osd.6 weight 1.000
        item osd.7 weight 1.000
        item osd.8 weight 1.000
        item osd.9 weight 1.000
}
root ssd {    # newly added root for the SSD hot-storage
        id -6
        alg straw
        hash 0
        item node-4 weight 4.00
}
rule ssd {
        ruleset 4
        type replicated
        min_size 0
        max_size 4
        step take ssd
        step chooseleaf firstn 0 type host
        step emit
}
[...]</screen>
     </step>
     <step>
      <para>
       Compile the edited <filename>c.txt</filename> file and save it as
       <filename>ssd.map</filename>:
      </para>
<screen><prompt>cephadm &gt; </prompt>crushtool -c c.txt -o ssd.map</screen>
     </step>
     <step>
      <para>
       Finally install <filename>ssd.map</filename> as the new CRUSH map:
      </para>
<screen>sudo ceph osd setcrushmap -i ssd.map</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Create the hot-storage pool to be used for cache tiering. Use the new
     'ssd' rule for it:
    </para>
<screen>sudo ceph osd pool create hot-storage 100 100 replicated ssd</screen>
   </step>
   <step>
    <para>
     Create the cold-storage pool using the default 'replicated_ruleset' rule:
    </para>
<screen>sudo ceph osd pool create cold-storage 100 100 replicated replicated_ruleset</screen>
   </step>
   <step>
    <para>
     Then setting up a cache tier involves associating a backing storage pool
     with a cache pool, in this case cold-storage (= storage pool) with
     hot-storage (= cache pool):
    </para>
<screen>sudo ceph osd tier add cold-storage hot-storage</screen>
   </step>
   <step>
    <para>
     To set the cache mode to <quote>writeback</quote>, execute the following:
    </para>
<screen>sudo ceph osd tier cache-mode hot-storage writeback</screen>
    <para>
     For more information about cache modes, see
     <xref linkend="sec.ceph.tiered.cachemodes" role="internalbook"/>.
    </para>
    <para>
     Writeback cache tiers overlay the backing storage tier, so they require
     one additional step: you must direct all client traffic from the storage
     pool to the cache pool. To direct client traffic directly to the cache
     pool, execute the following for example:
    </para>
<screen>sudo ceph osd tier set-overlay cold-storage hot-storage</screen>
   </step>
  </procedure>

  <sect2 xml:id="cache.tier.configure">
   <title>Configuring a Cache Tier</title>
   <para>
    There are several options you can use to configure cache tiers. Use the
    following syntax:
   </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
   <sect3>
    <title>Target Size and Type</title>
    <para>
     Ceph's production cache tiers use a Bloom Filter for the
     <option>hit_set_type</option>:
    </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> hit_set_type bloom</screen>
    <para>
     The <option>hit_set_count</option> and <option>hit_set_period</option>
     define how much time each HitSet should cover, and how many such HitSets
     to store.
    </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> hit_set_count 12
sudo ceph osd pool set <replaceable>cachepool</replaceable> hit_set_period 14400
sudo ceph osd pool set <replaceable>cachepool</replaceable> target_max_bytes 1000000000000</screen>
    <note>
     <para>
      A larger <option>hit_set_count</option> results in more RAM consumed by
      the <systemitem class="process">ceph-osd</systemitem> process.
     </para>
    </note>
    <para>
     The <option>min_read_recency_for_promote</option> defines how many HitSets
     to check for the existence of an object when handling a read operation.
     The checking result is used to decide whether to promote the object
     asynchronously. Its value should be between 0 and
     <option>hit_set_count</option>. If set to 0, the object is always
     promoted. If set to 1, the current HitSet is checked. And if this object
     is in the current HitSet, it is promoted, otherwise not. For the other
     values, the exact number of archive HitSets are checked. The object is
     promoted if the object is found in any of the most recent
     <option>min_read_recency_for_promote</option> HitSets.
    </para>
    <para>
     You can set a similar parameter
     <option>min_write_recency_for_promote</option> for the write operation:
    </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> min_read_recency_for_promote 2
sudo ceph osd pool set <replaceable>cachepool</replaceable> min_write_recency_for_promote 2</screen>
    <note>
     <para>
      The longer the period and the higher the
      <option>min_read_recency_for_promote</option> and
      <option>min_write_recency_for_promote</option> values, the more RAM the
      <systemitem class="process">ceph-osd</systemitem> daemon consumes. In
      particular, when the agent is active to flush or evict cache objects, all
      <option>hit_set_count</option> HitSets are loaded into RAM.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Cache Sizing</title>
    <para>
     The cache tiering agent performs two main functions:
    </para>
    <variablelist>
     <varlistentry>
      <term>Flushing</term>
      <listitem>
       <para>
        The agent identifies modified (or dirty) objects and forwards them to
        the storage pool for long-term storage.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Evicting</term>
      <listitem>
       <para>
        The agent identifies objects that have not been modified (or clean) and
        evicts the least recently used among them from the cache.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <sect4 xml:id="cache.tier.config.absizing">
     <title>Absolute Sizing</title>
     <para>
      The cache tiering agent can flush or evict objects based upon the total
      number of bytes or the total number of objects. To specify a maximum
      number of bytes, execute the following:
     </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> target_max_bytes <replaceable>num_of_bytes</replaceable></screen>
     <para>
      To specify the maximum number of objects, execute the following:
     </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> target_max_objects <replaceable>num_of_bytes</replaceable></screen>
     <note>
      <para>
       Ceph is not able to determine the size of a cache pool automatically,
       so the configuration on the absolute size is required here, otherwise
       the flush/evict will not work. If you specify both limits, the cache
       tiering agent will begin flushing or evicting when either threshold is
       triggered.
      </para>
     </note>
     <note>
      <para>
       All client requests will be blocked only when
       <option>target_max_bytes</option> or <option>target_max_objects</option>
       reached.
      </para>
     </note>
    </sect4>
    <sect4 xml:id="cache.tier.config.relsizing">
     <title>Relative Sizing</title>
     <para>
      The cache tiering agent can flush or evict objects relative to the size
      of the cache pool (specified by <option>target_max_bytes</option> /
      <option>target_max_objects</option> in
      <xref linkend="cache.tier.config.absizing" role="internalbook"/>). When the cache pool
      consists of a certain percentage of modified (or dirty) objects, the
      cache tiering agent will flush them to the storage pool. To set the
      <option>cache_target_dirty_ratio</option>, execute the following:
     </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> cache_target_dirty_ratio <replaceable>0.0...1.0</replaceable></screen>
     <para>
      For example, setting the value to 0.4 will begin flushing modified
      (dirty) objects when they reach 40% of the cache pool's capacity:
     </para>
<screen>sudo ceph osd pool set hot-storage cache_target_dirty_ratio 0.4</screen>
     <para>
      When the dirty objects reach a certain percentage of its capacity, flush
      dirty objects with a higher speed. Use
      <option>cache_target_dirty_high_ratio</option>:
     </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> cache_target_dirty_high_ratio <replaceable>0.0..1.0</replaceable></screen>
     <para>
      When the cache pool reaches a certain percentage of its capacity, the
      cache tiering agent will evict objects to maintain free capacity. To set
      the <option>cache_target_full_ratio</option>, execute the following:
     </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> cache_target_full_ratio <replaceable>0.0..1.0</replaceable></screen>
    </sect4>
   </sect3>
   <sect3>
    <title>Cache Age</title>
    <para>
     You can specify the minimum age of an object before the cache tiering
     agent flushes a recently modified (or dirty) object to the backing storage
     pool:
    </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> cache_min_flush_age <replaceable>num_of_seconds</replaceable></screen>
    <para>
     You can specify the minimum age of an object before it will be evicted
     from the cache tier:
    </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> cache_min_evict_age <replaceable>num_of_seconds</replaceable></screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
 </part>
 <part xml:id="part.dataccess">
  <title>Accessing Cluster Data</title>
  <chapter xml:base="admin_ceph_gateway.xml" version="5.0" xml:id="cha.ceph.gw">

 <title>Ceph <phrase>RADOS Gateway</phrase></title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Ceph <phrase>RADOS Gateway</phrase> is an object storage interface built on top of
  <systemitem>librgw</systemitem> to provide applications with a RESTful
  gateway to Ceph Storage Clusters. Ceph Object Storage supports two
  interfaces:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>S3-compatible</emphasis>: Provides object storage functionality
    with an interface that is compatible with a large subset of the Amazon S3
    RESTful API.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Swift-compatible</emphasis>: Provides object storage
    functionality with an interface that is compatible with a large subset of
    the OpenStack Swift API.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Ceph Object Storage uses the Ceph <phrase>RADOS Gateway</phrase> daemon
  (<systemitem>radosgw</systemitem>), which uses an embedded HTTP server
  (CivetWeb) for interacting with a Ceph Storage Cluster. Since it provides
  interfaces compatible with OpenStack Swift and Amazon S3, the Ceph <phrase>RADOS Gateway</phrase>
  has its own user management. Ceph <phrase>RADOS Gateway</phrase> can store data in the same Ceph
  Storage Cluster used to store data from Ceph File System clients or Ceph
  Block Device clients. The S3 and Swift APIs share a common name space, so you
  may write data with one API and retrieve it with the other.
 </para>
 <para>
  This section helps you install and manage the Ceph <phrase>RADOS Gateway</phrase> (<phrase>RADOS Gateway</phrase>). You can
  either choose to use the <command>ceph-deploy</command> tool, or do the
  installation and management manually.
 </para>
 <important>
  <para>
   Before installing <phrase>RADOS Gateway</phrase>, you need to have the Ceph cluster installed first
   (see <xref linkend="cha.ceph.install" role="internalbook"/> for more information).
  </para>
 </important>
 <sect1 xml:id="ceph.rgw.cephdeploy">
  <title>Managing <phrase>RADOS Gateway</phrase> with <command>ceph-deploy</command></title>

  <para>
   This section describes how to install and configure <phrase>RADOS Gateway</phrase> with
   <command>ceph-deploy</command>.
  </para>

  <sect2 xml:id="ceph.rgw.cephdeploy.install">
   <title>Installation</title>
   <para>
    The <command>ceph-deploy</command> script includes the
    <command>rgw</command> component that helps you manage the <phrase>RADOS Gateway</phrase> creation
    and operation.
   </para>
   <important>
    <title>Install Ceph</title>
    <para>
     Before running <command>ceph-deploy rgw</command> as suggested in the
     following step, make sure that Ceph together with the object gateway
     package are correctly installed on the node where you want to setup <phrase>RADOS Gateway</phrase>:
    </para>
    <screen>ceph-deploy install --rgw <replaceable>short_rgw_hostname</replaceable>
</screen>
   </important>
   <para>
    Prepare and activate the nodes in one step. You can specify several pairs
    of
    <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable>
    to install <phrase>RADOS Gateway</phrase> on a required number of nodes.
   </para>
<screen>ceph-deploy --overwrite-conf rgw create \
 <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable> ...</screen>
   <para>
    For example:
   </para>
<screen>ceph-deploy --overwrite-conf rgw create ceph-node1:rgw.gateway1</screen>
   <para>
    You now have a working <phrase>RADOS Gateway</phrase> on the specified nodes, and you need to give
    access to a client. For more information, see
    <xref linkend="ceph.rgw.access" role="internalbook"/>.
   </para>
  </sect2>

  <sect2>
   <title>Listing <phrase>RADOS Gateway</phrase> Installations</title>
   <para>
    To list all <phrase>RADOS Gateway</phrase> instances within the Ceph cluster, run:
   </para>
<screen>ceph-deploy rgw list</screen>
  </sect2>

  <sect2>
   <title>Removing <phrase>RADOS Gateway</phrase> from a Node</title>
   <para>
    To remove a <phrase>RADOS Gateway</phrase> installation from the node where it was previously
    installed, run:
   </para>
<screen>ceph-deploy --overwrite-conf rgw delete  \
  <replaceable>short_hostname</replaceable>:<replaceable>gatewayname</replaceable> ...</screen>
   <para>
    For example:
   </para>
<screen>ceph-deploy --overwrite-conf rgw delete ceph-node1:rgw.gateway1</screen>
   <tip>
    <para>
     You need a copy of the local <command>ceph.conf</command> file, in your
     current working directory. If you do not have a copy of it, copy it from
     your cluster.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.manual">
  <title>Managing <phrase>RADOS Gateway</phrase> Manually</title>

  <para>
   This section describes how to install and configure <phrase>RADOS Gateway</phrase> manually.
  </para>

  <sect2>
   <title>Installation</title>
   <procedure>
    <step>
     <para>
      Install <phrase>RADOS Gateway</phrase>. The following command installs all required components:
     </para>
<screen>sudo zypper ref &amp;&amp; sudo zypper in ceph-radosgw</screen>
    </step>
    <step>
     <para>
      If the Apache server from the previous <phrase>RADOS Gateway</phrase> instance is running, stop it
      and disable the relevant service:
     </para>
<screen>sudo systemctl stop disable apache2.service</screen>
    </step>
    <step>
     <para>
      Edit <filename>/etc/ceph/ceph.conf</filename> and add the following
      lines:
     </para>
<screen>[client.rgw.gateway]
 rgw frontends = "civetweb port=80"</screen>
     <tip>
      <para>
       If you want to configure <phrase>RADOS Gateway</phrase>/CivetWeb for use with SSL encryption,
       modify the line accordingly:
      </para>
<screen>rgw frontends = civetweb port=7480s ssl_certificate=<replaceable>path_to_certificate.pem</replaceable></screen>
     </tip>
    </step>
    <step>
     <para>
      Restart the <phrase>RADOS Gateway</phrase> service. See <xref linkend="ceph.rgw.operating" role="internalbook"/> for
      more information.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ses.rgw.config">
   <title>Configuring <phrase>RADOS Gateway</phrase></title>
   <para>
    Several steps are required to configure a <phrase>RADOS Gateway</phrase>.
   </para>
   <sect3>
    <title>Basic Configuration</title>
    <para>
     Configuring a Ceph <phrase>RADOS Gateway</phrase> requires a running Ceph Storage Cluster. The
     Ceph <phrase>RADOS Gateway</phrase> is a client of the Ceph Storage Cluster. As a Ceph
     Storage Cluster client, it requires:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       A host name for the gateway instance, for example
       <systemitem>gateway</systemitem>.
      </para>
     </listitem>
     <listitem>
      <para>
       A storage cluster user name with appropriate permissions and a keyring.
      </para>
     </listitem>
     <listitem>
      <para>
       Pools to store its data.
      </para>
     </listitem>
     <listitem>
      <para>
       A data directory for the gateway instance.
      </para>
     </listitem>
     <listitem>
      <para>
       An instance entry in the Ceph Configuration file.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Each instance must have a user name and key to communicate with a Ceph
     storage cluster. In the following steps, we use a monitor node to create a
     bootstrap keyring, then create the <phrase>RADOS Gateway</phrase> instance user keyring based on
     the bootstrap one. Then, we create a client user name and key. Next, we
     add the key to the Ceph Storage Cluster. Finally, we distribute the
     keyring to the node containing the gateway instance.
    </para>
    <procedure>
     <step>
      <para>
       Create a keyring for the gateway:
      </para>
<screen>sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.rgw.keyring
sudo chmod +r /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Generate a Ceph <phrase>RADOS Gateway</phrase> user name and key for each instance. As an
       example, we will use the name <systemitem>gateway</systemitem> after
       <systemitem>client.radosgw</systemitem>:
      </para>
<screen>sudo ceph-authtool /etc/ceph/ceph.client.rgw.keyring \
  -n client.rgw.gateway --gen-key</screen>
     </step>
     <step>
      <para>
       Add capabilities to the key:
      </para>
<screen>sudo ceph-authtool -n client.rgw.gateway --cap osd 'allow rwx' \
  --cap mon 'allow rwx' /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Once you have created a keyring and key to enable the Ceph Object
       Gateway with access to the Ceph Storage Cluster, add the key to your
       Ceph Storage Cluster. For example:
      </para>
<screen>sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.rgw.gateway \
  -i /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Distribute the keyring to the node with the gateway instance:
      </para>
<screen>sudo scp /etc/ceph/ceph.client.rgw.keyring  ceph@<replaceable>hostname</replaceable>:/home/ceph
ssh <replaceable>hostname</replaceable>
sudo mv ceph.client.rgw.keyring /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
    </procedure>
    <tip>
     <title>Use Bootstrap Keyring</title>
     <para>
      An alternative way is to create the <phrase>RADOS Gateway</phrase> bootstrap keyring, and then
      create the <phrase>RADOS Gateway</phrase> keyring from it:
     </para>
     <procedure>
      <step>
       <para>
        Create a <phrase>RADOS Gateway</phrase> bootstrap keyring on one of the monitor nodes:
       </para>
<screen>sudo ceph \
 auth get-or-create client.bootstrap-rgw mon 'allow profile bootstrap-rgw' \
 --connect-timeout=25 \
 --cluster=ceph \
 --name mon. \
 --keyring=/var/lib/ceph/mon/ceph-<replaceable>node_host</replaceable>/keyring \
 -o /var/lib/ceph/bootstrap-rgw/keyring</screen>
      </step>
      <step>
       <para>
        Create the
        <filename>/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable></filename>
        directory for storing the bootstrap keyring:
       </para>
<screen>sudo mkdir \
/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable></screen>
      </step>
      <step>
       <para>
        Create a <phrase>RADOS Gateway</phrase> keyring from the newly created bootstrap keyring:
       </para>
<screen>sudo ceph \
 auth get-or-create client.rgw.<replaceable>rgw_name</replaceable> osd 'allow rwx' mon 'allow rw' \
 --connect-timeout=25 \
 --cluster=ceph \
 --name client.bootstrap-rgw \
 --keyring=/var/lib/ceph/bootstrap-rgw/keyring \
 -o /var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable>/keyring</screen>
      </step>
      <step>
       <para>
        Copy the <phrase>RADOS Gateway</phrase> keyring to the <phrase>RADOS Gateway</phrase> host:
       </para>
<screen>sudo scp \
/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable>/keyring \
<replaceable>rgw_host</replaceable>:/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable>/keyring</screen>
      </step>
     </procedure>
    </tip>
   </sect3>
   <sect3>
    <title>Create Pools (Optional)</title>
    <para>
     Ceph <phrase>RADOS Gateway</phrase>s require Ceph Storage Cluster pools to store specific
     gateway data. If the user you created has proper permissions, the gateway
     will create the pools automatically. However, ensure that you have set an
     appropriate default number of placement groups per pool in the Ceph
     configuration file.
    </para>
    <para>
     When configuring a gateway with the default region and zone, the naming
     convention for pools typically uses 'default' for region and zone naming,
     but you can use any naming convention you prefer:
    </para>
<screen>.rgw.root
default.rgw.control
default.rgw.data.root
default.rgw.gc
default.rgw.log
default.rgw.users.uid
default.rgw.users.email
default.rgw.users.keys
default.rgw.meta
default.rgw.users.swift</screen>
    <para>
     To create the pools manually, see
     <xref linkend="ceph.pools.operate.add_pool" role="internalbook"/>.
    </para>
   </sect3>
   <sect3>
    <title>Adding Gateway Configuration to Ceph</title>
    <para>
     Add the Ceph <phrase>RADOS Gateway</phrase> configuration to the Ceph Configuration file. The
     Ceph <phrase>RADOS Gateway</phrase> configuration requires you to identify the Ceph <phrase>RADOS Gateway</phrase>
     instance. Then, specify the host name where you installed the Ceph <phrase>RADOS Gateway</phrase>
     daemon, a keyring (for use with cephx), and optionally a log file. For
     example:
    </para>
<screen>[client.rgw.<replaceable>instance-name</replaceable>]
host = <replaceable>hostname</replaceable>
keyring = /etc/ceph/ceph.client.rgw.keyring</screen>
    <tip>
     <title><phrase>RADOS Gateway</phrase> Log File</title>
     <para>
      To override the default <phrase>RADOS Gateway</phrase> log file, include the follwing:
     </para>
<screen>log file = /var/log/radosgw/client.rgw.<replaceable>instance-name</replaceable>.log</screen>
    </tip>
    <para>
     The <literal>[client.rgw.*]</literal> portion of the gateway instance
     identifies this portion of the Ceph configuration file as configuring a
     Ceph Storage Cluster client where the client type is a Ceph <phrase>RADOS Gateway</phrase>
     (radosgw). The instance name follows. For example:
    </para>
<screen>[client.rgw.gateway]
host = ceph-gateway
keyring = /etc/ceph/ceph.client.rgw.keyring</screen>
    <note>
     <para>
      The <replaceable>host</replaceable> must be your machine host name,
      excluding the domain name.
     </para>
    </note>
    <para>
     Then turn off <literal>print continue</literal>. If you have it set to
     true, you may encounter problems with PUT operations:
    </para>
<screen>rgw print continue = false</screen>

    <para>
     To use a Ceph <phrase>RADOS Gateway</phrase> with subdomain S3 calls (for example
     <literal>http://bucketname.hostname</literal>), you must add the Ceph
     <phrase>RADOS Gateway</phrase> DNS name under the <literal>[client.rgw.gateway]</literal> section
     of the Ceph configuration file:
    </para>
<screen>[client.rgw.gateway]
...
rgw dns name = <replaceable>hostname</replaceable></screen>
    <para>
     You should also consider installing a DNS server such as Dnsmasq on your
     client machine(s) when using the
     <literal>http://<replaceable>bucketname</replaceable>.<replaceable>hostname</replaceable></literal>
     syntax. The <filename>dnsmasq.conf</filename> file should include the
     following settings:
    </para>
<screen>address=/<replaceable>hostname</replaceable>/<replaceable>host-ip-address</replaceable>
listen-address=<replaceable>client-loopback-ip</replaceable></screen>
    <para>
     Then, add the <replaceable>client-loopback-ip</replaceable> IP address as
     the first DNS server on the client machine(s).
    </para>
   </sect3>
   <sect3>
    <title>Redeploy Ceph Configuration</title>
    <para>
     Use <command>ceph-deploy</command> to push a new copy of the configuration
     to the hosts in your cluster:
    </para>
<screen>ceph-deploy config push <replaceable>host-name [host-name]...</replaceable></screen>
   </sect3>
   <sect3>
    <title>Create Data Directory</title>
    <para>
     Deployment scripts may not create the default Ceph <phrase>RADOS Gateway</phrase> data directory.
     Create data directories for each instance of a radosgw daemon if not
     already done. The <literal>host</literal> variables in the Ceph
     configuration file determine which host runs each instance of a radosgw
     daemon. The typical form specifies the radosgw daemon, the cluster name
     and the daemon ID.
    </para>
<screen>sudo mkdir -p /var/lib/ceph/radosgw/<replaceable>cluster</replaceable>-<replaceable>id</replaceable></screen>
    <para>
     Using the exemplary ceph.conf settings above, you would execute the
     following:
    </para>
<screen>sudo mkdir -p /var/lib/ceph/radosgw/ceph-radosgw.gateway</screen>
   </sect3>
   <sect3>
    <title>Restart Services and Start the Gateway</title>
    <para>
     To ensure that all components have reloaded their configurations, we
     recommend restarting your Ceph Storage Cluster service. Then, start up
     the <systemitem>radosgw</systemitem> service. For more information, see
     <xref linkend="cha.ceph.operating" role="internalbook"/> and
     <xref linkend="ceph.rgw.operating" role="internalbook"/>.
    </para>
    <para>
     After the service is up and running, you can make an anonymous GET request
     to see if the gateway returns a response. A simple HTTP request to the
     domain name should return the following:
    </para>
<screen>&lt;ListAllMyBucketsResult&gt;
      &lt;Owner&gt;
              &lt;ID&gt;anonymous&lt;/ID&gt;
              &lt;DisplayName/&gt;
      &lt;/Owner&gt;
      &lt;Buckets/&gt;
&lt;/ListAllMyBucketsResult&gt;</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.operating">
  <title>Operating the <phrase>RADOS Gateway</phrase> Service</title>

  <para>
   <phrase>RADOS Gateway</phrase> service is operated with the <command>systemctl</command> command. You
   need to have <systemitem class="username">root</systemitem> privileges to operate the <phrase>RADOS Gateway</phrase> service. Note that
   <replaceable>gateway_host</replaceable> is the host name of the server whose
   <phrase>RADOS Gateway</phrase> instance you need to operate.
  </para>

  <para>
   The following subcommands are supported for the <phrase>RADOS Gateway</phrase> service:
  </para>

  <variablelist>
   <varlistentry>
    <term>systemctl status ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Prints the status information of the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl start ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Starts the service if it is not already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl restart ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Restarts the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl stop ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Stops the running service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl enable ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Enables the service so that it is automatically started on system
      start-up.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl disable ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Disables the service so that it is not automatically started on system
      start-up.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ceph.rgw.access">
  <title>Managing <phrase>RADOS Gateway</phrase> Access</title>

  <para>
   You can communicate with <phrase>RADOS Gateway</phrase> using either S3- or Swift-compatible
   interface. Both interfaces require you to create a specific user, and
   install the relevant client software to communicate with the gateway using
   the user's secret key.
  </para>

  <para>
   For an introduction and a few practical examples on <phrase>RADOS Gateway</phrase> access, see
   <xref linkend="storage.bp.inst.rgw_client" role="internalbook"/>.
  </para>

  <sect2>
   <title>Managing S3 Access</title>
   <para>
    S3 interface is compatible with a large subset of the Amazon S3 RESTful
    API.
   </para>
   <tip>
    <para>
     S3cmd is a command line S3 client. You can find it in the
     <link xlink:href="https://build.opensuse.org/package/show/Cloud:Tools/s3cmd">OpenSUSE
     Build Service</link>. The repository contains versions for both SUSE Linux Enterprise and
     openSUSE based distributions.
    </para>
   </tip>
   <sect3>
    <title>Adding Users</title>
    <para>
     See <xref linkend="storage.bp.account.s3add" role="internalbook"/>.
    </para>
   </sect3>
   <sect3>
    <title>Removing Users</title>
    <para>
     See <xref linkend="storage.bp.account.s3rm" role="internalbook"/>.
    </para>
   </sect3>
   <sect3>
    <title>Changing User Passwords</title>
    <para>
     See <xref linkend="storage.bp.account.user_pwd" role="internalbook"/>.
    </para>
   </sect3>
   <sect3>
    <title>Setting Quotas</title>
    <para>
     See <xref linkend="storage.bp.account.s3quota" role="internalbook"/>.
    </para>
   </sect3>
  </sect2>

  <sect2>
   <title>Managing Swift Access</title>
   <para>
    Swift interface is compatible with a large subset of the <phrase>OpenStack</phrase> Swift
    API.
   </para>
   <sect3>
    <title>Adding Users</title>
    <para>
     See <xref linkend="storage.bp.account.swiftadd" role="internalbook"/>.
    </para>
   </sect3>
   <sect3>
    <title>Removing Users</title>
    <para>
     See <xref linkend="storage.bp.account.swiftrm" role="internalbook"/>.
    </para>
   </sect3>
   <sect3>
    <title>Changing Passwords</title>
    <para>
     See <xref linkend="storage.bp.account.user_pwd" role="internalbook"/>.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.fed">


  <title>Multisite Object Storage Gateways</title>

  <para>
   You can configure each <phrase>RADOS Gateway</phrase> to participate in a federated architecture,
   working in an active zone configuration while allowing for writes to
   non-master zones.
  </para>

  <sect2 xml:id="ceph.rgw.fed.term">
   <title>Terminology</title>
   <para>
    A description of terms specific to a federated architecture follows:
   </para>
   <variablelist>
    <varlistentry>
     <term>Zone</term>
     <listitem>
      <para>
       A logical grouping of one or more <phrase>RADOS Gateway</phrase> instances. There must be one
       zone designated as the <emphasis>master</emphasis> zone in a
       <emphasis>zonegroup</emphasis>, which handles all bucket and user
       creation.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Zonegroup</term>
     <listitem>
      <para>
       A zonegroup consists of multiple zones. There should be a master
       zonegroup that will handle changes to the system configuration.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Zonegroup map</term>
     <listitem>
      <para>
       A configuration structure that holds the map of the entire system, for
       example which zonegroup is the master, relationships between different
       zonegroups, and certain configuration options such as storage policies.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Realm</term>
     <listitem>
      <para>
       A container for zonegroups. This allows for separation of zonegroups
       between clusters. It is possible to create multiple realms, making it
       easier to run completely different configurations in the same cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Period</term>
     <listitem>
      <para>
       A period holds the configuration structure for the current state of the
       realm. Every period contains a unique ID and an epoch. Every realm has
       an associated current period, holding the current state of configuration
       of the zonegroups and storage policies. Any configuration change for a
       non-master zone will increment the period's epoch. Changing the master
       zone to a different zone will trigger the following changes:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         A new period is generated with a new period ID and epoch of 1.
        </para>
       </listitem>
       <listitem>
        <para>
         Realm's current period is updated to point to the newly generated
         period ID.
        </para>
       </listitem>
       <listitem>
        <para>
         Realm's epoch is incremented.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.intro">
   <title>Example Cluster Setup</title>
   <para>
    In this example, we will focus on creating a single zone group with three
    separate zones, which actively synchronize their data. Two zones belong to
    the same cluster, while the third belongs to a different one. There is no
    synchronization agent involved in mirroring data changes between the
    <phrase>RADOS Gateway</phrase>s. This allows for a much simpler configuration scheme and
    active-active configurations. Note that metadata operations—such as
    creating a new user—still need to go through the master zone.
    However, data operations—such as creation of buckets and
    objects—can be handled by any of the zones.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.keys">
   <title>System Keys</title>
   <para>
    While configuring zones, <phrase>RADOS Gateway</phrase> expects creation of an S3-compatible system
    user together with their access and secret keys. This allows another <phrase>RADOS Gateway</phrase>
    instance to pull the configuration remotely with the access and secret
    keys. For more information on creating S3 users, see
    <xref linkend="storage.bp.account.s3add" role="internalbook"/>.
   </para>
   <tip>
    <para>
     It is useful to generate the access and secret keys before the zone
     creation itself because it makes scripting and use of configuration
     management tools easier later on.
    </para>
   </tip>
   <para>
    For the purpose of this example, let us assume that the access and secret
    keys are set in the environment variables:
   </para>
<screen># SYSTEM_ACCESS_KEY=1555b35654ad1656d805
# SYSTEM_SECRET_KEY=h7GhxuBLTrlhVUyxSPUKUV8r/2EI4ngqJxD7iBdBYLhwluN30JaT3Q==</screen>
   <para>
    Generally, access keys consist of 20 alphanumeric characters, while secret
    keys consist of 40 alphanumeric characters (they can contain +/= characters
    as well). You can generate these keys in the command line:
   </para>
<screen># SYSTEM_ACCESS_KEY=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 20 | head -n 1)
# SYSTEM_SECRET_KEY=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 40 | head -n 1)</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.naming">
   <title>Naming Conventions</title>
   <para>
    This example describes the process of setting up a master zone. We will
    assume a zonegroup called <literal>us</literal> spanning the United States,
    which will be our master zonegroup. This will contain two zones written in
    a <replaceable>zonegroup</replaceable>-<replaceable>zone</replaceable>
    format. This is our convention only and you can choose a format that you
    prefer. In summary:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Master zonegroup: United States <literal>us</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Master zone: United States, East Region 1: <literal>us-east-1</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Secondary zone: United States, East Region 2:
      <literal>us-east-2</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Secondary zone: United States, West Region: <literal>us-west</literal>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    This will be a part of a larger realm named <literal>gold</literal>. The
    <literal>us-east-1</literal> and <literal>us-east-2</literal> zones are
    part of the same Ceph cluster, <literal>us-east-1</literal> being the
    primary one. <literal>us-west</literal> is in a different Ceph cluster.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.pools">
   <title>Default Pools</title>
   <para>
    When configured with the appropriate permissions, <phrase>RADOS Gateway</phrase> creates default
    pools on its own. The <literal>pg_num</literal> and
    <literal>pgp_num</literal> values are taken from the
    <filename>ceph.conf</filename> configuration file. Pools related to a zone
    by default follow the convention of
    <replaceable>zone-name</replaceable>.<replaceable>pool-name</replaceable>.
    For example for the <literal>us-east-1</literal> zone, it will be the
    following pools:
   </para>
<screen>.rgw.root
us-east-1.rgw.control
us-east-1.rgw.data.root
us-east-1.rgw.gc
us-east-1.rgw.log
us-east-1.rgw.intent-log
us-east-1.rgw.usage
us-east-1.rgw.users.keys
us-east-1.rgw.users.email
us-east-1.rgw.users.swift
us-east-1.rgw.users.uid
us-east-1.rgw.buckets.index
us-east-1.rgw.buckets.data
us-east-1.rgw.meta</screen>
   <para>
    These pools can be created in other zones as well, by replacing
    <literal>us-east-1</literal> with the appropriate zone name.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.realm">
   <title>Creating a Realm</title>
   <para>
    Configure a realm called <literal>gold</literal> and make it the default
    realm:
   </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin realm create --rgw-realm=gold --default
{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "epoch": 1
}</screen>
   <para>
    Note that every realm has an ID, which allows for flexibility such as
    renaming the realm later if needed. The <literal>current_period</literal>
    changes whenever we change anything in the master zone. The
    <literal>epoch</literal> is incremented when there is a change in the
    master zone's configuration which results in a change of the current
    period.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.deldefzonegrp">
   <title>Deleting the Default Zonegroup</title>
   <para>
    The default installation of <phrase>RADOS Gateway</phrase> creates the default zonegroup called
    <literal>default</literal>. Because we no longer need the default
    zonegroup, remove it.
   </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zonegroup delete --rgw-zonegroup=default</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.createmasterzonegrp">
   <title>Creating a Master Zonegroup</title>
   <para>
    Create a master zonegroup called <literal>us</literal>. The zonegroup will
    manage the zonegroup map and propagate changes to the rest of the system.
    By marking the zonegroup as default, you allow explicitly mentioning the
    rgw-zonegroup switch for later commands.
   </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zonegroup create --rgw-zonegroup=us \
--endpoints=http://rgw1:80 --master --default
{
  "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "name": "us",
  "api_name": "us",
  "is_master": "true",
  "endpoints": [
      "http:\/\/rgw1:80"
  ],
  "hostnames": [],
  "hostnames_s3website": [],
  "master_zone": "",
  "zones": [],
  "placement_targets": [],
  "default_placement": "",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   <para>
    Alternatively, you can mark a zonegroup as default with the following
    command:
   </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zonegroup default --rgw-zonegroup=us</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.masterzone">
   <title>Creating a Master Zone</title>
   <para>
    Now create a default zone and add it to the default zonegroup. Note that
    you will use this zone for metadata operations such as user creation:
   </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-east-1 \
--endpoints=http://rgw1:80 --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "name": "us-east-1",
  "domain_root": "us-east-1/gc.rgw.data.root",
  "control_pool": "us-east-1/gc.rgw.control",
  "gc_pool": "us-east-1/gc.rgw.gc",
  "log_pool": "us-east-1/gc.rgw.log",
  "intent_log_pool": "us-east-1/gc.rgw.intent-log",
  "usage_log_pool": "us-east-1/gc.rgw.usage",
  "user_keys_pool": "us-east-1/gc.rgw.users.keys",
  "user_email_pool": "us-east-1/gc.rgw.users.email",
  "user_swift_pool": "us-east-1/gc.rgw.users.swift",
  "user_uid_pool": "us-east-1/gc.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-1/gc.rgw.buckets.index",
              "data_pool": "us-east-1/gc.rgw.buckets.data",
              "data_extra_pool": "us-east-1/gc.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-1/gc.rgw.meta",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   <para>
    Note that the <option>--rgw-zonegroup</option> and
    <option>--default</option> switches add the zone to a zonegroup and make it
    the default zone. Alternatively, the same can also be done with the
    following commands:
   </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zone default --rgw-zone=us-east-1
<prompt>cephadm &gt; </prompt>radosgw-admin zonegroup add --rgw-zonegroup=us --rgw-zone=us-east-1</screen>
   <sect3 xml:id="ceph.rgw.fed.masterzone.createuser">
    <title>Creating System Users</title>
    <para>
     To access zone pools, you need to create a system user. Note that you will
     need these keys when configuring the secondary zone as well.
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin user create --uid=zone.user \
--display-name="Zone User" --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> \
--secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable> --system</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.masterzone.updateperiod">
    <title>Update the Period</title>
    <para>
     Because you changed the master zone configuration, you need to commit the
     changes for them to take effect in the realm configuration structure.
     Initially, the period looks like this:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin period get
{
  "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "epoch": 1, "predecessor_uuid": "", "sync_status": [], "period_map":
  {
    "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "zonegroups": [], "short_zone_ids": []
  }, "master_zonegroup": "", "master_zone": "", "period_config":
  {
     "bucket_quota": {
     "enabled": false, "max_size_kb": -1, "max_objects": -1
     }, "user_quota": {
       "enabled": false, "max_size_kb": -1, "max_objects": -1
     }
  }, "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7", "realm_name": "gold", "realm_epoch": 1
}</screen>
    <para>
     Update the period and commit the changes:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 1,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }
              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          }
      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.masterzone.startrgw">
    <title>Start the <phrase>RADOS Gateway</phrase></title>
    <para>
     You need to mention the <phrase>RADOS Gateway</phrase> zone and port options in the configuration
     file before starting the <phrase>RADOS Gateway</phrase>. For more information on <phrase>RADOS Gateway</phrase> and its
     configuration, see <xref linkend="cha.ceph.gw" role="internalbook"/>. The configuration
     section of <phrase>RADOS Gateway</phrase> should look similar to this:
    </para>
<screen>[client.rgw.us-east-1]
rgw_frontends="civetweb port=80"
rgw_zone=us-east-1</screen>
    <para>
     Start the <phrase>RADOS Gateway</phrase>:
    </para>
<screen>sudo systemctl start ceph-radosgw@rgw.us-east-1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.secondaryzone">
   <title>Creating a Secondary Zone</title>
   <para>
    In the same cluster, create and configure the secondary zone named
    <literal>us-east-2</literal>. You can execute all the following commands in
    the node hosting the master zone itself.
   </para>
   <para>
    To create the secondary zone, use the same command as when you created the
    primary zone, except dropping the master flag:
   </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zone create --rgw-zonegroup=us --endpoints=http://rgw2:80 \
--rgw-zone=us-east-2 --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-east-2",
  "domain_root": "us-east-2.rgw.data.root",
  "control_pool": "us-east-2.rgw.control",
  "gc_pool": "us-east-2.rgw.gc",
  "log_pool": "us-east-2.rgw.log",
  "intent_log_pool": "us-east-2.rgw.intent-log",
  "usage_log_pool": "us-east-2.rgw.usage",
  "user_keys_pool": "us-east-2.rgw.users.keys",
  "user_email_pool": "us-east-2.rgw.users.email",
  "user_swift_pool": "us-east-2.rgw.users.swift",
  "user_uid_pool": "us-east-2.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-2.rgw.buckets.index",
              "data_pool": "us-east-2.rgw.buckets.data",
              "data_extra_pool": "us-east-2.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-2.rgw.meta",
  "realm_id": "815d74c2-80d6-4e63-8cfc-232037f7ff5c"
}</screen>
   <sect3 xml:id="ceph.rgw.fed.secondzone.updateperiod">
    <title>Update the Period</title>
    <para>
     Inform all the gateways of the new change in the system map by doing a
     period update and committing the changes:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }

              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          }

      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.secondzone.startrgw">
    <title>Start the <phrase>RADOS Gateway</phrase></title>
    <para>
     Adjust the configuration of the <phrase>RADOS Gateway</phrase> for the secondary zone, and start
     it:
    </para>
<screen>[client.rgw.us-east-2]
rgw_frontends="civetweb port=80"
rgw_zone=us-east-2</screen>
<screen><prompt>cephadm &gt; </prompt>sudo systemctl start ceph-radosgw@rgw.us-east-2</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.seccluster">
   <title>Adding <phrase>RADOS Gateway</phrase> to the Second Cluster</title>
   <para>
    The second Ceph cluster belongs to the same zonegroup as the initial one,
    but may be geographically located elsewhere.
   </para>
   <sect3 xml:id="ceph.rgw.fed.seccluster.realm">
    <title>Default Realm and Zonegroup</title>
    <para>
     Since you already created the realm for the first gateway, pull the realm
     here and make it the default here:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin realm pull --url=http://rgw1:80 \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2
}
<prompt>cephadm &gt; </prompt>radosgw-admin realm default --rgw-realm=gold</screen>
    <para>
     Get the configuration from the master zone by pulling the period:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin period pull --url=http://rgw1:80 \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable></screen>
    <para>
     Set the default zonegroup to the already created <literal>us</literal>
     zonegroup:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zonegroup default --rgw-zonegroup=us</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.seczone">
    <title>Secondary Zone Configuration</title>
    <para>
     Create a new zone named <literal>us-west</literal> with the same system
     keys:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-west \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable> \
--endpoints=http://rgw3:80 --default
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-west",
  "domain_root": "us-west.rgw.data.root",
  "control_pool": "us-west.rgw.control",
  "gc_pool": "us-west.rgw.gc",
  "log_pool": "us-west.rgw.log",
  "intent_log_pool": "us-west.rgw.intent-log",
  "usage_log_pool": "us-west.rgw.usage",
  "user_keys_pool": "us-west.rgw.users.keys",
  "user_email_pool": "us-west.rgw.users.email",
  "user_swift_pool": "us-west.rgw.users.swift",
  "user_uid_pool": "us-west.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-west.rgw.buckets.index",
              "data_pool": "us-west.rgw.buckets.data",
              "data_extra_pool": "us-west.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-west.rgw.meta",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.period">
    <title>Update the Period</title>
    <para>
     To propagate the zonegroup map changes, we update and commit the period:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin period update --commit --rgw-zone=us-west
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 3,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [
      "", # truncated
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "d9522067-cb7b-4129-8751-591e45815b16",
                      "name": "us-west",
                      "endpoints": [
                          "http:\/\/rgw3:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }
              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          },
          {
              "key": "d9522067-cb7b-4129-8751-591e45815b16",
              "val": 329470157
          }
      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
    <para>
     Note that the period epoch number has incremented, indicating a change in
     the configuration.
    </para>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.rgwstart">
    <title>Start the <phrase>RADOS Gateway</phrase></title>
    <para>
     This is similar to starting the <phrase>RADOS Gateway</phrase> in the first zone. The only
     difference is that the <phrase>RADOS Gateway</phrase> zone configuration should reflect the
     <literal>us-west</literal> zone name:
    </para>
<screen>[client.rgw.us-west]
rgw_frontends="civetweb port=80"
rgw_zone=us-west</screen>
    <para>
     Start the second <phrase>RADOS Gateway</phrase>:
    </para>
<screen>sudo systemctl start ceph-radosgw@rgw.us-west</screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
  <chapter xml:base="admin_ceph_iscsi.xml" version="5.0" xml:id="cha.ceph.iscsi">


 <title>Ceph iSCSI Gateway</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  iSCSI is a storage area network (SAN) protocol that allows clients (called
  <emphasis>initiators</emphasis>) to send SCSI commands to SCSI storage
  devices (<emphasis>targets</emphasis>) on remote servers. SUSE Enterprise Storage includes
  a facility that opens Ceph storage management to heterogeneous clients,
  such as Microsoft Windows* and VMware* vSphere, through the iSCSI protocol. Multipath
  iSCSI access enables availability and scalability for these clients, and the
  standardized iSCSI protocol also provides an additional layer of security
  isolation between clients and the SUSE Enterprise Storage cluster. The configuration
  facility is named <systemitem>lrbd</systemitem>. Using
  <systemitem>lrbd</systemitem>, Ceph storage administrators can define
  thin-provisioned, replicated, highly-available volumes supporting read-only
  snapshots, read-write clones, and automatic resizing with Ceph RADOS Block
  Device (RBD). Administrators can then export volumes either via a single
  <systemitem>lrbd</systemitem> gateway host, or via multiple gateway hosts
  supporting multipath failover. Linux, Microsoft Windows, and VMware hosts can connect
  to volumes using the iSCSI protocol, which makes them available like any
  other SCSI block device. This means SUSE Enterprise Storage customers can effectively run a
  complete block-storage infrastructure subsystem on Ceph that provides all
  features and benefits of a conventional SAN enabling future growth.
 </para>
 <para>
  This chapter introduces detailed information to set up a Ceph cluster
  infrastructure together with an iSCSI gateway so that the client hosts can
  use remotely stored data as local storage devices using the iSCSI protocol.
 </para>
 <sect1 xml:id="ceph.iscsi.iscsi">
  <title>iSCSI Block Storage</title>

  <para>
   iSCSI is an implementation of the Small Computer System Interface (SCSI)
   command set using the Internet Protocol (IP), specified in RFC 3720. iSCSI
   is implemented as a service where a client (the initiator) talks to a server
   (the target) via a session on TCP port 3260. An iSCSI target's IP address
   and port are called an iSCSI portal, where a target can be exposed through
   one or more portals. The combination of a target and one or more portals is
   called the target portal group (TPG).
  </para>

  <para>
   The underlying data link layer protocol for iSCSI is commonly Ethernet. More
   specifically, modern iSCSI infrastructures use 10 Gigabit Ethernet or faster
   networks for optimal throughput. 10 Gigabit Ethernet connectivity between
   the iSCSI gateway and the back-end Ceph cluster is strongly recommended.
  </para>

  <sect2 xml:id="ceph.iscsi.iscsi.target">
   <title>The Linux Kernel iSCSI Target</title>
   <para>
    The Linux kernel iSCSI target was originally named LIO for linux-iscsi.org,
    the project's original domain and Web site. For some time, no fewer than 4
    competing iSCSI target implementations were available for the Linux
    platform, but LIO ultimately prevailed as the single iSCSI reference
    target. The mainline kernel code for LIO uses the simple, but somewhat
    ambiguous name "target", distinguishing between "target core" and a variety
    of front-end and back-end target modules.
   </para>
   <para>
    The most commonly used front-end module is arguably iSCSI. However, LIO
    also supports Fibre Channel (FC), Fibre Channel over Ethernet (FCoE) and
    several other front-end protocols. At this time, only the iSCSI protocol is
    supported by SUSE Enterprise Storage.
   </para>
   <para>
    The most frequently used target back-end module is one that is capable of
    simply re-exporting any available block device on the target host. This
    module is named iblock. However, LIO also has an RBD-specific back-end
    module supporting parallelized multipath I/O access to RBD images.
   </para>
  </sect2>

  <sect2 xml:id="ceph.iscsi.iscsi.initiators">
   <title>iSCSI Initiators</title>
   <para>
    This section introduces a brief information on iSCSI initiators used on
    Linux, Microsoft Windows, and VMware platforms.
   </para>
   <sect3>
    <title>Linux</title>
    <para>
     The standard initiator for the Linux platform is
     <systemitem>open-iscsi</systemitem>. <systemitem>open-iscsi</systemitem>
     launches a daemon, <systemitem>iscsid</systemitem>, which the user can
     then use to discover iSCSI targets on any given portal, log in to targets,
     and map iSCSI volumes. <systemitem>iscsid</systemitem> communicates with
     the SCSI mid layer to create in-kernel block devices that the kernel can
     then treat like any other SCSI block device on the system. The
     <systemitem>open-iscsi</systemitem> initiator can be deploying in
     conjunction with the Device Mapper Multipath
     (<systemitem>dm-multipath</systemitem>) facility to provide a highly
     available iSCSI block device.
    </para>
   </sect3>
   <sect3>
    <title>Microsoft Windows and Hyper-V</title>
    <para>
     The default iSCSI initiator for the Microsoft Windows operating system is the
     Microsoft iSCSI initiator. The iSCSI service can be configured via a
     graphical user interface (GUI), and supports multipath I/O for high
     availability.
    </para>
   </sect3>
   <sect3>
    <title>VMware</title>
    <para>
     The default iSCSI initiator for VMware vSphere and ESX is the VMware
     ESX software iSCSI initiator, <systemitem>vmkiscsi</systemitem>. When
     enabled, it can be configured either from the vSphere client, or using the
     <command>vmkiscsi-tool</command> command. You can then format storage
     volumes connected through the vSphere iSCSI storage adapter with VMFS, and
     use them like any other VM storage device. The VMware initiator also
     supports multipath I/O for high availability.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.iscsi.lrdb">
  <title>General Information about lrdb</title>

  <para>
   <systemitem>lrbd</systemitem> combines the benefits of RADOS Block Devices
   with the ubiquitous versatility of iSCSI. By employing
   <systemitem>lrbd</systemitem> on an iSCSI target host (known as the
   <systemitem>lrbd</systemitem> gateway), any application that needs to make
   use of block storage can benefit from Ceph, even if it does not speak any
   Ceph client protocol. Instead, users can use iSCSI or any other target
   front-end protocol to connect to an LIO target, which translates all target
   I/O to RBD storage operations.
  </para>

  <figure>
   <title>Ceph Cluster with a Single iSCSI Gateway</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="lrbd_scheme1.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="lrbd_scheme1.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   <systemitem>lrbd</systemitem> is inherently highly-available and supports
   multipath operations. Thus, downstream initiator hosts can use multiple
   iSCSI gateways for both high availability and scalability. When
   communicating with an iSCSI configuration with more than one gateway,
   initiators may load-balance iSCSI requests across multiple gateways. In the
   event of a gateway failing, being temporarily unreachable, or being disabled
   for maintenance, I/O will transparently continue via another gateway.
  </para>

  <figure>
   <title>Ceph Cluster with Multiple iSCSI Gateways</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="lrbd_scheme2.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="lrbd_scheme2.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1 xml:id="ceph.iscsi.deploy">
  <title>Deployment Considerations</title>

  <para>
   A minimum configuration of SUSE Enterprise Storage with <systemitem>lrbd</systemitem>
   consists of the following components:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A Ceph storage cluster. The Ceph cluster consists of a minimum of four
     physical servers hosting at least eight object storage daemons (OSDs)
     each. In such a configuration, three OSD nodes also double as a monitor
     (MON) host.
    </para>
   </listitem>
   <listitem>
    <para>
     An iSCSI target server running the LIO iSCSI target, configured via
     <systemitem>lrbd</systemitem>.
    </para>
   </listitem>
   <listitem>
    <para>
     An iSCSI initiator host, running <systemitem>open-iscsi</systemitem>
     (Linux), the Microsoft iSCSI Initiator (Microsoft Windows), or any other compatible
     iSCSI initiator implementation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   A recommended production configuration of SUSE Enterprise Storage with
   <systemitem>lrbd</systemitem> consists of:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A Ceph storage cluster. A production Ceph cluster consists of any
     number of (typically more than 10) OSD nodes, each typically running 10-12
     object storage daemons (OSDs), with no fewer than three dedicated MON
     hosts.
    </para>
   </listitem>
   <listitem>
    <para>
     Several iSCSI target servers running the LIO iSCSI target, configured via
     <systemitem>lrbd</systemitem>. For iSCSI fail-over and load-balancing,
     these servers must run a kernel supporting the
     <systemitem>target_core_rbd</systemitem> module. Updates packages are
     available from the SUSE Linux Enterprise Server maintenance channel.
    </para>
   </listitem>
   <listitem>
    <para>
     Any number of iSCSI initiator hosts, running
     <systemitem>open-iscsi</systemitem> (Linux), the Microsoft iSCSI Initiator
     (Microsoft Windows), or any other compatible iSCSI initiator implementation.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph.iscsi.install">
  <title>Installation and Configuration</title>

  <para>
   This section describes steps to install and configure iSCSI gateway on top
   of SUSE Enterprise Storage.
  </para>

  <sect2>
   <title>Install SUSE Enterprise Storage and Deploy a Ceph Cluster</title>
   <para>
    Before you start installing and configuring an iSCSI gateway, you need to
    install SUSE Enterprise Storage and deploy a Ceph cluster as described in
    <xref linkend="cha.ceph.install" role="internalbook"/>.
   </para>
  </sect2>

  <sect2>
   <title>Installing the <systemitem>ceph_iscsi</systemitem> Pattern</title>
   <para>
    On your designated iSCSI target server nodes, install the
    <systemitem>ceph_iscsi</systemitem> pattern. Doing so will automatically
    install <systemitem>lrbd</systemitem>, the necessary Ceph binaries and
    libraries, and the <command>targetcli</command> command line tool:
   </para>
<screen>sudo zypper in -t pattern ceph_iscsi</screen>
   <para>
    Repeat this step on any node that you want to act as a fail-over or
    load-balancing target server node.
   </para>
  </sect2>

  <sect2>
   <title>Create RBD Images</title>
   <para>
    RBD images are created in the Ceph store and subsequently exported to
    iSCSI. We recommend that you use a dedicated RADOS pool for this purpose.
    You can create a volume from any host that is able to connect to your
    storage cluster using the Ceph <command>rbd</command> command line
    utility. This requires the client to have at least a minimal ceph.conf
    configuration file, and appropriate CephX authentication credentials.
   </para>
   <para>
    To create a new volume for subsequent export via iSCSI, use the
    <command>rbd create</command> command, specifying the volume size in
    megabytes. For example, in order to create a 100GB volume named
    <literal>testvol</literal> in the pool named <literal>iscsi</literal>, run:
   </para>
<screen>rbd --pool iscsi create --size=102400 testvol</screen>
   <para>
    The above command creates an RBD volume in the default format 2.
   </para>
   <note>
    <para>
     Since SUSE Enterprise Storage 3, the default volume format is 2, and format 1 is
     deprecated. However, you can still create the deprecated format 1 volumes
     with the <option>--image-format 1</option> option.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ceph.iscsi.rbd.export">
   <title>Export RBD Images via iSCSI</title>
   <para>
    To export RBD images via iSCSI, use the <systemitem>lrbd</systemitem>
    utility. <systemitem>lrbd</systemitem> allows you to create, review, and
    modify the iSCSI target configuration, which uses a JSON format.
   </para>
   <para>
    In order to edit the configuration, use <command>lrbd -e</command> or
    <command>lrbd --edit</command>. This command will invoke the default
    editor, as defined by the <literal>EDITOR</literal> environment variable.
    You may override this behavior by setting the <option>-E</option> option in
    addition to <option>-e</option>.
   </para>
   <para>
    Below is an example configuration for
   </para>
   <itemizedlist>
    <listitem>
     <para>
      two iSCSI gateway hosts named <literal>iscsi1.example.com</literal> and
      <literal>iscsi2.example.com</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      defining a single iSCSI target with an iSCSI Qualified Name (IQN) of
      <literal>iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      with a single iSCSI Logical Unit (LU),
     </para>
    </listitem>
    <listitem>
     <para>
      backed by an RBD image named <literal>testvol</literal> in the RADOS pool
      <literal>rbd</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      and exporting the target via two portals named "east" and "west":
     </para>
    </listitem>
   </itemizedlist>
<screen>{
    "auth": [
        {
            "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol",
            "authentication": "none"
        }
    ],
    "targets": [
        {
            "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol",
            "hosts": [
                {
                    "host": "iscsi1.example.com",
                    "portal": "east"
                },
                {
                    "host": "iscsi2.example.com",
                    "portal": "west"
                }
            ]
        }
    ],
    "portals": [
        {
            "name": "east",
            "addresses": [
                "192.168.124.104"
            ]
        },
        {
            "name": "west",
            "addresses": [
                "192.168.124.105"
            ]
        }
    ],
    "pools": [
        {
            "pool": "rbd",
            "gateways": [
                {
                    "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol",
                    "tpg": [
                        {
                            "image": "testvol"
                        }
                    ]
                }
            ]
        }
    ]
    }</screen>
   <para>
    Note that whenever you refer to a host name in the configuration, this host
    name must match the iSCSI gateway's <command>uname -n</command> command
    output.
   </para>
   <para>
    The edited JSON is stored in the extended attributes (xattrs) of a single
    RADOS object per pool. This object is available to the gateway hosts where
    the JSON is edited, and all gateway hosts connected to the same Ceph
    cluster. No configuration information is stored locally on the
    <systemitem>lrbd</systemitem> gateway.
   </para>
   <para>
    To activate the configuration, store it in the Ceph cluster, and do one
    of the following things (as <systemitem class="username">root</systemitem>):
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Run the <command>lrbd</command> command (without additional options) from
      the command line,
     </para>
    </listitem>
   </itemizedlist>
   <para>
    or
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Restart the <systemitem>lrbd</systemitem> service with <command>service
      lrbd restart</command>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The <systemitem>lrbd</systemitem> "service" does not operate any background
    daemon. Instead, it simply invokes the <command>lrbd</command> command.
    This type of service is known as a "one-shot" service.
   </para>
   <para>
    You should also enable <systemitem>lrbd</systemitem> to auto-configure on
    system start-up. To do so, run the <command>systemctl enable lrbd</command>
    command.
   </para>
   <para>
    The configuration above reflects a simple, one-gateway setup.
    <systemitem>lrbd</systemitem> configuration can be much more complex and
    powerful. The <systemitem>lrbd</systemitem> RPM package comes with an
    extensive set of configuration examples, which you may refer to by checking
    the contents of the
    <filename>/usr/share/doc/packages/lrbd/samples</filename> directory after
    installation. The samples are also available from
    <link xlink:href="https://github.com/SUSE/lrbd/tree/master/samples"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph.iscsi.rbd.optional">
   <title>Optional Settings</title>
   <para>
    The following settings may be useful for some environments. For images,
    there are <option>uuid</option>, <option>lun</option>,
    <option>retries</option>, <option>sleep</option>, and
    <option>retry_errors</option> attributes. The first
    two—<option>uuid</option> and <option>lun</option>—allow
    hardcoding of the 'uuid' or 'lun' for a specific image. You can specify
    either of them for an image. The <option>retries</option>,
    <option>sleep</option> and <option>retry_errors</option> affect the
    attempts to map an rbd image.
   </para>
<screen>"pools": [
    {
        "pool": "rbd",
        "gateways": [
        {
        "host": "igw1",
        "tpg": [
                    {
                        "image": "archive",
                        "uuid": "12345678-abcd-9012-efab-345678901234",
                        "lun": "2",
                        "retries": "3",
                        "sleep": "4",
                        "retry_errors": [ 95 ],
                        [...]
                    }
                ]
            }
        ]
    }
]</screen>
  </sect2>

  <sect2 xml:id="ceph.iscsi.rbd.advanced">
   <title>Advanced Settings</title>
   <para>
    <systemitem>lrdb</systemitem> can be configured with advanced parameters
    which are subsequently passed on to the LIO I/O target. The parameters are
    divided up into iSCSI and backing store components, which can then be
    specified in the "targets" and "tpg" sections, respectively, of the
    <systemitem>lrbd</systemitem> configuration.
   </para>
   <warning>
    <para>
     Changing these parameters from the default setting is not recommended.
    </para>
   </warning>
<screen>"targets": [
    {
        [...]
        "tpg_default_cmdsn_depth": "64",
        "tpg_default_erl": "0",
        "tpg_login_timeout": "10",
        "tpg_netif_timeout": "2",
        "tpg_prod_mode_write_protect": "0",
    }
]</screen>
   <para>
    Description of the options follows:
   </para>
   <variablelist>
    <varlistentry>
     <term>tpg_default_cmdsn_depth</term>
     <listitem>
      <para>
       Default CmdSN (Command Sequence Number) depth. Limits the amount of
       requests that an iSCSI initiator can have outstanding at any moment.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_default_erl</term>
     <listitem>
      <para>
       Default error recovery level.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_login_timeout</term>
     <listitem>
      <para>
       Login timeout value in seconds.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_netif_timeout</term>
     <listitem>
      <para>
       NIC failure timeout in seconds.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_prod_mode_write_protect</term>
     <listitem>
      <para>
       If set to 1, prevent writes to LUNs.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
<screen>"pools": [
    {
        "pool": "rbd",
        "gateways": [
        {
        "host": "igw1",
        "tpg": [
                    {
                        "image": "archive",
                        "backstore_block_size": "512",
                        "backstore_emulate_3pc": "1",
                        "backstore_emulate_caw": "1",
                        "backstore_emulate_dpo": "0",
                        "backstore_emulate_fua_read": "0",
                        "backstore_emulate_fua_write": "1",
                        "backstore_emulate_model_alias": "0",
                        "backstore_emulate_rest_reord": "0",
                        "backstore_emulate_tas": "1",
                        "backstore_emulate_tpu": "0",
                        "backstore_emulate_tpws": "0",
                        "backstore_emulate_ua_intlck_ctrl": "0",
                        "backstore_emulate_write_cache": "0",
                        "backstore_enforce_pr_isids": "1",
                        "backstore_fabric_max_sectors": "8192",
                        "backstore_hw_block_size": "512",
                        "backstore_hw_max_sectors": "8192",
                        "backstore_hw_pi_prot_type": "0",
                        "backstore_hw_queue_depth": "128",
                        "backstore_is_nonrot": "1",
                        "backstore_max_unmap_block_desc_count": "1",
                        "backstore_max_unmap_lba_count": "8192",
                        "backstore_max_write_same_len": "65535",
                        "backstore_optimal_sectors": "8192",
                        "backstore_pi_prot_format": "0",
                        "backstore_pi_prot_type": "0",
                        "backstore_queue_depth": "128",
                        "backstore_unmap_granularity": "8192",
                        "backstore_unmap_granularity_alignment": "4194304"
                    }
                ]
            }
        ]
    }
]</screen>
   <para>
    Description of the options follows:
   </para>
   <variablelist>
    <varlistentry>
     <term>backstore_block_size</term>
     <listitem>
      <para>
       Block size of the underlying device.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_3pc</term>
     <listitem>
      <para>
       If set to 1, enable Third Party Copy.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_caw</term>
     <listitem>
      <para>
       If set to 1, enable Compare and Write.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_dpo</term>
     <listitem>
      <para>
       If set to 1, turn on Disable Page Out.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_fua_read</term>
     <listitem>
      <para>
       If set to 1, enable Force Unit Access read.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_fua_write</term>
     <listitem>
      <para>
       If set to 1, enable Force Unit Access write.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_model_alias</term>
     <listitem>
      <para>
       If set to 1, use the back-end device name for the model alias.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_rest_reord</term>
     <listitem>
      <para>
       If set to 0, the Queue Algorithm Modifier is Restricted Reordering.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_tas</term>
     <listitem>
      <para>
       If set to 1, enable Task Aborted Status.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_tpu</term>
     <listitem>
      <para>
       If set to 1, enable Thin Provisioning Unmap.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_tpws</term>
     <listitem>
      <para>
       If set to 1, enable Thin Provisioning Write Same.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_ua_intlck_ctrl</term>
     <listitem>
      <para>
       If set to 1, enable Unit Attention Interlock.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_write_cache</term>
     <listitem>
      <para>
       If set to 1, turn on Write Cache Enable.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_enforce_pr_isids</term>
     <listitem>
      <para>
       If set to 1, enforce persistent reservation ISIDs.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_fabric_max_sectors</term>
     <listitem>
      <para>
       Maximum number of sectors the fabric can transfer at once.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_block_size</term>
     <listitem>
      <para>
       Hardware block size in bytes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_max_sectors</term>
     <listitem>
      <para>
       Maximum number of sectors the hardware can transfer at once.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_pi_prot_type</term>
     <listitem>
      <para>
       If non-zero, DIF protection is enabled on the underlying hardware.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_queue_depth</term>
     <listitem>
      <para>
       Hardware queue depth.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_is_nonrot</term>
     <listitem>
      <para>
       If set to 1, the backstore is a non rotational device.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_max_unmap_block_desc_count</term>
     <listitem>
      <para>
       Maximum number of block descriptors for UNMAP.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>"backstore_max_unmap_lba_count":</term>
     <listitem>
      <para>
       Maximum number of LBA for UNMAP.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_max_write_same_len</term>
     <listitem>
      <para>
       Maximum length for WRITE_SAME.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_optimal_sectors</term>
     <listitem>
      <para>
       Optimal request size in sectors.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_pi_prot_format</term>
     <listitem>
      <para>
       DIF protection format.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_pi_prot_type</term>
     <listitem>
      <para>
       DIF protection type.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_queue_depth</term>
     <listitem>
      <para>
       Queue depth.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_unmap_granularity</term>
     <listitem>
      <para>
       UNMAP granularity.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_unmap_granularity_alignment</term>
     <listitem>
      <para>
       UNMAP granularity alignment.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For targets, the <option>tpg</option> attributes allow tuning of kernel
    parameters. Use with caution.
   </para>
<screen>"targets": [
{
    "host": "igw1",
    "target": "iqn.2003-01.org.linux-iscsi.generic.x86:sn.abcdefghijk",
    "tpg_login_timeout": "10",
    "tpg_default_cmdsn_depth": "64",
    "tpg_default_erl": "0",
    "tpg_login_timeout": "10",
    "tpg_netif_timeout": "2",
    "tpg_prod_mode_write_protect": "0",
    "tpg_t10_pi": "0"
}</screen>
   <tip>
    <para>
     If a site needs statically assigned LUNs, then assign numbers to each LUN.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.iscsi.connect">
  <title>Connecting to lrbd-managed Targets</title>

  <para>
   This chapter describes how to connect to lrdb-managed targets from clients
   running Linux, Microsoft Windows, or VMware.
  </para>

  <sect2 xml:id="ceph.iscsi.connect.linux">
   <title>Linux (<systemitem>open-iscsi</systemitem>)</title>
   <para>
    Connecting to lrbd-backed iSCSI targets with
    <systemitem>open-iscsi</systemitem> is a two-step process. First the
    initiator must discover the iSCSI targets available on the gateway host,
    then it must log in and map the available Logical Units (LUs).
   </para>
   <para>
    Both steps require that the <systemitem>open-iscsi</systemitem> daemon is
    running. The way you start the <systemitem>open-iscsi</systemitem> daemon
    is dependent on your Linux distribution:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      On SUSE Linux Enterprise Server (SLES); and Red Hat Enterprise Linux (RHEL) hosts, run <command>systemctl start
      iscsid</command> (or <command>service iscsid start</command> if
      <command>systemctl</command> is not available).
     </para>
    </listitem>
    <listitem>
     <para>
      On Debian and Ubuntu hosts, run <command>systemctl start
      open-iscsi</command> (or <command>service open-iscsi start</command>).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If your initiator host runs SUSE Linux Enterprise Server, refer to
    <link xlink:href="https://www.suse.com/documentation/sles-12/stor_admin/data/sec_iscsi_initiator.html"/>
    or
    <link xlink:href="https://www.suse.com/documentation/sles11/stor_admin/data/sec_inst_system_iscsi_initiator.html"/>
    for details on how to connect to an iSCSI target.
   </para>
   <para>
    For any other Linux distribution supporting
    <systemitem>open-iscsi</systemitem>, proceed to discover targets on your
    <systemitem>lrbd</systemitem> gateway (this example uses iscsi1.example.com
    as the portal address; for multipath access repeat these steps with
    iscsi2.example.com):
   </para>
<screen>iscsiadm -m discovery -t sendtargets -p iscsi1.example.com
192.168.124.104:3260,1 iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol</screen>
   <para>
    Then, log in to the portal. If the login completes successfully, any
    RBD-backed logical units on the portal will immediately become available on
    the system SCSI bus:
   </para>
<screen>iscsiadm -m node -p iscsi1.example.com --login
Logging in to [iface: default, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] (multiple)
Login to [iface: default, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] successful.</screen>
   <para>
    Repeat this process for other portal IP addresses or hosts.
   </para>
   <para>
    If your system has the <systemitem>lsscsi</systemitem> utility installed,
    you use it to enumerate available SCSI devices on your system:
   </para>
<screen>lsscsi
[8:0:0:0]    disk    SUSE     RBD              4.0   /dev/sde
[9:0:0:0]    disk    SUSE     RBD              4.0   /dev/sdf</screen>
   <para>
    In a multipath configuration (where two connected iSCSI devices represent
    one and the same LU), you can also examine the multipath device state with
    the <systemitem>multipath</systemitem> utility:
   </para>
<screen>multipath -ll
360014050cf9dcfcb2603933ac3298dca dm-9 SUSE,RBD
size=49G features='0' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=1 status=active
| `- 8:0:0:0 sde 8:64 active ready running
`-+- policy='service-time 0' prio=1 status=enabled
`- 9:0:0:0 sdf 8:80 active ready running</screen>
   <para>
    You can now use this multipath device as you would any block device. For
    example, you can use the device as a Physical Volume for Linux Logical
    Volume Management (LVM), or you can simply create a file system on it. The
    example below demonstrates how to create an XFS file system on the newly
    connected multipath iSCSI volume:
   </para>
<screen>mkfs -t xfs /dev/mapper/360014050cf9dcfcb2603933ac3298dca
log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
log stripe unit adjusted to 32KiB
meta-data=/dev/mapper/360014050cf9dcfcb2603933ac3298dca isize=256    agcount=17, agsize=799744 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0        finobt=0
data     =                       bsize=4096   blocks=12800000, imaxpct=25
         =                       sunit=1024   swidth=1024 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal log           bsize=4096   blocks=6256, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
   <para>
    Note that XFS being a non-clustered file system, you may only ever mount it
    on a single iSCSI initiator node at any given time.
   </para>
   <para>
    If at any time you want to discontinue using the iSCSI LUs associated with
    a particular target, run the following command:
   </para>
<screen>iscsiadm -m node -p iscsi1.example.com --logout
Logging out of session [sid: 18, iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260]
Logout of [sid: 18, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] successful.</screen>
   <para>
    As with discovery and login, you must repeat the logout steps for all
    portal IP addresses or host names.
   </para>
   <sect3 xml:id="ceph.iscsi.connect.linux.multipath">
    <title>Multipath Configuration</title>
    <para>
     The multipath configuration is maintained on the clients or initiators and
     is independent of any <systemitem>lrbd</systemitem> configuration. Select
     a strategy prior to using block storage. After editing the
     <filename>/etc/multipath.conf</filename>, restart
     <systemitem>multipathd</systemitem> with
    </para>
<screen>sudo systemctl restart multipathd</screen>
    <para>
     For an active-passive configuration with friendly names, add
    </para>
<screen>defaults {
  user_friendly_names yes
}</screen>
    <para>
     to your <filename>/etc/multipath.conf</filename>. After connecting to your
     targets successfully, run
    </para>
<screen>multipath -ll
mpathd (36001405dbb561b2b5e439f0aed2f8e1e) dm-0 SUSE,RBD
size=2.0G features='0' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=1 status=active
| `- 2:0:0:3 sdl 8:176 active ready running
|-+- policy='service-time 0' prio=1 status=enabled
| `- 3:0:0:3 sdj 8:144 active ready running
`-+- policy='service-time 0' prio=1 status=enabled
  `- 4:0:0:3 sdk 8:160 active ready running</screen>
    <para>
     Note the status of each link. For an active-active configuration, add
    </para>
<screen>defaults {
  user_friendly_names yes
}

devices {
  device {
    vendor "(LIO-ORG|SUSE)"
    product "RBD"
    path_grouping_policy "multibus"
    path_checker "tur"
    features "0"
    hardware_handler "1 alua"
    prio "alua"
    failback "immediate"
    rr_weight "uniform"
    no_path_retry 12
    rr_min_io 100
  }
}</screen>
    <para>
     to your <filename>/etc/multipath.conf</filename>. Restart
     <systemitem>multipathd</systemitem> and run
    </para>
<screen>multipath -ll
mpathd (36001405dbb561b2b5e439f0aed2f8e1e) dm-3 SUSE,RBD
size=2.0G features='1 queue_if_no_path' hwhandler='1 alua' wp=rw
`-+- policy='service-time 0' prio=50 status=active
  |- 4:0:0:3 sdj 8:144 active ready running
  |- 3:0:0:3 sdk 8:160 active ready running
  `- 2:0:0:3 sdl 8:176 active ready running</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.iscsi.connect.win">
   <title>Microsoft Windows (Microsoft iSCSI initiator)</title>
   <para>
    To connect to a SUSE Enterprise Storage iSCSI target from a Windows 2012 server, follow
    these steps:
   </para>
   <procedure>
    <step>
     <para>
      Open Windows Server Manager. From the Dashboard, select
      <menuchoice><guimenu>Tools</guimenu><guimenu>iSCSI
      Initiator</guimenu></menuchoice>. The <guimenu>iSCSI Initiator
      Properties</guimenu> dialog appears. Select the
      <guimenu>Discovery</guimenu> tab:
     </para>
     <figure>
      <title>iSCSI Initiator Properties</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-initiator-props.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-initiator-props.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      In the <guimenu>Discover Target Portal</guimenu> dialog, enter the
      target's host name or IP address in the <guimenu>Target</guimenu> field
      and click <guimenu>OK</guimenu>:
     </para>
     <figure>
      <title>Discover Target Portal</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-target-ip.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-target-ip.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Repeat this process for all other gateway host names or IP addresses.
      When completed, review the <guimenu>Target Portals</guimenu> list:
     </para>
     <figure>
      <title>Target Portals</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-target-ip-list.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-target-ip-list.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Next, switch to the <guimenu>Targets</guimenu> tab and review your
      discovered target(s).
     </para>
     <figure>
      <title>Targets</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-targets.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-targets.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Click <guimenu>Connect</guimenu> in the <guimenu>Targets</guimenu> tab.
      The <guimenu>Connect To Target</guimenu> dialog appears. Select the
      <guimenu>Enable Multi-path</guimenu> check box to enable multipath I/O
      (MPIO), then click <guimenu>OK</guimenu>:
     </para>
    </step>
    <step>
     <para>
      When the <guimenu>Connect to Target</guimenu> dialog closes, select
      <guimenu>Properties</guimenu> to review the target's properties:
     </para>
     <figure>
      <title>iSCSI Target Properties</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-target-properties.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-target-properties.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Select <guimenu>Devices</guimenu>, and click <guimenu>MPIO</guimenu> to
      review the multipath I/O configuration:
     </para>
     <figure>
      <title>Device Details</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-device-details.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-device-details.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      The default <guimenu>Load Balance policy</guimenu> is <guimenu>Round
      Robin With Subset</guimenu>. If you prefer a pure fail-over
      configuration, change it to <guimenu>Fail Over Only</guimenu>.
     </para>
    </step>
   </procedure>
   <para>
    This concludes the iSCSI initiator configuration. The iSCSI volumes are now
    available like any other SCSI devices, and may be initialized for use as
    volumes and drives. Click <guimenu>OK</guimenu> to close the <guimenu>iSCSI
    Initiator Properties</guimenu> dialog, and proceed with the<guimenu> File
    and Storage Services</guimenu> role from the <guimenu>Server
    Manager</guimenu> dashboard.
   </para>
   <para>
    Observe the newly connected volume. It identifies as <emphasis>SUSE RBD
    SCSI Multi-Path Drive</emphasis> on the iSCSI bus, and is initially marked
    with an <emphasis>Offline</emphasis> status and a partition table type of
    <emphasis>Unknown</emphasis>. If the new volume does not appear
    immediately, select <guimenu>Rescan Storage</guimenu> from the
    <guimenu>Tasks</guimenu> drop-down box to rescan the iSCSI bus.
   </para>
   <procedure>
    <step>
     <para>
      Right-click on the iSCSI volume and select <guimenu>New Volume</guimenu>
      from the context menu. The <guimenu>New Volume Wizard</guimenu> appears.
      Click <guimenu>Next</guimenu>, highlight the newly connected iSCSI volume
      and click <guimenu>Next</guimenu> to begin.
     </para>
     <figure>
      <title>New Volume Wizard</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-volume-wizard.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-volume-wizard.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Initially, the device is empty and does not contain a partition table.
      When prompted, confirm the dialog indicating that the volume will be
      initialized with a GPT partition table:
     </para>
     <figure>
      <title>Offline Disk Prompt</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-win-prompt1.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-win-prompt1.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Select the volume size. Typically, you would use the device's full
      capacity. Then assign a drive letter or folder name where the newly
      created volume will become available. Then select a file system to create
      on the new volume, and finally confirm your selections with
      <guimenu>Create</guimenu> to finish creating the volume:
     </para>
     <figure>
      <title>Confirm Volume Selections</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-volume-confirm.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-volume-confirm.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      When the process finishes, review the results, then
      <guimenu>Close</guimenu> to conclude the drive initialization. Once
      initialization completes, the volume (and its NTFS file system) becomes
      available like a newly initialized local drive.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph.iscsi.connect.vmware">
   <title>VMware</title>
   <para/>
   <procedure>
    <step>
     <para>
      To connect to <systemitem>lrbd</systemitem> managed iSCSI volumes you
      need a configured iSCSI software adapter. If no such adapter is available
      in your vSphere configuration, create one by selecting
      <menuchoice><guimenu>Configuration</guimenu><guimenu>Storage
      Adapters</guimenu> <guimenu>Add</guimenu><guimenu>iSCSI Software
      initiator</guimenu></menuchoice>.
     </para>
    </step>
    <step>
     <para>
      When available, select the adapter's properties by right-clicking the
      adapter and selecting <guimenu>Properties</guimenu> from the context
      menu:
     </para>
     <figure>
      <title>iSCSI Initiator Properties</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi_vmware_adapter_props.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi_vmware_adapter_props.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      In the <guimenu>iSCSI Software Initiator</guimenu> dialog, click the
      <guimenu>Configure</guimenu> button. Then go to the <guimenu>Dynamic
      Discovery</guimenu> tab and select <guimenu>Add</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Enter the IP address or host name of your <systemitem>lrbd</systemitem>
      iSCSI gateway. If you run multiple iSCSI gateways in a failover
      configuration, repeat this step for as many gateways as you operate.
     </para>
     <figure>
      <title>Add Target Server</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-add-target.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-add-target.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      When you have entered all iSCSI gateways, click <guimenu>OK</guimenu> in
      the dialog to initiate a rescan of the iSCSI adapter.
     </para>
    </step>
    <step>
     <para>
      When the rescan completes, the new iSCSI device appears below the
      <guimenu>Storage Adapters</guimenu> list in the
      <guimenu>Details</guimenu> pane. For multipath devices, you can now
      right-click on the adapter and select <guimenu>Manage Paths</guimenu>
      from the context menu:
     </para>
     <figure>
      <title>Manage Multipath Devices</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-multipath.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-multipath.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      You should now see all paths with a green light under
      <guimenu>Status</guimenu>. One of your paths should be marked
      <guimenu>Active (I/O)</guimenu> and all others simply
      <guimenu>Active</guimenu>:
     </para>
     <figure>
      <title>Paths Listing for Multipath</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-paths.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-paths.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      You can now switch from <guimenu>Storage Adapters</guimenu> to the item
      labeled <guimenu>Storage</guimenu>. Select <guimenu>Add
      Storage...</guimenu> in the top-right corner of the pane to bring up the
      <guimenu>Add Storage</guimenu> dialog. Then, select
      <guimenu>Disk/LUN</guimenu> and click <guimenu>Next</guimenu>. The newly
      added iSCSI device appears in the <guimenu>Select Disk/LUN</guimenu>
      list. Select it, then click <guimenu>Next</guimenu> to proceed:
     </para>
     <figure>
      <title>Add Storage Dialog</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-add-storage-dialog.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-add-storage-dialog.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Click <guimenu>Next</guimenu> to accept the default disk layout.
     </para>
    </step>
    <step>
     <para>
      In the <guimenu>Properties</guimenu> pane, assign a name to the new
      datastore, and click <guimenu>Next</guimenu>. Accept the default setting
      to use the volume's entire space for the datastore, or select
      <guimenu>Custom Space Setting</guimenu> for a smaller datastore:
     </para>
     <figure>
      <title>Custom Space Setting</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-custom-datastore.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-custom-datastore.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Click <guimenu>Finish</guimenu> to complete the datastore creation.
     </para>
     <para>
      The new datastore now appears in the datastore list and you can select it
      to retrieve details. You are now able to use the
      <systemitem>lrbd</systemitem>-backed iSCSI volume like any other vSphere
      datastore.
     </para>
     <figure>
      <title>iSCSI Datastore Overview</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-overview.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-overview.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.iscsi.conclude">
  <title>Conclusion</title>

  <para>
   <systemitem>lrbd</systemitem> is a key component of SUSE Enterprise Storage that enables
   access to distributed, highly available block storage from any server or
   client capable of speaking the iSCSI protocol. By using
   <systemitem>lrbd</systemitem> on one or more iSCSI gateway hosts, Ceph RBD
   images become available as Logical Units (LUs) associated with iSCSI
   targets, which can be accessed in an optionally load-balanced, highly
   available fashion.
  </para>

  <para>
   Since all of <systemitem>lrbd</systemitem>'s configuration is stored in the
   Ceph RADOS object store, <systemitem>lrbd</systemitem> gateway hosts are
   inherently without persistent state and thus can be replaced, augmented, or
   reduced at will. As a result, SUSE Enterprise Storage enables SUSE customers to run a
   truly distributed, highly-available, resilient, and self-healing enterprise
   storage technology on commodity hardware and an entirely open source
   platform.
  </para>
 </sect1>
</chapter>
  <chapter xml:base="admin_ceph_cephfs.xml" version="5.0" xml:id="cha.ceph.cephfs">


 <title>Clustered File System</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The Ceph file system (CephFS) is a POSIX-compliant file system that uses
  a Ceph storage cluster to store its data. CephFS uses the same cluster
  system as Ceph block devices, Ceph object storage with its S3 and Swift
  APIs, or native bindings (<systemitem>librados</systemitem>).
 </para>
 <para>
  To use CephFS, you need to have a running Ceph storage cluster, and at
  least one running <emphasis>Ceph metadata server</emphasis>.
 </para>
 <warning>
  <para>
   CephFS file layout changes can be performed as documented in
   <link xlink:href="http://docs.ceph.com/docs/jewel/cephfs/file-layouts/"/>.
   However, a data pool must not be added to an existing CephFS file system
   (via <command>ceph mds add_data_pool</command>) while the file system is
   mounted by any clients.
  </para>
 </warning>
 <sect1 xml:id="ceph.cephfs.mds">
  <title>Ceph Metadata Server</title>

  <para>
   Ceph metadata server (MDS) stores metadata for the CephFS. Ceph block
   devices and Ceph object storage <emphasis>do not</emphasis> use MDS. MDSs
   make it possible for POSIX file system users to execute basic
   commands—such as <command>ls</command> or
   <command>find</command>—without placing an enormous burden on the
   Ceph storage cluster.
  </para>

  <sect2 xml:id="ceph.cephfs.mdf.add">
   <title>Adding a Metadata Server</title>
   <para>
    After you deploy OSDs and monitors, you can deploy metadata servers.
    Although MDS service can share a node with an OSD and/or monitor service,
    you are encouraged to deploy it on a separate cluster node for performance
    reasons.
   </para>
<screen><prompt>cephadm &gt; </prompt>ceph-deploy install <replaceable>mds-host-name</replaceable>
<prompt>cephadm &gt; </prompt>ceph-deploy mds create <replaceable>host-name</replaceable>:<replaceable>daemon-name</replaceable></screen>
   <para>
    You may optionally specify a daemon instance name if you need to run
    multiple daemons on a single server.
   </para>
   <para>
    After you deploy your MDS, allow the <literal>Ceph OSD/MDS</literal>
    service in the firewall setting of the server where MDS is deployed. Start
    <literal>yast</literal>, navigate to <menuchoice> <guimenu>Security and
    Users</guimenu> <guimenu>Firewall</guimenu> <guimenu>Allowed
    Services</guimenu> </menuchoice> and in the <guimenu>Service to
    Allow</guimenu> drop–down menu select <guimenu>Ceph
    OSD/MDS</guimenu>. If the Ceph MDS node is not allowed full traffic,
    mounting of a file system fails, even though other operations may work
    properly.
   </para>
  </sect2>

  <sect2 xml:id="ceph.cephfs.mdf.config">
   <title>Configuring a Metadata Server</title>
   <para>
    You can fine-tune the MDS behavior by inserting relevant options in the
    <filename>ceph.conf</filename> configuration file. For detailed list of MDS
    related configuration options, see
    <link xlink:href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/"/>.
   </para>
   <para>
    For detailed list of MDS journaler configuration options, see
    <link xlink:href="http://docs.ceph.com/docs/master/cephfs/journaler/"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.cephfs.cephfs">


  <title>CephFS</title>

  <para>
   When you have a healthy Ceph storage cluster with at least one Ceph
   metadata server, you may create and mount your Ceph file system. Ensure
   that your client has network connectivity and a proper authentication
   keyring.
  </para>

  <sect2 xml:id="ceph.cephfs.cephfs.create">
   <title>Creating CephFS</title>
   <para>
    A CephFS requires at least two RADOS pools: one for
    <emphasis>data</emphasis> and one for <emphasis>metadata</emphasis>. When
    configuring these pools, you might consider:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Using a higher replication level for the metadata pool, as any data loss
      in this pool can render the whole file system inaccessible.
     </para>
    </listitem>
    <listitem>
     <para>
      Using lower-latency storage such as SSDs for the metadata pool, as this
      will improve the observed latency of file system operations on clients.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    For more information on managing pools, see <xref linkend="ceph.pools" role="internalbook"/>.
   </para>
   <para>
    To create the two required pools—for example 'cephfs_data' and
    'cephfs_metadata'—with default settings for use with CephFS, run
    the following commands:
   </para>
<screen>ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    When the pools are created, you may enable the file system with the
    <command>ceph fs new</command> command:
   </para>
<screen>ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    For example:
   </para>
<screen>ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    You can check that the file system was created by listing all available
    CephFS's:
   </para>
<screen>ceph fs ls
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    When the file system has been created, your MDS will be able to enter an
    <emphasis>active</emphasis> state. For example, in a single MDS system:
   </para>
<screen>ceph mds stat
 e5: 1/1/1 up</screen>
  </sect2>

  <sect2 xml:id="ceph.cephfs.cephfs.mount">
   <title>Mounting CephFS</title>
   <para>
    Once the file system is created and the MDS is active, you are ready to
    mount the file system from a client host.
   </para>
   <sect3 xml:id="Creating_Secret_File">
    <title>Create a Secret File</title>
    <para>
     The Ceph cluster runs with authentication turned on by default. You
     should create a file that stores your secret key (not the keyring itself).
     To obtain the secret key for a particular user and then create the file,
     do the following:
    </para>
    <procedure>
     <title>Creating a Secret Key</title>
     <step>
      <para>
       View the key for the particular user in a keyring file:
      </para>
<screen>cat /etc/ceph/ceph.client.admin.keyring</screen>
     </step>
     <step>
      <para>
       Copy the key of the user who will be using the mounted Ceph FS
       filesystem. Usually the key looks similar like the following:
      </para>
<screen>[client.admin]
   key = AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
     </step>
     <step>
      <para>
       Create a file with the user name as a filename part, e.g.
       <filename>/etc/ceph/admin.secret</filename> for the user
       <emphasis>admin</emphasis>
      </para>
     </step>
     <step>
      <para>
       Paste the key value to the file created in the previous step.
      </para>
     </step>
     <step>
      <para>
       Set proper access rights to the file. The user should be the only one
       who can read the file, others may not have any access rights.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3>
    <title>Mount CephFS with the Kernel Driver</title>
    <para>
     You can mount CephFS, normally with the <command>mount</command>
     command. You need to specify the monitor host name or IP address.
    </para>
    <tip>
     <title>Specify Multiple Monitors</title>
     <para>
      It is a good idea to specify multiple monitors separated by commas on the
      <command>mount</command> command line in case one monitor happens to be
      down at the time of mount. Each monitor address takes the form<literal>
      host[:port]</literal>. If the port is not specified, it defaults to 6789.
     </para>
    </tip>
    <para>
     Create the mount point on the local host:
    </para>
<screen>sudo mkdir /mnt/cephfs</screen>
    <para>
     Mount the CephFS:
    </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs</screen>
    <para>
     A subdirectory <filename>subdir</filename> may be specified if a subset of
     the file system is to be mounted:
    </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs</screen>
    <para>
     You can specify more than one monitor host in the <command>mount</command>
     command:
    </para>
<screen>sudo mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs</screen>
    <tip>
     <title>CephFS and <systemitem>cephx</systemitem> Authentication</title>
     <para>
      To mount CephFS with <systemitem>cephx</systemitem> authentication
      enabled, you need to specify a user name and a secret:
     </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
     <para>
      As the previous command remains in the shell history, a more secure
      approach is to read the secret from a file:
     </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
    </tip>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.cephfs.cephfs.unmount">
   <title>Unmounting CephFS</title>
   <para>
    To unmount the CephFS, use the <command>umount</command> command:
   </para>
<screen>sudo umount /mnt/cephfs</screen>
  </sect2>

  <sect2 xml:id="ceph.cephfs.cephfs.fstab">
   <title>CephFS in <filename>/etc/fstab</filename></title>
   <para>
    To mount CephFS automatically on the client start-up, insert the
    corresponding line in its file systems table
    <filename>/etc/fstab</filename>:
   </para>
<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime 0 2</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.cephfs.failover">
  <title>Managing Failover</title>

  <para>
   If an MDS daemon stops communicating with the monitor, the monitor will wait
   <option>mds_beacon_grace</option> seconds (default 15 seconds) before
   marking the daemon as <emphasis>laggy</emphasis>. You can configure one or
   more 'standby' daemons that can will take over during the MDS daemon
   failover.
  </para>

  <sect2 xml:id="ceph.cephfs.failover.standby">
   <title>Configuring Standby Daemons</title>
   <para>
    There are several configuration settings that control how a daemon will
    behave while in standby. You can specify them in the
    <filename>ceph.conf</filename> on the host where the MDS daemon runs. The
    daemon loads these settings when it starts, and sends them to the monitor.
   </para>
   <para>
    By default, if none of these settings are used, all MDS daemons which do
    not hold a rank will be used as 'standbys' for any rank.
   </para>
   <para>
    The settings which associate a standby daemon with a particular name or
    rank do not guarantee that the daemon will only be used for that rank. They
    mean that when several standbys are available, the associated standby
    daemon will be used. If a rank is failed, and a standby is available, it
    will be used even if it is associated with a different rank or named
    daemon.
   </para>
   <variablelist>
    <varlistentry>
     <term>mds_standby_replay</term>
     <listitem>
      <para>
       If set to true, then the standby daemon will continuously read the
       metadata journal an up rank. This will give it a warm metadata cache,
       and speed up the process of failing over if the daemon serving the rank
       fails.
      </para>
      <para>
       An up rank may only have one standby replay daemon assigned to it. If
       two daemons are both set to be standby replay then one of them will
       arbitrarily win, and the other will become a normal non-replay standby.
      </para>
      <para>
       Once a daemon has entered the standby replay state, it will only be used
       as a standby for the rank that it is following. If another rank fails,
       this standby replay daemon will not be used as a replacement, even if no
       other standbys are available.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_name</term>
     <listitem>
      <para>
       Set this to make the standby daemon only take over a failed rank if the
       last daemon to hold it matches this name.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_rank</term>
     <listitem>
      <para>
       Set this to make the standby daemon only take over the specified rank.
       If another rank fails, this daemon will not be used to replace it.
      </para>
      <para>
       Use in conjunction with<option>mds_standby_for_fscid</option> to be
       specific about which file system's rank you are targeting in case of
       multiple file systems.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_fscid</term>
     <listitem>
      <para>
       If <option>mds_standby_for_rank</option> is set, this is simply a
       qualifier to say which file system's rank is referred to.
      </para>
      <para>
       If <option>mds_standby_for_rank</option> is not set, then setting FSCID
       will cause this daemon to target any rank in the specified FSCID. Use
       this if you have a daemon that you want to use for any rank, but only
       within a particular file system.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mon_force_standby_active</term>
     <listitem>
      <para>
       This setting is used on monitor hosts. It defaults to true.
      </para>
      <para>
       If it is false, then daemons configured with
       <option>standby_replay=true</option> will only become active if the
       rank/name that they have been configured to follow fails. On the other
       hand, if this setting is true, then a daemon configured with
       <option>standby_replay=true</option> may be assigned some other rank.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph.cephfs.failover.examples">
   <title>Examples</title>
   <para>
    Several example <filename>ceph.conf</filename> configurations follow. You
    can either copy a <filename>ceph.conf</filename> with the configuration of
    all daemons to all your servers, or you can have a different file on each
    server that contains just that server's daemons configuration.
   </para>
   <sect3>
    <title>Simple Pair</title>
    <para>
     Two MDS daemons 'a' and 'b' acting as a pair. Whichever one is not
     currently assigned a rank will be the standby replay follower of the
     other.
    </para>
<screen>[mds.a]
mds standby replay = true
mds standby for rank = 0

[mds.b]
mds standby replay = true
mds standby for rank = 0</screen>
   </sect3>
   
  </sect2>
 </sect1>
</chapter>
 </part>
 <part xml:id="part.gui">
  <title>Managing Cluster with GUI Tools</title>
  <chapter xml:base="admin_gui_oa.xml" version="5.0" xml:id="ceph.oa">
 <title>openATTIC</title>
 <para>
  openATTIC is a central storage management system which supports Ceph storage
  cluster. With openATTIC you can control everything from a central management
  interface. It is no longer necessary to be familiar with the inner workings
  of the Ceph storage tools. Cluster management tasks can be carried out by
  either using openATTIC's intuitive Web interface, or via its REST API.
 </para>
 <sect1 xml:id="ceph.oa.install.pkgs">
  <title>Installing Required Packages</title>

  <para>
   While you can install and run openATTIC on any existing Ceph cluster node, we
   recommend to install it on the admin node. openATTIC is included in the SUSE Enterprise Storage
   extension. To install the required packages, run
  </para>

<screen>sudo zypper in openattic</screen>

  <tip>
   <para>
    openATTIC will work correctly only if it is the only Web-based application on
    the specific host. Do not share the host with another Web application such
    as Calamari.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ceph.oa.install.oa">
  <title>openATTIC Initial Setup</title>

  <para>
   After the packages are installed, run the actual openATTIC setup:
  </para>

<screen>sudo oaconfig install</screen>

  <para>
   <command>oaconfig install</command> will start a number of services,
   initialize the openATTIC database, and scan the system for pools and volumes to
   include.
  </para>

  <para>
   By default, <command>oaconfig</command> creates an administrative user
   account <systemitem>openattic</systemitem>, with the same password as the
   user name. As a security precaution, we strongly recommend to change this
   password immediately:
  </para>

<screen>sudo oaconfig changepassword openattic
Changing password for user 'openattic'
Password: &lt;enter password&gt;
Password (again): &lt;re-enter password&gt;
Password changed successfully for user 'openattic'
</screen>

  <para>
   Now your openATTIC storage system can be managed by the Web user interface.
  </para>
 </sect1>
 <sect1 xml:id="ceph.oa.install.webui">
  <title>openATTIC Web User Interface</title>

  <para>
   openATTIC can be managed using a Web user interface. Open a Web browser and
   navigate to http://www.example.org/openattic. To log in, use the default
   user name <emphasis>openattic</emphasis> and the corresponding password.
  </para>

  <figure>
   <title>openATTIC Login Screen</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="oa_login.png" width="70%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="oa_login.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The openATTIC user interface is graphically divided into a top menu pane and a
   content pane.
  </para>

  <para>
   The right part of the top pane includes a link to the current user settings,
   and a <guimenu>Logout</guimenu> link. The rest of the top pane includes the
   main openATTIC menu.
  </para>

  <para>
   The content pane changes depending on which item menu is activated. By
   default, a <guimenu>Dashboard</guimenu> is displayed showing general Ceph
   cluster statistics.
  </para>

  <figure>
   <title>openATTIC Dashboard</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="oa_dashboard.png" width="90%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="oa_dashboard.png" width="90%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1 xml:id="ceph.oa.webui.dash">
  <title>Dashboard</title>

  <para>
   <guimenu>Dashboard</guimenu> shows the overall statistics of the running
   Ceph cluster. By default it shows the following widgets: <guimenu>Ceph
   Status</guimenu>, <guimenu>Utilization</guimenu>, <guimenu>OSD
   Status</guimenu>, and <guimenu>Throughput</guimenu>.
  </para>

  <para>
   The <guimenu>Ceph Status</guimenu> widget tells whether the cluster is
   operating correctly. In case a problem is detected, you can view the
   detailed error message by clicking the subtitle inside the widget.
  </para>

  <figure>
   <title>Ceph Status</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="oa_error.png" width="30%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="oa_error.png" width="30%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The <guimenu>OSD Status</guimenu> widget shows the total number of OSD nodes
   and the number of online OSD node in the cluster in time.
  </para>

  <figure>
   <title>OSDs Status</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="oa_osdcount.png" width="70%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="oa_osdcount.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The <guimenu>Utilization</guimenu> widget shows the storage usage in time.
   You can activate or deactivate the following charts:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <guimenu>Bytes total</guimenu> - shows the total storage size.
    </para>
   </listitem>
   <listitem>
    <para>
     <guimenu>Bytes available</guimenu> - shows the remaining available space.
    </para>
   </listitem>
   <listitem>
    <para>
     <guimenu>Bytes used</guimenu> - shows the occupied space.
    </para>
   </listitem>
  </itemizedlist>

  <figure>
   <title>Cluster Utilization</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="oa_util.png" width="35%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="oa_util.png" width="35%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   The <guimenu>Throughput</guimenu> widget shows the read/write per
   second statistics in time.
  </para>

  <figure>
   <title>Throughput</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="oa_io.png" width="60%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="oa_io.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <tip>
   <title>More Details on Mouse Over</title>
   <para>
    If you move the mouse pointer over any of the displayed charts, it will
    show you more details related to the pointed date and time in a pop-up
    window.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ceph.oa.webui.ceph">
  <title>Ceph Related Tasks</title>

  <para>
   openATTIC's main menu lists Ceph related tasks. Currently, the following tasks
   are relevant: <guimenu>OSDs</guimenu>, <guimenu>RBDs</guimenu>,
   <guimenu>Pools</guimenu>, <guimenu>Nodes</guimenu> and <guimenu>CRUSH
   Map</guimenu>.
  </para>

  <sect2>
   <title>Common Web UI Features</title>
   <para>
    In openATTIC you often work with <emphasis>lists</emphasis>—for example
    lists of pools, OSD nodes, or RBD devices. The following common widgets
    help you manage or adjust these list:
   </para>
   <para>
    Click <inlinemediaobject>
    <imageobject role="fo">
     <imagedata fileref="oa_widget_reload.png" width="1.2em" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="oa_widget_reload.png" width="1.2em" format="PNG"/>
    </imageobject>
    </inlinemediaobject> to refresh the list of items.
   </para>
   <para>
    Click <inlinemediaobject>
    <imageobject role="fo">
     <imagedata fileref="oa_widget_columns.png" width="1.3em" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="oa_widget_columns.png" width="1.3em" format="PNG"/>
    </imageobject>
    </inlinemediaobject> to display or hide individual table columns.
   </para>
   <para>
    Click <inlinemediaobject>
    <imageobject role="fo">
     <imagedata fileref="oa_widget_rows.png" width="1.8em" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="oa_widget_rows.png" width="1.8em" format="PNG"/>
    </imageobject>
    </inlinemediaobject> and select how many rows to display on a single page.
   </para>
   <para>
    Click inside <inlinemediaobject>
    <imageobject role="fo">
     <imagedata fileref="oa_widget_search.png" width="5em" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="oa_widget_search.png" width="5em" format="PNG"/>
    </imageobject>
    </inlinemediaobject> and filter the rows by the typing the string to search
    for.
   </para>
   <para>
    Use <inlinemediaobject>
    <imageobject role="fo">
     <imagedata fileref="oa_widget_pager.png" width="6em" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="oa_widget_pager.png" width="6em" format="PNG"/>
    </imageobject>
    </inlinemediaobject> to change the currently displayed page if the list
    spans across multiple pages.
   </para>
  </sect2>

  <sect2>
   <title>Listing OSD Nodes</title>
   <para>
    To list all available OSD nodes, click <guimenu>OSDs</guimenu> from the
    main menu.
   </para>
   <para>
    The list shows each OSD's name, host name, status, and weight.
   </para>
   <figure>
    <title>List of OSD nodes</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_osds.png" width="90%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_osds.png" width="90%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2>
   <title>Managing RADOS Block Devices (RBDs)</title>
   <para>
    To list all available RADOS block devices, click <guimenu>RBDs</guimenu>
    from the main menu.
   </para>
   <para>
    The list shows each device's name, the related pool name, size of the
    device, and how many percents are already occupied.
   </para>
   <figure>
    <title>List of RBDs</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_rbds.png" width="90%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_rbds.png" width="90%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    To view more detailed information about a device, activate its check box in
    the very left column:
   </para>
   <figure>
    <title>RBD Details</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_rbd_status.png" width="40%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_rbd_status.png" width="40%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <sect3>
    <title>Deleting RBDs</title>
    <para>
     To delete a device or a group of devicesws, activate their check boxes in
     the very left column and click <guimenu>Delete</guimenu> in the top left
     of the RBDs table:
    </para>
    <figure>
     <title>Deleting RBD</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="oa_rbd_delete.png" width="70%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="oa_rbd_delete.png" width="70%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
   </sect3>
   <sect3>
    <title>Adding RBDs</title>
    <para>
     To add a new device, click <guimenu>Add</guimenu> in the top left of the
     RBDs table and do the following on the <guimenu>Create RBD</guimenu>
     screen:
    </para>
    <figure>
     <title>Adding a New RBD</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="oa_rbd_add.png" width="70%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="oa_rbd_add.png" width="70%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <procedure>
     <step>
      <para>
       Enter the name of the new device. Refer to
       <xref linkend="sysreq.naming" role="internalbook"/> for naming limitations.
      </para>
     </step>
     <step>
      <para>
       Select the cluster that will store the new pool.
      </para>
     </step>
     <step>
      <para>
       Select the pool from which the new RBD device will be created.
      </para>
     </step>
     <step>
      <para>
       Specify the size of the new device. If you click the <guimenu>use
       max</guimenu> link above, the maximum pool size is populated.
      </para>
     </step>
     <step>
      <para>
       To fine tune the device parameters, click <guimenu>Expert
       settings</guimenu> and activate or deactivate displayed options.
      </para>
     </step>
     <step>
      <para>
       Confirm with <guimenu>Create</guimenu>.
      </para>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2>
   <title>Managing Pools</title>
   <tip>
    <title>More Information on Pools</title>
    <para>
     For more general information about Ceph pools, refer to
     <xref linkend="ceph.pools" role="internalbook"/>. For information specific to erasure coded
     pools, refer to <xref linkend="cha.ceph.erasure" role="internalbook"/>.
    </para>
   </tip>
   <para>
    To list all available pools, click <guimenu>Pools</guimenu> from the main
    menu.
   </para>
   <para>
    The list shows each pool's name, ID, the percentage of used space, the
    number of placement groups, replica size, type ('replicated' or 'erasure'),
    erasure code profile, and the crush ruleset.
   </para>
   <figure>
    <title>List of Pools</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_pools.png" width="90%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_pools.png" width="90%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    To view more detailed information about a pool, activate its check box in
    the very left column:
   </para>
   <figure>
    <title>Pool Details</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_pools_status.png" width="40%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_pools_status.png" width="40%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <sect3>
    <title>Deleting Pools</title>
    <para>
     To delete a pool or a group of pools, activate their check boxes in the
     very left column and click <guimenu>Delete</guimenu> in the top left of
     the pools table:
    </para>
    <figure>
     <title>Deleting Pools</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="oa_pools_delete.png" width="70%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="oa_pools_delete.png" width="70%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
   </sect3>
   <sect3>
    <title>Adding Pools</title>
    <para>
     To add a new pool, click <guimenu>Add</guimenu> in the top left of the
     pools table and do the following on the <guimenu>Create Ceph
     pool</guimenu> screen:
    </para>
    <figure>
     <title>Adding a New Pool</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="oa_pools_add.png" width="70%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="oa_pools_add.png" width="70%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <procedure>
     <step>
      <para>
       Enter the name of the new pool. Refer to <xref linkend="sysreq.naming" role="internalbook"/>
       for naming limitations.
      </para>
     </step>
     <step>
      <para>
       Select the cluster that will store the new pool.
      </para>
     </step>
     <step>
      <para>
       Select the pool type. Pools can be either replicated or erasure coded.
      </para>
     </step>
     <step>
      <para>
       Specify the number of the pool's placement groups.
      </para>
     </step>
     <step>
      <para>
       For a replicated pool, specify the replica size.
      </para>
     </step>
     <step>
      <para>
       Confirm with <guimenu>Create</guimenu>.
      </para>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.oa.minions">
   <title>Listing Nodes</title>
   <important>
    <title>Salt Only Deployment</title>
    <para>
     The <guimenu>Nodes</guimenu> tab is only available when the cluster is
     deployed via Salt. Refer to
     <xref linkend="ceph.install.saltstack" role="internalbook"/> for more information on
     Salt.
    </para>
   </important>
   <para>
    Click <guimenu>Nodes</guimenu> from the main menu to view the list of
    nodes available on the cluster.
   </para>
   <figure>
    <title>List of Nodes</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_minion.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_minion.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    Each node is represented by its host name, public IP address, cluster ID
    it belongs to, node role (for example 'admin', 'storage', or 'master'), and
    key acceptance status.
   </para>
  </sect2>

  <sect2 xml:id="ceph.oa.crushmap">
   <title>Viewing the Cluster CRUSH Map</title>
   <para>
    Click <guimenu>CRUSH Map</guimenu> from the main menu to view cluster
    CRUSH Map.
   </para>
   <figure>
    <title>CRUSH Map</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_crush.png" width="60%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_crush.png" width="60%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    In the <guimenu>Physical setup</guimenu> pane, you can see the structure of
    the cluster as described by the CRUSH Map.
   </para>
   <para>
    In the <guimenu>Replication rules</guimenu> pane, you can view individual
    rulesets after selecting one of them from the <guimenu>Content</guimenu>
    from the drop-down box.
   </para>
   <figure>
    <title>Replication rules</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="oa_crush_rulesets.png" width="60%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="oa_crush_rulesets.png" width="60%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>
 </sect1>
</chapter>
  <chapter xml:base="admin_gui_calamari.xml" version="5.0" xml:id="ceph.install.calamari">
 <title>Calamari</title>
 <para>
  Calamari is a management and monitoring system for Ceph storage cluster. It
  provides a Web user interface that makes Ceph cluster monitoring very
  simple and handy.
 </para>
 <para>
  The Calamari installation procedure differs according to the used deployment
  procedure. If you deployed your Ceph by using
  <command>ceph-deploy</command>, refer to
  <xref linkend="ceph-deploy.calamari.installation" role="internalbook"/>. If you deployed your
  cluster by using Crowbar, refer to
  <xref linkend="crowbar.calamari.installation" role="internalbook"/>.
 </para>
 <sect1 xml:id="ceph-deploy.calamari.installation">
  <title>Installing Calamari with <command>ceph-deploy</command></title>

  <para>
   To install Calamari, do the following:
  </para>

  <procedure>
   <step>
    <para>
     Install the client part of Calamari:
    </para>
<screen>sudo zypper in romana</screen>
   </step>
   <step>
    <para>
     Initialize Calamari installation. You will be asked for superuser user
     name and password. These will be needed when logging in to the Web
     interface after the setup is complete.
    </para>
<screen>sudo calamari-ctl initialize
[INFO] Loading configuration..
[INFO] Starting/enabling salt...
[INFO] Starting/enabling postgres...
[INFO] Initializing database...
[INFO] Initializing web interface...
[INFO] You will now be prompted for login details for the administrative user
account.  This is the account you will use to log into the web interface once
setup is complete.
Username (leave blank to use 'root'):
Email address:
Password:
Password (again):
Superuser created successfully.
[INFO] Starting/enabling services...
[INFO] Restarting services...
[INFO] Complete.</screen>
   </step>
   <step>
    <para>
     Check the firewall status
    </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
    <para>
     and if it is off, check its configuration and turn it on with
    </para>
<screen>sudo /sbin/SuSEfirewall2 on</screen>
    <para>
     You can find detailed information in
     <xref linkend="storage.bp.net.firewall" role="internalbook"/>.
    </para>
   </step>
   <step>
    <tip>
     <para>
      In order for Calamari to work correctly, the admin keyring needs to be
      installed on each monitor node:
     </para>
<screen><prompt>cephadm &gt; </prompt>ceph-deploy admin mon1 mon2 mon3</screen>
     <para>
      where <replaceable>mon1</replaceable>, <replaceable>mon2</replaceable>,
      or <replaceable>mon3</replaceable> are the host names of the monitors.
     </para>
    </tip>
    <para>
     Now open your Web browser and point it to the host name/IP address of the
     server where you installed Calamari. Log in with the credentials you
     entered when installing the Calamari client. A welcome screen appears,
     instructing you to enter the <command>ceph-deploy calamari
     connect</command> command. Switch to the terminal on the Calamari host and
     enter the following command. Note that the <option>--master</option>
     option specifies the host name of the Calamari server to which all the
     cluster nodes connect to:
    </para>
<screen><prompt>cephadm &gt; </prompt>ceph-deploy calamari connect --master <replaceable>master_host</replaceable> <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>...</replaceable></screen>
    <para>
     After the command is successfully finished, reload the Web browser. Now
     you can monitor your Ceph cluster, OSDs, pools, etc.
    </para>
    <tip>
     <title>Empty Usage Graphs</title>
     <para>
      If, after having installed Calamari initially, the usage graphs are empty/blank,
      it is possible that the diamond metrics collector was not automatically
      installed. To fix this, run <command>salt '*' state.highstate</command> on
      the Calamari host.
     </para>
    </tip>
    <important>
     <para>
      The Calamari dashboard screen shows the current status of the cluster.
      This updates regularly, so any change to the cluster state—for
      example if a node goes offline—should be reflected in Calamari
      within a few seconds. The <guimenu>Health</guimenu> panel includes a
      timer to indicate how long it has been since Calamari last saw heartbeat
      information from the cluster. Normally, this will not be more than one
      minute old, but in certain failure cases, for example when a network
      outage occurs or if the cluster loses quorum (that is if more than half
      of the monitor nodes are down), Calamari will no longer be able to
      determine cluster state. In this case, the <guimenu>Health</guimenu>
      panel will indicate that the last update was more than one minute ago.
      After too long time with no updates, Calamari displays a warning at the
      top of the screen "Cluster Updates Are Stale. The Cluster is not updating
      Calamari." If this occurs, the other status information Calamari presents
      will not be correct so you should investigate further to check the status
      of your storage nodes and network.
     </para>
    </important>
     <note>
      <title>Salt Installed by Default with Calamari</title>
      <para>
       Even though you deployed your Ceph cluster by using
       <command>ceph-deploy</command>, salt is installed along with Calamari.
       The <command>salt</command> command is thus installed even though you
       did not install salt manually.
      </para>
     </note>
    <tip>
     <para>
      They may be leftovers of the previous Calamari setup on the system. If
      after logging in to the Calamari application some nodes are already
      joined or registered, run the following on the Calamari host to trigger a
      re-run of salt on all Ceph nodes, which should clear up any odd state
      or missing bits and pieces.
     </para>
<screen>salt '*' state.highstate</screen>
     <para>
      We also recommend to remove files from the previous Calamari setup, such
      as state files, configuration files, or PostgreSQL database files. At
      minimum, remove the files in the following directories:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        <filename>/etc/calamari/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/salt/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/graphite/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/*/salt/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/lib/graphite/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/lib/pgsql/</filename>
       </para>
      </listitem>
     </itemizedlist>
    </tip>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="crowbar.calamari.installation">
  <title>Installing Calamari Using Crowbar</title>

  <note>
   <title>Conflicts in Combination with Deployment using Crowbar</title>
   <para>
    If you used Crowbar to install SUSE Enterprise Storage, install Calamari on a different
    server than Crowbar as Crowbar uses the same port as Calamari (port 80).
   </para>
  </note>

  <para>
   Use the Crowbar UI to deploy Calamari as described in
   <xref linkend="sec.depl.ceph.ceph" role="internalbook"/>.
  </para>
 </sect1>
</chapter>
 </part>
 <part xml:id="part.virt">
  <title>Integration with Virtualization Tools</title>
  <chapter xml:base="admin_ceph_libvirt.xml" version="5.0" xml:id="cha.ceph.libvirt">
 <title>Using <systemitem class="library">libvirt</systemitem> with Ceph</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The <systemitem class="library">libvirt</systemitem> library creates a virtual machine abstraction layer between
  hypervisor interfaces and the software applications that use them. With
  <systemitem class="library">libvirt</systemitem>, developers and system administrators can focus on a common
  management framework, common API, and common shell interface
  (<command>virsh</command>) to many different hypervisors, including
  QEMU/KVM, Xen, LXC, or VirtualBox.
 </para>
 <para>
  Ceph block devices support QEMU/KVM. You can use Ceph block devices
  with software that interfaces with <systemitem class="library">libvirt</systemitem>. The cloud solution uses
  <systemitem class="library">libvirt</systemitem> to interact with QEMU/KVM, and QEMU/KVM interacts with Ceph
  block devices via <systemitem>librbd</systemitem>.
 </para>
 <para>
  To create VMs that use Ceph block devices, use the procedures in the
  following sections. In the examples, we have used
  <literal>libvirt-pool</literal> for the pool name,
  <literal>client.libvirt</literal> for the user name, and
  <literal>new-libvirt-image</literal> for the image name. You may use any
  value you like, but ensure you replace those values when executing commands
  in the subsequent procedures.
 </para>
 <sect1 xml:id="ceph.libvirt.cfg_ceph">
  <title>Configuring Ceph</title>

  <para>
   To configure Ceph for use with <systemitem class="library">libvirt</systemitem>, perform the following steps:
  </para>

  <procedure>
   <step>
    <para>
     Create a pool. The following example uses the pool name
     <literal>libvirt-pool</literal> with 128 placement groups.
    </para>
<screen>ceph osd pool create libvirt-pool 128 128</screen>
    <para>
     Verify that the pool exists.
    </para>
<screen>ceph osd lspools</screen>
   </step>
   <step>
    <para>
     Create a Ceph User. The following example uses the Ceph user name
     <literal>client.libvirt</literal> and references
     <literal>libvirt-pool</literal>.
    </para>
<screen>ceph auth get-or-create client.libvirt mon 'allow r' osd \
 'allow class-read object_prefix rbd_children, allow rwx pool=libvirt-pool'</screen>
    <para>
     Verify the name exists.
    </para>
<screen>ceph auth list</screen>
    <note>
     <para>
      <systemitem class="library">libvirt</systemitem> will access Ceph using the ID <literal>libvirt</literal>, not
      the Ceph name <literal>client.libvirt</literal>. See
      <link xlink:href="http://ceph.com/docs/master/rados/operations/user-management#user"/>
      for a detailed explanation of the difference between ID and name.
     </para>
    </note>
   </step>
   <step>
    <para>
     Use QEMU to create an image in your RBD pool. The following example uses
     the image name <literal>new-libvirt-image</literal> and references
     <literal>libvirt-pool</literal>.
    </para>
<screen>qemu-img create -f rbd rbd:libvirt-pool/new-libvirt-image 2G</screen>
    <para>
     Verify the image exists.
    </para>
<screen>rbd -p libvirt-pool ls</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.libvirt.virt-manager">
  <title>Preparing the VM Manager</title>

  <para>
   You may use <systemitem class="library">libvirt</systemitem> without a VM manager, but you may find it simpler to
   create your first domain with <command>virt-manager</command>.
  </para>

  <procedure>
   <step>
    <para>
     Install a virtual machine manager.
    </para>
<screen>sudo zypper in virt-manager</screen>
   </step>
   <step>
    <para>
     Prepare/download an OS image of the system you want to run virtualized.
    </para>
   </step>
   <step>
    <para>
     Launch the virtual machine manager.
    </para>
<screen>virt-manager</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.libvirt.create_vm">
  <title>Creating a VM</title>

  <para>
   To create a VM with <command>virt-manager</command>, perform the following
   steps:
  </para>

  <procedure>
   <step>
    <para>
     Choose the connection from the list, right-click it, and select
     <guimenu>New</guimenu>.
    </para>
   </step>
   <step>
    <para>
     <guimenu>Import existing disk image</guimenu> by providing the path to the
     existing storage. Specify OS type, memory settings, and
     <guimenu>Name</guimenu> the virtual machine, for example
     <literal>libvirt-virtual-machine</literal>.
    </para>
   </step>
   <step>
    <para>
     Finish the configuration and start the VM.
    </para>
   </step>
   <step>
    <para>
     Verify that the newly created domain exists with <command>sudo virsh
     list</command>. If needed, specify the connection string, such as
    </para>
<screen role="ceph.libvirt.create_vm1"><command>virsh -c qemu+ssh://root@vm_host_hostname/system list</command>
Id    Name                           State
-----------------------------------------------
[...]
 9     libvirt-virtual-machine       running</screen>
   </step>
   <step>
    <para>
     Log in to the VM and stop it before configuring it for use with Ceph.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.libvirt.cfg_vm">
  <title>Configuring the VM</title>

  <para>
   When configuring the VM for use with Ceph, it is important to use
   <command>virsh</command> where appropriate. Additionally,
   <command>virsh</command> commands often require root privileges
   (<command>sudo</command>) and will not return appropriate results or notify
   you that root privileges are required. For a reference of
   <command>virsh</command> commands, refer to
   <link xlink:href="http://www.libvirt.org/virshcmdref.html">Virsh Command
   Reference</link>.
  </para>

  <procedure>
   <step>
    <para>
     Open the configuration file with <command>virsh edit</command>
     <replaceable>vm-domain-name</replaceable>.
    </para>
<screen>sudo virsh edit libvirt-virtual-machine</screen>
   </step>
   <step>
    <para>
     Under &lt;devices&gt; there should be a &lt;disk&gt; entry.
    </para>
<screen>&lt;devices&gt;
    &lt;emulator&gt;/usr/bin/qemu-system-x86_64&lt;/emulator&gt;
    &lt;disk type='file' device='disk'&gt;
      &lt;driver name='qemu' type='raw'/&gt;
      &lt;source file='/path/to/image/recent-linux.img'/&gt;
      &lt;target dev='vda' bus='virtio'/&gt;
      &lt;address type='drive' controller='0' bus='0' unit='0'/&gt;
    &lt;/disk&gt;</screen>
    <para>
     Replace <filename>/path/to/image/recent-linux.img</filename> with the path
     to the OS image.
    </para>
    <important>
     <para>
      Use <command>sudo virsh edit</command> instead of a text editor. If you
      edit the configuration file under <filename>/etc/libvirt/qemu</filename>
      with a text editor, <systemitem class="library">libvirt</systemitem> may not recognize the change. If there is a
      discrepancy between the contents of the XML file under
      <filename>/etc/libvirt/qemu</filename> and the result of <command>sudo
      virsh dumpxml</command> <replaceable>vm-domain-name</replaceable>, then
      your VM may not work properly.
     </para>
    </important>
   </step>
   <step>
    <para>
     Add the Ceph RBD image you previously created as a &lt;disk&gt; entry.
    </para>
<screen>&lt;disk type='network' device='disk'&gt;
        &lt;source protocol='rbd' name='libvirt-pool/new-libvirt-image'&gt;
                &lt;host name='<replaceable>monitor-host</replaceable>' port='6789'/&gt;
        &lt;/source&gt;
        &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</screen>
    <para>
     Replace <replaceable>monitor-host</replaceable> with the name of your
     host, and replace the pool and/or image name as necessary. You may add
     multiple &lt;host&gt; entries for your Ceph monitors. The
     <literal>dev</literal> attribute is the logical device name that will
     appear under the <filename>/dev</filename> directory of your VM. The
     optional bus attribute indicates the type of disk device to emulate. The
     valid settings are driver specific (for example ide, scsi, virtio, xen,
     usb or sata). See
     <link xlink:href="http://www.libvirt.org/formatdomain.html#elementsDisks">Disks</link>
     for details of the &lt;disk&gt; element, and its child elements and
     attributes.
    </para>
   </step>
   <step>
    <para>
     Save the file.
    </para>
   </step>
   <step>
    <para>
     If your Ceph cluster has authentication enabled (it does by default),
     you must generate a secret.
    </para>
<screen>cat &gt; secret.xml &lt;&lt;EOF
&lt;secret ephemeral='no' private='no'&gt;
        &lt;usage type='ceph'&gt;
                &lt;name&gt;client.libvirt secret&lt;/name&gt;
        &lt;/usage&gt;
&lt;/secret&gt;
EOF</screen>
   </step>
   <step>
    <para>
     Define the secret.
    </para>
<screen>sudo virsh secret-define --file secret.xml
&lt;uuid of secret is output here&gt;</screen>
   </step>
   <step>
    <para>
     Get the <literal>client.libvirt</literal> key and save the key string to a
     file.
    </para>
<screen>ceph auth get-key client.libvirt | sudo tee client.libvirt.key</screen>
   </step>
   <step>
    <para>
     Set the UUID of the secret.
    </para>
<screen>sudo virsh secret-set-value --secret <replaceable>uuid of secret</replaceable> \
--base64 $(cat client.libvirt.key) &amp;&amp; rm client.libvirt.key secret.xml</screen>
    <para>
     You must also set the secret manually by adding the following
     <literal>&amp;auth&gt;</literal> entry to the
     <literal>&amp;disk&gt;</literal> element you entered earlier (replacing
     the uuid value with the result from the command line example above).
    </para>
<screen>sudo virsh edit libvirt-virtual-machine</screen>
    <para>
     Then, add <literal>&amp;auth&gt;&amp;/auth&gt;</literal> element to the
     domain configuration file:
    </para>
<screen>...
&amp;/source&gt;
&amp;auth username='libvirt'&gt;
        &amp;secret type='ceph' uuid='9ec59067-fdbc-a6c0-03ff-df165c0587b8'/&gt;
&amp;/auth&gt;
&amp;target ...</screen>
    <note>
     <para>
      The exemplary ID is <literal>libvirt</literal>, not the Ceph name
      <literal>client.libvirt</literal> as generated at step 2 of
      <xref linkend="ceph.libvirt.cfg_ceph" role="internalbook"/>. Ensure you use the ID component
      of the Ceph name you generated. If for some reason you need to regenerate
      the secret, you will need to execute <command>sudo virsh
      secret-undefine</command> <replaceable>uuid</replaceable> before
      executing <command>sudo virsh secret-set-value</command> again.
     </para>
    </note>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.libvirt.summary">
  <title>Summary</title>

  <para>
   Once you have configured the VM for use with Ceph, you can start the VM.
   To verify that the VM and Ceph are communicating, you may perform the
   following procedures.
  </para>

  <procedure>
   <step>
    <para>
     Check to see if Ceph is running:
    </para>
<screen>ceph health</screen>
   </step>
   <step>
    <para>
     Check to see if the VM is running:
    </para>
<screen>sudo virsh list</screen>
   </step>
   <step>
    <para>
     Check to see if the VM is communicating with Ceph. Replace
     <replaceable>vm-domain-name</replaceable> with the name of your VM domain:
    </para>
<screen>sudo virsh qemu-monitor-command --hmp <replaceable>vm-domain-name</replaceable> 'info block'</screen>
   </step>
   <step>
    <para>
     Check to see if the device from <literal>&amp;target dev='hdb'
     bus='ide'/&gt;</literal> appears under <filename>/dev</filename> or under
     <filename>/proc/partitions</filename>:
    </para>
<screen>ls /dev
cat /proc/partitions</screen>
   </step>
  </procedure>
 </sect1>
</chapter>
  <chapter xml:base="admin_ceph_kvm.xml" version="5.0" xml:id="cha.ceph.kvm">
 <title>Ceph as a Back-end for QEMU KVM Instance</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The most frequent Ceph use case involves providing block device images to
  virtual machines. For example, a user may create a 'golden' image with an OS
  and any relevant software in an ideal configuration. Then, the user takes a
  snapshot of the image. Finally, the user clones the snapshot (usually many
  times, see <xref linkend="cha.ceph.snapshots" role="internalbook"/> for details). The ability to
  make copy-on-write clones of a snapshot means that Ceph can provision block
  device images to virtual machines quickly, because the client does not need
  to download an entire image each time it spins up a new virtual machine.
 </para>
 <para>
  Ceph block devices can integrate with the QEMU virtual machines. For more
  information on QEMU KVM, see
  <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/part_virt_qemu.html"/>.
 </para>
 <sect1 xml:id="ceph.kvm.install">
  <title>Installation</title>

  <para>
   In order to use Ceph block devices, QEMU needs to have the appropriate
   driver installed. Check whether the <systemitem>qemu-block-rbd</systemitem>
   package is installed, and install it if needed:
  </para>

<screen>sudo zypper install qemu-block-rbd</screen>
 </sect1>
 <sect1 xml:id="ceph.kvm.usage">
  <title>Usage</title>

  <para>
   The QEMU command line expects you to specify the pool name and image name.
   You may also specify a snapshot name.
  </para>

<screen>qemu-img <replaceable>command</replaceable> <replaceable>options</replaceable> \
rbd:<replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable><replaceable>:option1=value1</replaceable><replaceable>:option2=value2...</replaceable></screen>

  <para>
   For example, specifying the <replaceable>id</replaceable> and
   <replaceable>conf</replaceable> options might look like the following:
  </para>

<screen>qemu-img <replaceable>command</replaceable> <replaceable>options</replaceable> \
rbd:<replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable>:<option>id=glance:conf=/etc/ceph/ceph.conf</option></screen>
 </sect1>
 <sect1>
  <title>Creating Images with QEMU</title>

  <para>
   You can create a block device image from QEMU. You must specify
   <literal>rbd</literal>, the pool name, and the name of the image you want to
   create. You must also specify the size of the image.
  </para>

<screen>qemu-img create -f raw rbd:<replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable> <replaceable>size</replaceable></screen>

  <para>
   For example:
  </para>

<screen>qemu-img create -f raw rbd:pool1/image1 10G
Formatting 'rbd:pool1/image1', fmt=raw size=10737418240 nocow=off cluster_size=0</screen>

  <important>
   <para>
    The <literal>raw</literal> data format is really the only sensible format
    option to use with RBD. Technically, you could use other QEMU-supported
    formats such as <literal>qcow2</literal>, but doing so would add additional
    overhead, and would also render the volume unsafe for virtual machine live
    migration when caching is enabled.
   </para>
  </important>
 </sect1>
 <sect1>
  <title>Resizing Images with QEMU</title>

  <para>
   You can resize a block device image from QEMU. You must specify
   <literal>rbd</literal>, the pool name, and the name of the image you want to
   resize. You must also specify the size of the image.
  </para>

<screen>qemu-img resize rbd:<replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable> <replaceable>size</replaceable></screen>

  <para>
   For example:
  </para>

<screen>qemu-img resize rbd:pool1/image1 9G
Image resized.</screen>
 </sect1>
 <sect1>
  <title>Retrieving Image Info with QEMU</title>

  <para>
   You can retrieve block device image information from QEMU. You must
   specify <literal>rbd</literal>, the pool name, and the name of the image.
  </para>

<screen>qemu-img info rbd:<replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>

  <para>
   For example:
  </para>

<screen>qemu-img info rbd:pool1/image1
image: rbd:pool1/image1
file format: raw
virtual size: 9.0G (9663676416 bytes)
disk size: unavailable
cluster_size: 4194304</screen>
 </sect1>
 <sect1>
  <title>Running QEMU with RBD</title>

  <para>
   QEMU can access an image as a virtual block device directly via
   <systemitem>librbd</systemitem>. This avoids an additional context switch,
   and can take advantage of RBD caching.
  </para>

  <para>
   You can use <command>qemu-img</command> to convert existing virtual machine
   images to Ceph block device images. For example, if you have a qcow2
   image, you could run:
  </para>

<screen>qemu-img convert -f qcow2 -O raw sles12.qcow2 rbd:pool1/sles12</screen>

  <para>
   To run a virtual machine booting from that image, you could run:
  </para>

<screen>qemu -m 1024 -drive format=raw,file=rbd:pool1/sles12</screen>

  <para>
   <link xlink:href="http://ceph.com/docs/master/rbd/rbd-config-ref/#cache-settings">RBD
   caching</link> can significantly improve performance. QEMU’s cache
   options control <systemitem>librbd</systemitem> caching:
  </para>

<screen>qemu -m 1024 -drive format=rbd,file=rbd:pool1/sles12,cache=writeback</screen>
 </sect1>
 <sect1>
  <title>Enabling Discard/TRIM</title>

  <para>
   Ceph block devices support the discard operation. This means that a guest
   can send TRIM requests to let a Ceph block device reclaim unused space.
   This can be enabled in the guest by mounting <systemitem>XFS</systemitem>
   with the discard option.
  </para>

  <para>
   For this to be available to the guest, it must be explicitly enabled for the
   block device. To do this, you must specify a
   <option>discard_granularity</option> associated with the drive:
  </para>

<screen>qemu -m 1024 -drive format=raw,file=rbd:pool1/sles12,id=drive1,if=none \
-device driver=ide-hd,drive=drive1,discard_granularity=512</screen>

  <note>
   <para>
    The above example uses the IDE driver. The virtio driver does not support
    discard.
   </para>
  </note>

  <para>
   If using <systemitem>libvirt</systemitem>, edit your libvirt domain’s
   configuration file using <command>virsh edit</command> to include the
   <literal>xmlns:qemu</literal> value. Then, add a <literal>qemu:commandline
   block</literal> as a child of that domain. The following example shows how
   to set two devices with <literal>qemu id=</literal> to different
   <literal>discard_granularity</literal> values.
  </para>

<screen>
&lt;domain type='kvm' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'&gt;
 &lt;qemu:commandline&gt;
  &lt;qemu:arg value='-set'/&gt;
  &lt;qemu:arg value='block.scsi0-0-0.discard_granularity=4096'/&gt;
  &lt;qemu:arg value='-set'/&gt;
  &lt;qemu:arg value='block.scsi0-0-1.discard_granularity=65536'/&gt;
 &lt;/qemu:commandline&gt;
&lt;/domain&gt;</screen>
 </sect1>
 <sect1>
  <title>QEMU Cache Options</title>

  <para>
   QEMU’s cache options correspond to the following Ceph RBD Cache
   settings.
  </para>

  <para>
   Writeback:
  </para>

<screen>rbd_cache = true</screen>

  <para>
   Writethrough:
  </para>

<screen>rbd_cache = true
rbd_cache_max_dirty = 0</screen>

  <para>
   None:
  </para>

<screen>rbd_cache = false</screen>

  <para>
   QEMU’s cache settings override Ceph’s default settings (settings
   that are not explicitly set in the Ceph configuration file). If you
   explicitly set
   <link xlink:href="http://ceph.com/docs/master/rbd/rbd-config-ref/#cache-settings">RBD
   Cache</link> settings in your Ceph configuration file, your Ceph
   settings override the QEMU cache settings. If you set cache settings on
   the QEMU command line, the QEMU command line settings override the
   Ceph configuration file settings.
  </para>
 </sect1>
</chapter>
 </part>
 <part xml:id="part.best">
  <title>Best Practices</title>
  <chapter xml:base="admin_bestpractices_intro.xml" version="5.0" xml:id="cha.storage.bestpractice">
 <title>Introduction</title>
 <para>
  This chapter introduces a list of selected topics which you may encounter
  when managing the Ceph environment. To every topic there is a recommended
  solution that helps you understand or fix the existing problem. The topics
  are sorted into relevant categories.
 </para>
 <sect1 xml:id="storage.bp.report_bug">
  <title>Reporting Software Problems</title>

  <para>
   If you come across a problem when running SUSE Enterprise Storage related to some of its
   components, such as Ceph, <phrase>RADOS Gateway</phrase>, or Calamari, report the problem to SUSE
   Technical Support. The recommended way is with the
   <command>supportconfig</command> utility.
  </para>

  <tip>
   <para>
    Because <command>supportconfig</command> is modular software, make sure
    that the <systemitem>supportutils-plugin-ses</systemitem> package is
    installed.
   </para>
<screen>rpm -q supportutils-plugin-ses</screen>
   <para>
    If it is missing on the Ceph server, install it with
   </para>
<screen>zypper ref &amp;&amp; zypper in supportutils-plugin-ses</screen>
  </tip>

  <para>
   Although you can use <command>supportconfig</command> on the command line,
   we recommend using the related YaST module. Find more information about
   <command>supportconfig</command> in
   <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_sle_admin/book_sle_admin.html#sec.admsupport.supportconfig"/>.
  </para>
 </sect1>
</chapter>
  <chapter xml:base="admin_bestpractices_hwrecommend.xml" version="5.0" xml:id="storage.bp.hwreq">
 <title>Hardware Recommendations</title>
 <sect1 xml:id="storage.bp.hwreq.replicas">
  <title>Can I Reduce Data Replication</title>

  <para>
   Ceph stores data within pools. Pools are logical groups for storing
   objects. Data objects within a pool are replicated so that they can be
   recovered when OSDs fail. New pools are created with the default of three
   replicas. This number includes the 'original' data object itself. Three
   replicas then mean the data object and two its copies for a total of three
   instances.
  </para>

  <para>
   You can manually change the number of pool replicas (see
   <xref linkend="ceph.pools.options.num_of_replicas" role="internalbook"/>). Setting a pool to two
   replicas means that there is only <emphasis>one</emphasis> copy of the data
   object besides the object itself, so if you lose one object instance, you
   need to trust that the other copy has not been corrupted for example since
   the last
   <link xlink:href="http://ceph.com/docs/master/rados/configuration/osd-config-ref/#scrubbing">scrubbing</link>
   during recovery.
  </para>

  <para>
   Setting a pool to one replica means that there is exactly
   <emphasis>one</emphasis> instance of the data object in the pool. If the OSD
   fails, you lose the data. A possible usage for a pool with one replica is
   storing temporary data for a short time.
  </para>

  <para>
   Setting more than three replicas for a pool means only a small increase in
   reliability, but may be suitable in rare cases. Remember that the more
   replicas, tho more disk space needed for storing the object copies. If you
   need the ultimate data security, we recommend using erasure coded pools. For
   more information, see <xref linkend="cha.ceph.erasure" role="internalbook"/>.
  </para>

  <warning>
   <para>
    We strongly encourage you to either leave the number of replicas for a pool
    at the default value of 3, or use higher value if suitable. Setting the
    number of replicas to a smaller number is dangerous and may cause the loss
    of data stored in the cluster.
   </para>
  </warning>
 </sect1>
 <sect1 xml:id="storage.bp.hwreq.ec">
  <title>Can I Reduce Redundancy Similar to RAID 6 Arrays?</title>

  <para>
   When creating a new pool, Ceph uses the replica type by default, which
   replicates objects across multiple disks to be able to recover from an OSD
   failure. While this type of pool is safe, it uses a lot of disk space to
   store objects.
  </para>

  <para>
   To reduce the disk space needed, Ceph implements <emphasis>erasure
   coded</emphasis> pools. This method adds extra chunks of data to detect
   errors in a data stream. Erasure coded pools exhibit similar performance,
   reliability, and storage saved as RAID 6 arrays.
  </para>

  <para>
   As erasure coding is a complex topic, you need to study it properly to be
   able to deploy it for optimum performance. For more information, see
   <xref linkend="cha.ceph.erasure" role="internalbook"/>.
  </para>
 </sect1>
 <sect1 xml:id="ses.bp.mindisk">
  <title>What is the Minimum Disk Size for an OSD node?</title>

  <para>
   There are two types of disk space needed to run on OSD: the space for the
   disk journal, and the space for the stored data. The minimum (and default)
   value for the journal is 6GB. The minimum space for data is 5GB as
   partitions smaller than 5GB are automatically assigned the weight of 0.
  </para>

  <para>
   So although the minimum disk space for an OSD is 11GB, we do not recommend a
   disk smaller than 20GB, even for testing purposes.
  </para>
 </sect1>
 <sect1 xml:id="ses.bp.ram">
  <title>How Much RAM Do I Need in a Storage Server?</title>

  <para>
   The recommended minimum is 2GB per OSD. Note that during recovery, 1 or even
   2GB of RAM per terabyte of OSD disk space is optimal.
  </para>
 </sect1>
 <sect1 xml:id="ses.bp.diskshare">
  <title>OSD and Monitor Sharing One Server</title>

  <para>
   Although it is technically possible to run OSDs and monitor nodes on the
   same server in test environments, we strongly recommend having a separate
   server for each monitor node in production. The main reason is
   performance—the more OSDs the cluster has, the more I/O operations the
   monitor nodes need to perform. And when one server is shared between a
   monitor node and OSD(s), the OSD I/O operations are a limiting factor for
   the monitor node.
  </para>

  <para>
   Another aspect is whether to share disks between an OSD, a monitor node, and
   the operating system on the server. The answer is simple: if possible,
   dedicate a separate disk to OSD, and a separate server to a monitor node.
  </para>

  <para>
   Although Ceph supports directory-based OSDs, an OSD should always have a
   dedicated disk other than the operating system one.
  </para>

  <tip>
   <para>
    If it is <emphasis>really</emphasis> necessary to run OSD and monitor node
    on the same server, run the monitor on a separate disk by mounting the disk
    to the <filename>/var/lib/ceph/mon</filename> directory for slightly better
    performance.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ses.bp.numofdisks">
  <title>How Many Disks Can I Have in a Server</title>

  <para>
   You can have as many disks in one server as it allows. There are a few
   things to consider when planning the number of disks per server:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <emphasis>Network bandwidth.</emphasis> The more disks you have in a
     server, the more data must be transferred via the network card(s) for the
     disk write operations.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Memory.</emphasis> For optimum performance, reserve at least 2GB
     of RAM per terabyte of disk space installed.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Fault tolerance.</emphasis> If the complete server fails, the
     more disks it has, the more OSDs the cluster temporarily loses. Moreover,
     to keep the replication rules running, you need to copy all the data from
     the failed server between the other nodes in the cluster.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ses.bp.share_ssd_journal">
  <title>How Many OSDs Can Share a Single SSD Journal</title>

  <para>
   Solid-state drives (SSD) have no moving parts. This reduces random access
   time and read latency while accelerating data throughput. Because their
   price per 1MB is significantly higher than the price of spinning hard disks,
   SSDs are only suitable for smaller storage.
  </para>

  <para>
   OSDs may see a significant performance improvement by storing their journal
   on an SSD and the object data on a separate hard disk. The <option>osd
   journal</option> configuration setting defaults to
   <filename>/var/lib/ceph/osd/<replaceable>cluster</replaceable>-<replaceable>id</replaceable>/journal</filename>.
   You can mount this path to an SSD or to an SSD partition so that it is not
   merely a file on the same disk as the object data.
  </para>

  <tip>
   <title>Sharing an SSD for Multiple Journals</title>
   <para>
    As journal data occupies relatively small space, you can mount several
    journal directories to a single SSD disk. Keep in mind that with each
    shared journal, the performance of the SSD disk degrades. We do not
    recommend sharing more than 6 journals on the same SSD disk.
   </para>
  </tip>
 </sect1>
</chapter>
  <chapter xml:base="admin_bestpractices_install.xml" version="5.0" xml:id="storage.bp.inst">
 <title>Cluster Administration</title>
 <para>
  The chapter describes some useful operations that can be performed after the
  cluster is completely deployed and running, like adding nodes, disks.
 </para>
 <sect1 xml:id="storage.bp.inst.cephdeploy_usage">
  <title>Using <command>ceph-deploy</command> on an Already Setup Server</title>

  <para>
   <command>ceph-deploy</command> is a command line utility to easily deploy a
   Ceph cluster (see <xref linkend="ceph.install.ceph-deploy" role="internalbook"/>). After the
   cluster is deployed, you can use <command>ceph-deploy</command> to
   administer the clusters' nodes. You can add OSD nodes, monitor nodes, gather
   authentication keys, or purge a running cluster.
   <command>ceph-deploy</command> has the following general syntax:
  </para>

<screen>ceph-deploy <replaceable>subcommands</replaceable> <replaceable>options</replaceable></screen>

  <para>
   A list of selected <command>ceph-deploy</command> subcommands with short
   descriptions follow.
  </para>

  <tip>
   <para>
    Administer Ceph nodes with <command>ceph-deploy</command> from the admin
    node. Before administering them, always create a new temporary directory
    and <command>cd</command> into it. Then choose one monitor node and gather
    the authentication keys with the <command>gatherkeys</command> subcommand
    from it, and copy the <filename>/etc/ceph/ceph.conf</filename> file from
    the monitor node into the current local directory.
   </para>
<screen><prompt>cephadm &gt; </prompt> mkdir ceph_tmp
<prompt>cephadm &gt; </prompt> cd ceph_tmp
<prompt>cephadm &gt; </prompt> ceph-deploy gatherkeys ceph_mon_host
<prompt>cephadm &gt; </prompt> scp ceph_mon_host:/etc/ceph/ceph.conf .</screen>
  </tip>

  <variablelist>
   <varlistentry>
    <term>gatherkeys</term>
    <listitem>
     <para>
      Gather authentication keys for provisioning new nodes. It takes host
      names as arguments. It checks for and fetches <literal>client.admin
      keyring</literal>, monitor keyring and
      <literal>bootstrap-mds/bootstrap-osd</literal> keyring from monitor host.
      These authentication keys are used when new
      <literal>monitors/OSDs/MDS</literal> are added to the cluster.
     </para>
     <para>
      Usage:
     </para>
<screen>ceph-deploy gatherkeys <replaceable>hostname</replaceable></screen>
     <para>
      <replaceable>hostname</replaceable> is the host name of the monitor from
      where keys are to be pulled.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mon add</term>
    <listitem>
     <para>
      Adds a monitor to an existing cluster. It first detects the platform and
      distribution for the target host, and checks if the host name is
      compatible for deployment. It then uses the monitor keyring, ensures
      configuration for new monitor host and adds the monitor to the cluster.
      If the section for the monitor exists, it can define the monitor address
      by the <literal>mon addr</literal> option, otherwise it will fall back by
      resolving the host name to an IP. If <option>--address</option> is used,
      it will override all other options. After adding the monitor to the
      cluster, it gives it some time to start. It then looks for any monitor
      errors, and checks monitor status. Monitor errors arise if the monitor is
      not added in the <option>mon initial members</option> option, if it does
      not exist in <option>monmap</option>, or if neither
      <option>public_addr</option> nor <option>public_network</option> keys
      were defined for monitors. Under such conditions, monitors may not be
      able to form quorum. Monitor status tells if the monitor is up and
      running normally. The status is checked by running <command>ceph daemon
      mon.hostname mon_status</command> on remote end which provides the output
      and returns a Boolean status of what is going on.
      <literal>False</literal> means a monitor that is not fine even if it is
      up and running, while <literal>True</literal> means the monitor is up and
      running correctly.
     </para>
     <para>
      Usage:
     </para>
<screen>ceph-deploy mon add <replaceable>host</replaceable>
ceph-deploy mon add <replaceable>host</replaceable> <option>--address <replaceable>IP</replaceable></option></screen>
     <para>
      <replaceable>host</replaceable> is the host name and
      <replaceable>IP</replaceable> is the IP address of the desired monitor
      node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>osd prepare</term>
    <listitem>
     <para>
      Prepares a directory, disk or drive for a Ceph OSD. It first checks
      against multiple OSDs getting created and warns about the possibility of
      more than the recommended which would cause issues with max allowed PIDs
      in a system. It then reads the bootstrap-osd key for the cluster or
      writes the bootstrap key if not found. It then uses
      <command>ceph-disk</command> utility’s <command>prepare</command>
      subcommand to prepare the disk and journal and deploy the OSD on the
      desired host. It gives some time to the OSD to settle and checks for any
      possible errors and if found, reports them to the user.
     </para>
     <para>
      Usage:
     </para>
<screen>ceph-deploy osd prepare <replaceable>host</replaceable>:<replaceable>disk</replaceable>[<replaceable>journal</replaceable>] ...</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>osd activate</term>
    <listitem>
     <para>
      Activates the OSD prepared using the <command>prepare</command>
      subcommand. It actually uses <command>ceph-disk</command> utility’s
      <command>activate</command> subcommand to activate the OSD with the
      appropriate initialization type based on the distribution. When
      activated, it gives some time to the OSD to start and checks for any
      possible errors and if found, reports to the user. It checks the status
      of the prepared OSD, checks the OSD tree and makes sure the OSDs are up
      and in.
     </para>
     <tip>
      <para>
       <command>osd activate</command> is usually not needed as
       <systemitem>udev</systemitem> rules explicitly trigger "activate" after
       a disk is prepared after <command>osd prepare</command>.
      </para>
     </tip>
     <para>
      Usage:
     </para>
<screen>ceph-deploy osd activate <replaceable>host</replaceable>:<replaceable>disk</replaceable>[<replaceable>journal</replaceable>] ...</screen>
     <tip>
      <para>
       You can use <command>ceph-deploy osd create</command> to join
       <command>prepare</command> and <command>activate</command> functionality
       into one command.
      </para>
     </tip>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rgw prepare/activate/create</term>
    <listitem>
     <para>
      Find more information in <xref linkend="storage.bp.inst.rgw" role="internalbook"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>purge, purgedata, forgetkeys</term>
    <listitem>
     <para>
      You can use the subcommands to completely purge the Ceph cluster (or
      some of its nodes) as if Ceph was never installed on the cluster
      servers. They are typically used when Ceph installation fails and you
      want to start with a clean environment. Or, you can purge one or more
      nodes because you want to remove them from the cluster as their
      life-cycle ends.
     </para>
     <para>
      For more information on purging the cluster or its nodes, see
      <xref linkend="ceph.install.ceph-deploy.purge" role="internalbook"/>.
     </para>
     <tip>
      <para>
       If you do not intend to purge the whole cluster, do not use the
       <command>forgetkeys</command> subcommand, as the keys will remain in
       place for the remaining cluster infrastructure.
      </para>
     </tip>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="storage.bp.inst.add_osd_cephdisk">
  <title>Adding OSDs with <command>ceph-disk</command></title>

  <para>
   <command>ceph-disk</command> is a utility that can prepare and activate a
   disk, partition or directory as a Ceph OSD. It automates the multiple
   steps involved in manual creation and start of an OSD into two steps of
   preparing and activating the OSD by using the subcommands
   <command>prepare</command> and <command>activate</command>.
  </para>

  <variablelist>
   <varlistentry>
    <term><command>prepare</command>
    </term>
    <listitem>
     <para>
      Prepares a directory, disk or drive for a Ceph OSD. It creates a GPT
      partition, marks the partition with Ceph type uuid, creates a file
      system, marks the file system as ready for Ceph consumption, uses
      entire partition and adds a new partition to the journal disk.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>activate</command>
    </term>
    <listitem>
     <para>
      Activates the Ceph OSD. It mounts the volume in a temporary location,
      allocates an OSD ID (if needed), remounts in the correct location
      <filename>/var/lib/ceph/osd/<replaceable>cluster</replaceable>-<replaceable>id</replaceable></filename>
      and starts <command>ceph-osd</command>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The following example shows steps for adding an OSD with
   <command>ceph-osd</command>.
  </para>

  <procedure>
   <step>
    <para>
     Make sure a new disk is physically present on the node where you want to
     add the OSD. In our example, it is <emphasis>node1</emphasis> belonging to
     cluster <emphasis>ceph</emphasis>.
    </para>
   </step>
   <step>
    <para>
     <command>ssh</command> to node1.
    </para>
   </step>
   <step>
    <para>
     Generate a unique identification for the new OSD:
    </para>
<screen>uuidgen
c70c032a-6e88-4962-8376-4aa119cb52ee</screen>
   </step>
   <step>
    <para>
     Prepare the disk:
    </para>
<screen>sudo ceph-disk prepare --cluster ceph \
--cluster-uuid c70c032a-6e88-4962-8376-4aa119cb52ee --fs-type xfs /dev/hdd1</screen>
   </step>
   <step>
    <para>
     Activate the OSD:
    </para>
<screen>sudo ceph-disk activate /dev/hdd1</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="storage.bp.inst.add_osd_cephdeploy">
  <title>Adding OSDs with <command>ceph-deploy</command></title>

  <para>
   <command>ceph-deploy</command> is a command line utility to simplify the
   installation and configuration of a Ceph cluster. It can be used to add or
   remove OSDs as well. To add a new OSD to a node <literal>node2</literal>
   with <command>ceph-deploy</command>, follow these steps:
  </para>

  <tip>
   <para>
    <command>ceph-deploy</command> is usually run from the administration node,
    from which you installed the cluster.
   </para>
  </tip>

  <procedure>
   <step>
    <para>
     List available disks on a node:
    </para>
<screen>ceph-deploy disk list node2
[...]
[node2][DEBUG ] /dev/sr0 other, unknown
[node2][DEBUG ] /dev/vda :
[node2][DEBUG ]  /dev/vda1 swap, swap
[node2][DEBUG ]  /dev/vda2 other, btrfs, mounted on /
[node2][DEBUG ] /dev/vdb :
[node2][DEBUG ]  /dev/vdb1 ceph data, active, cluster ceph, osd.1, journal /dev/vdb2
[node2][DEBUG ]  /dev/vdb2 ceph journal, for /dev/vdb1
[node2][DEBUG ] /dev/vdc other, unknown</screen>
    <para>
     <filename>/dev/vdc</filename> seems to be unused, so let us focus on
     adding it as an OSD.
    </para>
   </step>
   <step>
    <para>
     Zap the disk. Zapping deletes the disk's partition table.
    </para>
<screen>ceph-deploy disk zap node2:vdc</screen>
    <warning>
     <para>
      Zapping deletes all data from the disk
     </para>
    </warning>
   </step>
   <step>
    <para>
     Prepare the OSD. The <command>prepare</command> command expects you to
     specify the disk for data, and optionally the disk for its journal. We
     recommend storing the journal on a separate drive to maximize throughput.
    </para>
<screen>ceph-deploy osd prepare node2:vdc:/dev/ssd</screen>
   </step>

  </procedure>
 </sect1>
 <sect1 xml:id="storage.bp.inst.add_rm_monitor">
  <title>Adding and Removing Monitors</title>

  <para>
   With <command>ceph-deploy</command>, adding and removing monitors is a
   simple task. Also, take into account the following
   restrictions/recommendation.
  </para>

  <important>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <command>ceph-deploy</command> restricts you to only install one monitor
      per host.
     </para>
    </listitem>
    <listitem>
     <para>
      We do not recommend mixing monitors and OSDs on the same host.
     </para>
    </listitem>
    <listitem>
     <para>
      For high availability, you should run a production Ceph cluster with
      <emphasis>at least</emphasis> three monitors.
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <sect2 xml:id="storage.bp.inst.add_rm_monitor.addmon">
   <title>Adding a Monitor</title>
   <para>
    After you create a cluster and install Ceph packages to the monitor
    host(s) (see <xref linkend="ceph.install.ceph-deploy" role="internalbook"/> for more
    information), you may deploy the monitors to the monitor hosts. You may
    specify more monitor host names in the same command.
   </para>
<screen>ceph-deploy mon create <replaceable>host-name</replaceable></screen>
   <note>
    <para>
     When adding a monitor on a host that was not in hosts initially defined
     with the <command>ceph-deploy new</command> command, a <option>public
     network</option> statement needs to be added to the
     <filename>ceph.conf</filename> file.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="storage.bp.inst.add_rm_monitor.rmmon">
   <title>Removing a Monitor</title>
   <para>
    If you have a monitor in your cluster that you want to remove, you may use
    the destroy option. You may specify more monitor host names in the same
    command.
   </para>
<screen>ceph-deploy mon destroy <replaceable>host-name</replaceable></screen>
   <note>
    <para>
     Ensure that if you remove a monitor, the remaining monitors will be able
     to establish a consensus. If that is not possible, consider adding a
     monitor before removing the monitor you want to take offline.
    </para>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.inst.rgw">
  <title>Usage of <command>ceph-deploy rgw</command></title>

  <para>
   The <command>ceph-deploy</command> script includes the
   <command>rgw</command> component that helps you manage <phrase>RADOS Gateway</phrase> instances. Its
   general form follows this pattern:
  </para>

<screen>ceph-deploy rgw <replaceable>subcommand</replaceable> <replaceable>rgw-host</replaceable>:<replaceable>rgw-instance</replaceable>:<replaceable>fqdn</replaceable>:<replaceable>port</replaceable>:<replaceable>redirect</replaceable></screen>

  <variablelist>
   <varlistentry>
    <term>subcommand</term>
    <listitem>
     <para>
      One of <command>list</command>, <command>prepare</command>,
      <command>activate</command>, <command>create</command> (=
      <command>prepare</command> + <command>activate</command>), or
      <command>delete</command>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rgw-host</term>
    <listitem>
     <para>
      Host name where you want to operate the <phrase>RADOS Gateway</phrase>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rgw-instance</term>
    <listitem>
     <para>
      Ceph instance name. Default is 'rgw-host'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>fqdn</term>
    <listitem>
     <para>
      Virtual host to listen to. Default is 'None'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>port</term>
    <listitem>
     <para>
      Port to listen to. Default is 80.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>redirect</term>
    <listitem>
     <para>
      The URL redirect. Default is '^/(.*)'.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For example:
  </para>

<screen>ceph-deploy rgw prepare example_host2:gateway1</screen>

  <para>
   or
  </para>

<screen>ceph-deploy activate example_host1:gateway1:virtual_srv2:81</screen>

  <tip>
   <title>Specifying Multiple <phrase>RADOS Gateway</phrase> Instances</title>
   <para>
    You can specify more <option>rgw_hostname:rgw_instance</option> pairs on
    the same command line if you separate them with a comma:
   </para>
<screen>ceph-deploy rgw create hostname1:rgw,hostname2:rgw,hostname3:rgw</screen>
  </tip>

  <para>
   For a practical example of setting <phrase>RADOS Gateway</phrase> with
   <command>ceph-deploy</command>, see <xref linkend="ses.rgw.config" role="internalbook"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.inst.rgw_client">
  <title><phrase>RADOS Gateway</phrase> Client Usage</title>

  <para>
   To use <phrase>RADOS Gateway</phrase> REST interfaces, you need to create a user for the S3
   interface, then a subuser for the Swift interface. Find more information on
   creating <phrase>RADOS Gateway</phrase> users in <xref linkend="storage.bp.account.swiftadd" role="internalbook"/> and
   <xref linkend="storage.bp.account.s3add" role="internalbook"/>.
  </para>

  <sect2>
   <title>S3 Interface Access</title>
   <para>
    To access the S3 interface, you need to write a Python script. The script
    will connect to <phrase>RADOS Gateway</phrase>, create a new bucket, and list all buckets. The
    values for <option>aws_access_key_id</option> and
    <option>aws_secret_access_key</option> are taken from the values of
    <option>access_key</option> and <option>secret_key</option> returned by the
    <command>radosgw_admin</command> command from
    <xref linkend="storage.bp.account.s3add" role="internalbook"/>.
   </para>
   <procedure>
    <step>
     <para>
      Install the <systemitem>python-boto</systemitem> package:
     </para>
<screen>sudo zypper in python-boto</screen>
    </step>
    <step>
     <para>
      Create a new Python script called <filename>s3test.py</filename> with the
      following content:
     </para>
<screen>import boto
import boto.s3.connection
access_key = '11BS02LGFB6AL6H1ADMW'
secret_key = 'vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY'
conn = boto.connect_s3(
aws_access_key_id = access_key,
aws_secret_access_key = secret_key,
host = '{hostname}',
is_secure=False,
calling_format = boto.s3.connection.OrdinaryCallingFormat(),
)
bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
print "{name}\t{created}".format(
name = bucket.name,
created = bucket.creation_date,
)</screen>
     <para>
      Replace <literal>{hostname}</literal> with the host name of the host
      where you configured <phrase>RADOS Gateway</phrase> service, for example
      <literal>gateway_host</literal>.
     </para>
    </step>
    <step>
     <para>
      Run the script:
     </para>
<screen>python s3test.py</screen>
     <para>
      The script outputs something like the following:
     </para>
<screen>my-new-bucket 2015-07-22T15:37:42.000Z</screen>
    </step>
   </procedure>
  </sect2>

  <sect2>
   <title>Swift Interface Access</title>
   <para>
    To access <phrase>RADOS Gateway</phrase> via Swift interface, you need the <command>swift</command>
    command line client. Its manual page <command>man 1 swift</command> tells
    you more about its command line options.
   </para>
   <para>
    To install <command>swift</command>, run the following:
   </para>
<screen>sudo zypper in python-swiftclient</screen>
   <para>
    The swift access uses the following syntax:
   </para>
<screen>swift -A http://<replaceable>IP_ADDRESS</replaceable>/auth/1.0 \
-U example_user:swift -K '<replaceable>swift_secret_key</replaceable>' list</screen>
   <para>
    Replace <replaceable>IP_ADDRESS</replaceable> with the IP address of the
    gateway server, and <replaceable>swift_secret_key</replaceable> with its
    value from the output of the <command>radosgw-admin key create</command>
    command executed for the <systemitem>swift</systemitem> user in
    <xref linkend="storage.bp.account.swiftadd" role="internalbook"/>.
   </para>
   <para>
    For example:
   </para>
<screen>swift -A http://gateway.example.com/auth/1.0 -U example_user:swift \
-K 'r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h' list</screen>
   <para>
    The output is:
   </para>
<screen>my-new-bucket</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="salt.automated.installation">
  <title>Automated Installation via Salt</title>

  <para>
   The installation can be automated by using the Salt reactor. For virtual
   environments or consistent hardware environments, this configuration will
   allow the creation of a Ceph cluster with the specified behavior.
  </para>

  <warning>
   <para>
    Salt cannot perform dependency checks based on reactor events. Putting
    your Salt master into a death spiral is a real risk.
   </para>
  </warning>

  <para>
   The automated installation requires the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A properly created
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
     Prepared custom configuration, placed to the
     <filename>/srv/pillar/ceph/stack</filename> directory.
    </para>
   </listitem>
   <listitem>
    <para>
     The example reactor file
     <filename>/usr/share/doc/packages/deepsea/reactor.conf</filename> must be
     copied to <filename>/etc/salt/master.d/reactor.conf</filename>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The default reactor configuration will only run Stages 0 and 1. This allows
   testing of the reactor without waiting for subsequent stages to complete.
  </para>

  <para>
   When the first salt-minion starts, Stage 0 will begin. A lock prevents
   multiple instances. When all minions complete Stage 0, Stage 1 will begin.
  </para>

  <para>
   If the operation is performed properly, change the last line in the
   <filename>/etc/salt/master.d/reactor.conf</filename>:
  </para>

<screen>- /srv/salt/ceph/reactor/discovery.sls</screen>

  <para>
   to
  </para>

<screen>- /srv/salt/ceph/reactor/all_stages.sls</screen>
 </sect1>
 
 <sect1 xml:id="Deepsea.restart">
 	<title>Restarting Ceph services using DeepSea</title>
 	<para>
 	When you install updates, specifically ceph-&lt;mon,osd etc&gt; you need to restart the services to make use of the recently installed version. To do so, run:
 	</para>
 	<screen>salt-run state.orch ceph.restart</screen>
 	<para>
 	The script iterates over all roles you have configured in the following order: MON, OSD, MDS, RGW, IGW. To keep the downtime low and to find potential issues as early as possible, nodes are restarted sequentially. For example, only one monitoring node is restarted at a time. The command also waits for the cluster to recover if the cluster is in a degraded unhealthy state.
 	</para>
 	<note>
 		<title>Watching the Restarting</title>
 		<para>
 		The process of restarting the cluster may take some time. You can watch the events by using the Salt event bus by running:
 		</para>
 		<screen>salt-run state.event pretty=True</screen>
 	</note>
 </sect1>
 
 <sect1 xml:id="bp.node.management">
  <title>Node Management</title>

  <para>
   After you set up a complete cluster you may need to perform additional
   changes to the cluster like adding or removing monitoring nodes or
   adding/removing Ceph  OSD nodes. Adding and removing of cluster nodes
   can be done without shutting down the whole cluster, but it might increase
   replication traffic.
  </para>

  <note>
   <title>Limitations</title>
   <para>
    The procedures described in sections: <xref linkend="Adding.OSD.Nodes" role="internalbook"/>
    and <xref linkend="Removing.OSD.Nodes" role="internalbook"/> can be performed only with the
    default CRUSH map. The default CRUSH map must have been created by using
    <literal>Ceph-deploy</literal> or <emphasis>DeepSea</emphasis>.
   </para>
  </note>

  <sect2 xml:id="Adding.OSD.Nodes">
   <title>Adding Ceph OSD Nodes</title>
   <para>
    The procedure below describes adding of a Ceph OSD node to your cluster.
   </para>
   <procedure xml:id="proc.Adding.Ceph.Node">
    <title>Adding a Ceph OSD Node</title>
    <step>
     <para>
      List all Ceph OSD nodes and then choose a proper name for the new
      node/s
     </para>
<screen>ceph osd tree</screen>
    </step>
    <step>
     <para>
      Inspect your CRUSH map to find out the bucket type, for a procedure refer
      to <xref linkend="op.crush" role="internalbook"/>. Typically the bucket type is
      <emphasis>host</emphasis>.
     </para>
    </step>
    <step>
     <para>
      Create a record for the new node in your CRUSH map.
     </para>
<screen>ceph osd crush add-bucket <replaceable>{bucket name} {bucket type}</replaceable></screen>
     <para>
      for example:
     </para>
<screen>ceph osd crush add-bucket ses4-4 host</screen>
    </step>
    <step>
     <para>
      Add all OSD that the new node should use. For a procedure refer to
      <xref linkend="storage.bp.inst.add_osd_cephdeploy" role="internalbook"/>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="Removing.OSD.Nodes">
   <title>Removing Ceph OSD Nodes</title>
   <para>
    To remove a Ceph OSD node follow the procedure:
   </para>
   <procedure xml:id="proc.Removing.Ceph.Node">
    <title>Removing a Ceph OSD Node</title>
    <step>
     <para>
      Remove all OSD on the node you want to delete as described in
      <xref linkend="storage.bp.disk.del" role="internalbook"/>.
     </para>
    </step>
    <step>
     <para>
      Verify that all OSDs have been removed:
     </para>
<screen>ceph osd tree</screen>
     <para>
      The OSD to be removed must not have any OSD.
     </para>
    </step>
    <step>
     <para>
      Remove the node from the cluster:
     </para>
<screen>ceph osd crush remove <replaceable>{bucket name}</replaceable></screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="salt.node.removing">
   <title>Removing and Reinstalling Salt Cluster Nodes</title>
   <para>
    You may want remove a role from your minion, to do so use the Stage 5
    command:
   </para>
<screen><prompt>root # </prompt>salt-run state.orch ceph.stage.5</screen>
   <para>
    When a role is removed from a minion, the objective is to undo all changes
    related to that role. For most of the roles, the task is simple, but there
    may be problems with package dependencies. If a package is uninstalled, its
    dependencies are not.
   </para>
   <para>
    Removed OSDs appear as blank drives. The related tasks overwrite the
    beginning of the file systems and remove backup partitions in addition to
    wiping the partition tables.
   </para>
   <note>
    <title>Preserving Partitions Created by Other Methods</title>
    <para>
     Disk drives previously configured by other methods, such as
     <command>ceph-deploy</command>, may still contain partitions. DeepSea
     will not automatically destroy these. Currently, the administrator must
     reclaim these drives.
    </para>
   </note>
  </sect2>
 </sect1>
</chapter>
  <chapter xml:base="admin_bestpractices_monitoring.xml" version="5.0" xml:id="storage.bp.monitoring">
 <title>Monitoring</title>
 <para/>
 <sect1 xml:id="storage.bp.monitoring.calamari_usage_graphs">
  <title>Usage Graphs on Calamari</title>

  <para>
   Calamari—a Ceph's Web front-end for managing and monitoring the
   cluster—includes several graphs on the cluster's usage.
  </para>

  <para>
   At the bottom of the <guimenu>Dashboard</guimenu>—the home page of
   Calamari—there are two usage related boxes. While
   <guimenu>IOPS</guimenu> shows the cluster's overall number of input/output
   operations per second, the <guimenu>Usage</guimenu> graph shows the number
   of the cluster's total/used disk space.
  </para>

  <figure>
   <title>IOPS and Usage Graphs on Calamari Dashboard</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="calamari_iops_usage.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="calamari_iops_usage.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   You can find more detailed and interactive graphs after clicking the
   <guimenu>Charts</guimenu> menu item. It shows the cluster's overall
   input/output operations per second and free disk space by default. Select
   <guimenu>Pool IOPS</guimenu> from the top drop-down box to detail the view
   by existing pools.
  </para>

  <figure>
   <title>Pool IOPS Detailed View</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="calamari_mypool_iops.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="calamari_mypool_iops.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   By moving the slider in the <guimenu>Time Axis</guimenu> pane, you can
   change the displayed time interval in the graph. By moving the mouse over
   the graph, the time/read/write information changes accordingly. By clicking
   and dragging the mouse horizontally across the graph, the specified time
   interval gets zoomed. You can see more help by moving the mouse over the
   little question mark in the top right corner of the graph.
  </para>

  <para>
   If you select the host name of a specific Ceph server, Calamari displays
   detailed information about CPU, average load, and memory related to the
   specified host.
  </para>

  <figure>
   <title>Ceph Host Average Load</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="calamari_host_avgload.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="calamari_host_avgload.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1 xml:id="storage.bp.monitoring.fullosd">
  <title>Checking for Full OSDs</title>

  <para>
   Ceph prevents you from writing to a full OSD so that you do not lose data.
   In an operational cluster, you should receive a warning when your cluster is
   getting near its full ratio. The <command>mon osd full ratio</command>
   defaults to 0.95, or 95% of capacity before it stops clients from writing
   data. The <command>mon osd nearfull ratio</command> defaults to 0.85, or 85%
   of capacity, when it generates a health warning.
  </para>

  <para>
   Full OSD nodes will be reported by <command>ceph health</command>:
  </para>

<screen>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   or
  </para>

<screen>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   The best way to deal with a full cluster is to add new OSD nodes allowing
   the cluster to redistribute data to the newly available storage.
  </para>

  <para>
   If you cannot start an OSD because it is full, you may delete some data by
   deleting some placement group directories in the full OSD.
  </para>

  <tip>
   <title>Preventing Full OSDs</title>
   <para>
    After an OSD becomes full—is uses 100% of its disk space—it
    will normally crash quickly without warning. Following are a few tips to
    remember when administering OSD nodes.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Each OSD's disk space (usually mounted under
      <filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) needs to be placed on
      a dedicated underlying disk or partition.
     </para>
    </listitem>
    <listitem>
     <para>
      Check the Ceph configuration files and make sure that Ceph does not
      store its log file to the disks/partitions dedicated for use by OSDs.
     </para>
    </listitem>
    <listitem>
     <para>
      Make sure that no other process writes to the disks/partitions dedicated
      for use by OSDs.
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 <sect1 xml:id="storage.bp.monitoring.osd">
  <title>Checking if OSD Daemons are Running on a Node</title>

  <para>
   To check the status of OSD services on a specific node, log in to the node,
   and run the following:
  </para>

<screen>sudo systemctl status ceph-osd*
 ceph-osd@0.service - Ceph object storage daemon
    Loaded: loaded (/usr/lib/systemd/system/ceph-osd@.service; enabled)
    Active: active (running) since Fri 2015-02-20 11:13:18 CET; 2 days ago
  Main PID: 1822 (ceph-osd)
    CGroup: /system.slice/system-ceph\x2dosd.slice/ceph-osd@0.service
            └─1822 /usr/bin/ceph-osd -f --cluster ceph --id 0</screen>

  <para>
   For more information, see <xref linkend="ceph.operating.services" role="internalbook"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.monitoring.mon">
  <title>Checking if Monitor Daemons are Running on a Node</title>

  <para>
   To check the status of monitor services on a specific node, log in to the
   node, and run the following:
  </para>

<screen>sudo systemctl status ceph-mon*
 ceph-mon@doc-ceph1.service - Ceph cluster monitor daemon
    Loaded: loaded (/usr/lib/systemd/system/ceph-mon@.service; enabled)
    Active: active (running) since Wed 2015-02-18 16:57:17 CET; 4 days ago
  Main PID: 1203 (ceph-mon)
    CGroup: /system.slice/system-ceph\x2dmon.slice/ceph-mon@doc-ceph1.service
            └─1203 /usr/bin/ceph-mon -f --cluster ceph --id doc-ceph1</screen>

  <para>
   For more information, see <xref linkend="ceph.operating.services" role="internalbook"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.monitoring.diskfails">
  <title>What Happens When a Disk Fails?</title>

  <para>
   When a disk with a stored cluster data has a hardware problem and fails to
   operate, here is what happens:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     The related OSD crashed and is automatically removed from the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     The failed disk's data is replicated to another OSD in the cluster from
     other copies of the same data stored in other OSDs.
    </para>
   </listitem>
   <listitem>
    <para>
     Then you should remove the disk from the cluster CRUSH Map, and
     physically from the host hardware.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="storage.bp.monitoring.journalfails">
  <title>What Happens When a Journal Disk Fails?</title>

  <para>
   Ceph OSDs use journaling file systems (see
   <link xlink:href="http://en.wikipedia.org/wiki/Journaling_file_system"/> for
   more information) to store data. When a disk dedicated to a journal fails,
   the related OSD(s) fail as well (see
   <xref linkend="storage.bp.monitoring.diskfails" role="internalbook"/>).
  </para>

  <warning>
   <title>Hosting Multiple Journals on One Disk</title>
   <para>
    For performance boost, you can use a fast disk (such as SSD) to store
    journal partitions for several OSDs. We do not recommend to host journals
    for more than 4 OSDs on one disk, because in case of the journals' disk
    failure, you risk losing stored data for all the related OSDs' disks.
   </para>
  </warning>
 </sect1>
</chapter>
  <chapter xml:base="admin_bestpractices_diskmgmt.xml" version="5.0" xml:id="storage.bp.disk">
 <title>Disk Management</title>
 <para/>
 <sect1 xml:id="storage.bp.disk.add">
  <title>Adding Disks</title>

  <important>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      This can be done on a live cluster without downtime.
     </para>
    </listitem>
    <listitem>
     <para>
      This will cause increased replication traffic between servers.
     </para>
    </listitem>
    <listitem>
     <para>
      Doing this operation repeatedly before the last operation has completed
      replication can save the cluster overall rebuild time.
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   To add a disk (<filename>/dev/sdd</filename> in our example) to a Ceph
   cluster, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Create a partition <literal>sdd1</literal> on the disk:
    </para>
<screen>sudo parted /dev/sdd1 mkpart primary 0.0 -1s</screen>
   </step>
   <step>
    <para>
     Format the partition with XFS file system:
    </para>
<screen>sudo mkfs.xfs -f /dev/sdd1</screen>
   </step>
   <step>
    <para>
     Find out the UUID (Universally Unique Identifier) of the disk:
    </para>
<screen>ls -l /dev/disk/by-uuid | grep sdd1
 [...] 04bb24f1-d631-47ff-a2ee-22d94ad4f80c -&gt; ../../sdd1</screen>
   </step>
   <step>
    <para>
     Add the corresponding line to <filename>/etc/fstab</filename> for the
     example disk <literal>osd.12</literal>:
    </para>
<screen>[...]
 UUID=04bb24f1-d631-47ff-a2ee-22d94ad4f80c /mnt/osd.12 xfs \
 defaults,errors=remount-ro 0 1
 [...]</screen>
   </step>
   <step>
    <para>
     Mount the disk:
    </para>
<screen>sudo mount /mnt/osd.12</screen>
   </step>
   <step>
    <para>
     Add the new disk to <filename>/etc/ceph/ceph.conf</filename> and copy the
     updated configuration file to all other nodes in the cluster.
    </para>
   </step>
   <step>
    <para>
     Create the OSD:
    </para>
<screen>ceph osd create 04bb24f1-d631-47ff-a2ee-22d94ad4f80c</screen>
   </step>
   <step>
    <para>
     Make sure that the new OSD is accepted into the cluster:
    </para>
<screen>sudo mkdir /srv/ceph/04bb24f1-d631-47ff-a2ee-22d94ad4f80c
 ceph-osd -i 12 --mkfs --mkkey
 ceph auth add osd.12 osd 'allow *' mon 'allow rwx' -i /etc/ceph/keyring.osd.12</screen>
   </step>
   <step>
    <para>
     Start the newly added OSD:
    </para>
<screen>sudo systemctl start ceph-osd@12.service</screen>
   </step>
   <step>
    <para>
     Add it to the cluster and allow replication based on CRUSH Map:
    </para>
<screen>ceph osd crush set 12 osd.12 1.0 \
 pool=<replaceable>pool_name</replaceable> rack=<replaceable>rack_name</replaceable> host=<replaceable>host_name</replaceable>-osd</screen>
   </step>
   <step>
    <para>
     Check that the new OSD is in the right place within the cluster:
    </para>
<screen>ceph osd tree</screen>
   </step>
  </procedure>

  <tip>
   <para>
    The process of preparing/adding a disk can be simplified with the
    <command>ceph-disk</command> command. See
    <link xlink:href="http://ceph.com/docs/master/man/8/ceph-disk/"/> for more
    information on <command>ceph-disk</command>.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="storage.bp.disk.del">
  <title>Deleting disks</title>

  <important>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      This can be done on a live cluster without downtime.
     </para>
    </listitem>
    <listitem>
     <para>
      This will cause increased replication traffic between servers.
     </para>
    </listitem>
    <listitem>
     <para>
      Be sure not to remove too many disks from your cluster to be able to keep
      the replication rules. See <xref linkend="datamgm.rules" role="internalbook"/> for more
      information.
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   To delete a disk (for example <literal>osd.12</literal>) from a Ceph
   cluster, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Make sure you have the right disk:
    </para>
<screen>ceph osd tree</screen>
   </step>
   <step>
    <para>
     If the disk is a member of a pool and/or active:
    </para>
    <substeps performance="required">
     <step>
      <para>
       <emphasis>Drain</emphasis> the OSD by setting its weight to zero:
      </para>
<screen>ceph osd crush reweight osd.12 0</screen>
      <para>
       Then wait for all the placement groups to be moved away to other OSDs
       with <command>ceph -w</command>. Optionally, you can check if the OSD is
       emptying with <command>df -h</command>.
      </para>
     </step>
     <step>
      <para>
       Mark the disk out:
      </para>
<screen>ceph osd out 12</screen>
     </step>
     <step>
      <para>
       Stop the related OSD service:
      </para>
<screen>sudo systemctl stop ceph-osd@12.service</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Remove the disk from CRUSH Map:
    </para>
<screen>ceph osd crush remove osd.12</screen>
   </step>
   <step>
    <para>
     Remove authentication information for the disk:
    </para>
<screen>ceph auth del osd.12</screen>
   </step>
   <step>
    <para>
     Remove the disk from the cluster:
    </para>
<screen>ceph osd rm 12</screen>
   </step>
   <step>
    <para>
     Wipe the disk to remove all the data:
    </para>
<screen>sudo sgdisk --zap-all -- <replaceable>disk_device_name</replaceable>
sudo sgdisk --clear --mbrtogpt -- <replaceable>disk_device_name</replaceable></screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="bp.osd_on_exisitng_partitions">
  <title>How to Use Existing Partitions for OSDs Including OSD Journals</title>

  <important>
   <para>
    This section describes an advanced topic that only storage experts and
    developers should examine. It is mostly needed when using non-standard OSD
    journal sizes. If the OSD partition's size is less than 10GB, its initial
    weight is rounded to 0 and because no data are therefore placed on it, you
    should increase its weight. We take no responsibility for overfilled
    journals.
   </para>
  </important>

  <para>
   If you need to use existing disk partitions as an OSD node, the OSD journal
   and data partitions need to be in a GPT partition table.
  </para>

  <para>
   You need to set the correct partition types to the OSD partitions so that
   <systemitem>udev</systemitem> recognizes them correctly and sets their
   ownership to <literal>ceph:ceph</literal>.
  </para>

  <para>
   For example, to set the partition type for the journal partition
   <filename>/dev/vdb1</filename> and data partition
   <filename>/dev/vdb2</filename>, run the following:
  </para>

<screen>sudo sgdisk --typecode=1:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/vdb
sudo sgdisk --typecode=2:4fbd7e29-9d25-41b8-afd0-062c0ceff05d /dev/vdb</screen>

  <tip>
   <para>
    The Ceph partition table types are listed in
    <filename>/usr/lib/udev/rules.d/95-ceph-osd.rules</filename>:
   </para>
<screen>cat /usr/lib/udev/rules.d/95-ceph-osd.rules
# OSD_UUID
ACTION=="add", SUBSYSTEM=="block", \
  ENV{DEVTYPE}=="partition", \
  ENV{ID_PART_ENTRY_TYPE}=="4fbd7e29-9d25-41b8-afd0-062c0ceff05d", \
  OWNER:="ceph", GROUP:="ceph", MODE:="660", \
  RUN+="/usr/sbin/ceph-disk --log-stdout -v trigger /dev/$name"
ACTION=="change", SUBSYSTEM=="block", \
  ENV{ID_PART_ENTRY_TYPE}=="4fbd7e29-9d25-41b8-afd0-062c0ceff05d", \
  OWNER="ceph", GROUP="ceph", MODE="660"

# JOURNAL_UUID
ACTION=="add", SUBSYSTEM=="block", \
  ENV{DEVTYPE}=="partition", \
  ENV{ID_PART_ENTRY_TYPE}=="45b0969e-9b03-4f30-b4c6-b4b80ceff106", \
  OWNER:="ceph", GROUP:="ceph", MODE:="660", \
  RUN+="/usr/sbin/ceph-disk --log-stdout -v trigger /dev/$name"
ACTION=="change", SUBSYSTEM=="block", \
  ENV{ID_PART_ENTRY_TYPE}=="45b0969e-9b03-4f30-b4c6-b4b80ceff106", \
  OWNER="ceph", GROUP="ceph", MODE="660"
[...]</screen>
  </tip>
 </sect1>
</chapter>
  <chapter xml:base="admin_bestpractices_recovery.xml" version="5.0" xml:id="storage.bp.recover">
 <title>Recovery</title>
 <para/>
 <sect1 xml:id="storage.bp.recover.toomanypgs">
  <title>'Too Many PGs per OSD' Status Message</title>

  <para>
   If you receive a <literal>Too Many PGs per OSD</literal> message after
   running <command>ceph status</command>, it means that the
   <option>mon_pg_warn_max_per_osd</option> value (300 by default) was
   exceeded. This value is compared to the number of PGs per OSD ratio. This
   means that the cluster setup is not optimal.
  </para>

  <para>
   As the number of PGs cannot be reduced after the pool is created, the only
   solution is to add OSDs to the cluster so that the ratio of PGs per OSD
   becomes lower.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.recover.stalecalamari">
  <title>Calamari Has a Stale Cluster</title>

  <para>
   The Calamari back-end supports operating multiple clusters, while its
   front-end does not yet. This means that if you point Calamari at one
   cluster, then destroy that cluster and create a new one, and then point the
   same Calamari instance at the new cluster, it will still remember the old
   cluster and possibly/probably try to display the old cluster state by
   default.
  </para>

  <para>
   To make Calamari 'forget' the old cluster, run:
  </para>

<screen>sudo systemctl stop cthulhu.service
sudo calamari-ctl clear --yes-i-am-sure
sudo calamari-ctl initialize</screen>

  <para>
   This will make Calamari forget all the old clusters it knows about. It will,
   however, not clear out the salt minion keys from the master. This is fine if
   you are reusing the same nodes for the new cluster.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.recover.stuckinactive">
  <title>'<emphasis>nn</emphasis> pg stuck inactive' Status Message</title>

  <para>
   If you receive a <literal>stuck inactive</literal> status message after
   running <command>ceph status</command>, it means that Ceph does not know
   where to replicate the stored data to fulfill the replication rules. It can
   happen shortly after the initial Ceph setup and fix itself automatically.
   In other cases, this may require a manual interaction, such as bringing up a
   broken OSD, or adding a new OSD to the cluster. In very rare cases, reducing
   the replication level may help.
  </para>

  <para>
   If the placement groups are stuck perpetually, you need to check the output
   of <command>ceph osd tree</command>. The output should look tree-structured,
   similar to the example in <xref linkend="storage.bp.recover.osddown" role="internalbook"/>.
  </para>

  <para>
   If the output of <command>ceph osd tree</command> is rather flat as in the
   following example
  </para>

<screen>ceph osd tree
ID WEIGHT TYPE NAME    UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1      0 root default
 0      0 osd.0             up  1.00000          1.00000
 1      0 osd.1             up  1.00000          1.00000
 2      0 osd.2             up  1.00000          1.00000</screen>

  <para>
   you should check that the related CRUSH map has a tree structure. If it is
   also flat, or with no hosts as in the above example, it may mean that host
   name resolution is not working correctly across the cluster.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.recover.osdweight">
  <title>OSD Weight is 0</title>

  <para>
   When OSD starts, it is assigned a weight. The higher the weight, the bigger
   the chance that the cluster writes data to the OSD. The weight is either
   specified in a cluster CRUSH Map, or calculated by the OSDs' start-up
   script.
  </para>

  <para>
   In some cases, the calculated value for OSDs' weight may be rounded down to
   zero. It means that the OSD is not scheduled to store data, and no data is
   written to it. The reason is usually that the disk is too small (smaller
   than 15GB) and should be replaced with a bigger one.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.recover.osddown">
  <title>OSD is Down</title>

  <para>
   OSD daemon is either running, or stopped/down. There are 3 general reasons
   why an OSD is down:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Hard disk failure.
    </para>
   </listitem>
   <listitem>
    <para>
     The OSD crashed.
    </para>
   </listitem>
   <listitem>
    <para>
     The server crashed.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   You can see the detailed status of OSDs by running
  </para>

<screen>ceph osd tree
# id  weight  type name up/down reweight
 -1    0.02998  root default
 -2    0.009995   host doc-ceph1
 0     0.009995      osd.0 up  1
 -3    0.009995   host doc-ceph2
 1     0.009995      osd.1 up  1
 -4    0.009995   host doc-ceph3
 2     0.009995      osd.2 down  1</screen>

  <para>
   The example listing shows that the <literal>osd.2</literal> is down. Then
   you may check if the disk where the OSD is located is mounted:
  </para>

<screen>lsblk -f
 [...]
 vdb
 ├─vdb1               /var/lib/ceph/osd/ceph-2
 └─vdb2</screen>

  <para>
   You can track the reason why the OSD is down by inspecting its log file
   <filename>/var/log/ceph/ceph-osd.2.log</filename>. After you find and fix
   the reason why the OSD is not running, start it with
  </para>

<screen>sudo systemctl start ceph-osd@2.service</screen>

  <para>
   Do not forget to replace <literal>2</literal> with the actual number of your
   stopped OSD.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.recover.clockskew">
  <title>Fixing Clock Skew Warnings</title>

  <para>
   The time information in all cluster nodes must be synchronized. If a node's
   time is not fully synchronized, you may get clock skew warnings when
   checking the state of the cluster.
  </para>

  <para>
   Time synchronization is managed with NTP (see
   <link xlink:href="http://en.wikipedia.org/wiki/Network_Time_Protocol"/>).
   Set each node to synchronize its time with one or more NTP servers,
   preferably to the same group of NTP servers. If the time skew still occurs
   on a node, follow these steps to fix it:
  </para>

<screen>systemctl stop ntpd.service
systemctl stop ceph-mon.target
systemctl start ntpd.service
systemctl start ceph-mon.target</screen>

  <para>
   You can then query the NTP peers and check the time offset with
   <command>sudo ntpq -p</command>.
  </para>

  <para>
   The Ceph monitors need to have their clocks synchronized to within 0.05
   seconds of each other. In a typical
   <systemitem class="daemon">ntpd</systemitem> configuration with remote NTP
   servers, it may be impossible for
   <systemitem class="daemon">ntpd</systemitem> to reliably maintain this
   degree of accuracy. In such cases, the Ceph developers recommend running
   an NTP server in the local network.
  </para>
 </sect1>
</chapter>
  <chapter xml:base="admin_bestpractices_accountancy.xml" version="5.0" xml:id="storage.bp.account">
 <title>Accountancy</title>
 <para/>
 <sect1 xml:id="storage.bp.account.s3add">
  <title>Adding S3 Users</title>

  <para>
   S3 (Simple Storage Service) is an online file storage Web service, offered
   by Amazon. You can use the S3 interface to interact with the Ceph <phrase>RADOS Gateway</phrase>,
   besides the Swift interface. You need to create a user to interact with
   the gateway.
  </para>

  <para>
   To create a user for the S3 interface, follow these steps:
  </para>

<screen>sudo radosgw-admin user create --uid=<replaceable>username</replaceable> \
 --display-name="<replaceable>display-name</replaceable>" --email=<replaceable>email</replaceable></screen>

  <para>
   For example:
  </para>

<screen>sudo radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</screen>

  <para>
   The command also creates the user's access and secret key. Check its output
   for <literal>access_key</literal> and <literal>secret_key</literal> keywords
   and their values:
  </para>

<screen>[...]
 "keys": [
       { "user": "example_user",
         "access_key": "11BS02LGFB6AL6H1ADMW",
         "secret_key": "vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY"}],
 [...]</screen>
 </sect1>
 <sect1 xml:id="storage.bp.account.s3rm">
  <title>Removing S3 Users</title>

  <para>
   To remove a user previously created to interact with the S3 interface, use
   the following command:
  </para>

<screen>sudo radosgw-admin user rm --uid=example_user</screen>

  <para>
   For more information on the command's options, see
   <xref linkend="storage.bp.account.swiftrm" role="internalbook"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.account.s3quota">
  <title>User Quota Management</title>

  <para>
   The Ceph <phrase>RADOS Gateway</phrase> enables you to set quotas on users and buckets owned by
   users. Quotas include the maximum number of objects in a bucket and the
   maximum storage size in megabytes.
  </para>

  <para>
   Before you enable a user quota, you first need to set its parameters:
  </para>

<screen>radosgw-admin quota set --quota-scope=user --uid=<replaceable>example_user</replaceable> \
 --max-objects=1024 --max-size=1024</screen>

  <variablelist>
   <varlistentry>
    <term><option>--max-objects</option>
    </term>
    <listitem>
     <para>
      Specifies the maximum number of objects. A negative value disables the
      check.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>--max-size</option>
    </term>
    <listitem>
     <para>
      Specifies the maximum number of bytes. A negative value disables the
      check.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>--quota-scope</option>
    </term>
    <listitem>
     <para>
      Sets the scope for the quota. The options are <literal>bucket</literal>
      and <literal>user</literal>. Bucket quotas apply to buckets a user owns.
      User quotas apply to a user.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Once you set a user quota, you may enable it:
  </para>

<screen>radosgw-admin quota enable --quota-scope=user --uid=<replaceable>example_user</replaceable></screen>

  <para>
   To disable a quota:
  </para>

<screen>radosgw-admin quota disable --quota-scope=user --uid=<replaceable>example_user</replaceable></screen>

  <para>
   To list quota settings:
  </para>

<screen>radosgw-admin user info --uid=<replaceable>example_user</replaceable></screen>

  <para>
   To update quota statistics:
  </para>

<screen>radosgw-admin user stats --uid=<replaceable>example_user</replaceable> --sync-stats</screen>
 </sect1>
 <sect1 xml:id="storage.bp.account.swiftadd">
  <title>Adding Swift Users</title>

  <para>
   Swift is a standard for stored data access compatible with <phrase>OpenStack</phrase>. It is
   used to interact with the Ceph <phrase>RADOS Gateway</phrase>. You need to create a Swift user,
   access key and secret to enable end users to interact with the gateway.
   There are two types of users: a <emphasis>user</emphasis> and
   <emphasis>subuser</emphasis>. While <emphasis>users</emphasis> are used when
   interacting with the S3 interface, <emphasis>subusers</emphasis> are users
   of the Swift interface. Each subuser is associated to a user.
  </para>

  <procedure>
   <step>
    <para>
     To create a Swift user—which is a <emphasis>subuser</emphasis> in
     our terminology—you need to create the associated
     <emphasis>user</emphasis> first.
    </para>
<screen>sudo radosgw-admin user create --uid=<replaceable>username</replaceable> \
 --display-name="<replaceable>display-name</replaceable>" --email=<replaceable>email</replaceable></screen>
    <para>
     For example:
    </para>
<screen>sudo radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</screen>
   </step>
   <step>
    <para>
     To create a subuser (Swift interface) for the user, you must specify the
     user ID (--uid=<replaceable>username</replaceable>), a subuser ID, and the
     access level for the subuser.
    </para>
<screen>radosgw-admin subuser create --uid=<replaceable>uid</replaceable> \
 --subuser=<replaceable>uid</replaceable> \
 --access=[ <replaceable>read | write | readwrite | full</replaceable> ]</screen>
    <para>
     For example:
    </para>
<screen>radosgw-admin subuser create --uid=example_user \
 --subuser=example_user:swift --access=full</screen>
   </step>
   <step>
    <para>
     Generate a secret key for the user.
    </para>
<screen>sudo radosgw-admin key create \
   --gen-secret \
   --subuser=example_user:swift \
   --key-type=swift</screen>
   </step>
   <step>
    <para>
     Both commands will output JSON-formatted data showing the user state.
     Notice the following lines, and remember the <literal>secret_key</literal>
     value:
    </para>
<screen>"swift_keys": [
   { "user": "example_user:swift",
     "secret_key": "r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h"}],</screen>
   </step>
  </procedure>

  <para>
   For more information on using Swift client, see
   <xref linkend="ceph.rgw.access" role="internalbook"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.account.swiftrm">
  <title>Removing Swift Users</title>

  <para>
   When you remove a user, the user and subuser are removed from the system.
   However, you may remove only the subuser if you want. To remove a user (and
   subuser), specify <option>user rm</option> and the user ID.
  </para>

<screen>radosgw-admin user rm --uid=example_user</screen>

  <para>
   To remove the subuser only, specify <option>subuser rm</option> and the
   subuser ID.
  </para>

<screen>radosgw-admin subuser rm --uid=example_user:swift</screen>

  <para>
   You can make use of the following options:
  </para>

  <variablelist>
   <varlistentry>
    <term>--purge-data</term>
    <listitem>
     <para>
      Purges all data associated to the user ID.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>--purge-keys</term>
    <listitem>
     <para>
      Purges all keys associated to the user ID.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <title>Removing a Subuser</title>
   <para>
    When you remove a subuser, you are removing access to the Swift interface.
    The user will remain in the system. To remove the subuser, specify
    <option>subuser rm</option> and the subuser ID.
   </para>
<screen>sudo radosgw-admin subuser rm --uid=example_user:swift</screen>
   <para>
    You can make use of the following option:
   </para>
   <variablelist>
    <varlistentry>
     <term>--purge-keys</term>
     <listitem>
      <para>
       Purges all keys associated to the user ID.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </tip>
 </sect1>
 <sect1 xml:id="storage.bp.account.user_pwd">
  <title>Changing S3 and Swift User Access and Secret Keys</title>

  <para>
   The <literal>access_key</literal> and <literal>secret_key</literal>
   parameters identify the <phrase>RADOS Gateway</phrase> user when accessing the gateway. Changing the
   existing user keys is the same as creating new ones, as the old keys get
   overwritten.
  </para>

  <para>
   For S3 users, run the following:
  </para>

<screen>radosgw-admin key create --uid=<replaceable>example_user</replaceable> --key-type=s3 --gen-access-key --gen-secret</screen>

  <para>
   For Swift users, run the following:
  </para>

<screen>radosgw-admin key create --subuser=<replaceable>example_user</replaceable>:swift --key-type=swift --gen-secret</screen>

  <variablelist>
   <varlistentry>
    <term><option>--key-type=<replaceable>type</replaceable></option>
    </term>
    <listitem>
     <para>
      Specifies the type of key. Either <literal>swift</literal> or
      <literal>s3</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>--gen-access-key</option>
    </term>
    <listitem>
     <para>
      Generates a random access key (for S3 user by default).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>--gen-secret</option>
    </term>
    <listitem>
     <para>
      Generates a random secret key.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>--secret=<replaceable>key</replaceable></option>
    </term>
    <listitem>
     <para>
      Specifies a secret key, for example manually generated.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
</chapter>
  <chapter xml:base="admin_bestpractices_tuneups.xml" version="5.0" xml:id="storage.bp.tuneups">
 <title>Tune-ups</title>
 <para/>
 <sect1 xml:id="storage.bp.tuneups.pg_num">
  <title>How Does the Number of Placement Groups Affect the Cluster Performance?</title>

  <para>
   Placement groups (PGs) are internal data structures for storing data in a
   pool across OSDs. The way Ceph stores data into PGs is defined in a
   CRUSH Map, and you can override the default by editing it. When creating a
   new pool, you need to specify the initial number of PGs for the pool.
  </para>

  <para>
   When your cluster is becoming 70% to 80% full, it is time to add more OSDs
   to it. When you increase the number of OSDs, you may consider increasing the
   number of PGs as well.
  </para>

  <warning>
   <para>
    Changing the number of PGs causes a lot of data transfer within the
    cluster.
   </para>
  </warning>

  <para>
   To calculate the optimal value for your newly-resized cluster is a complex
   task.
  </para>

  <para>
   A high number of PGs creates small chunks of data. This speeds up recovery
   after an OSD failure, but puts a lot of load on the monitor nodes as they
   are responsible for calculating the data location.
  </para>

  <para>
   On the other hand, a low number of PGs takes more time and data transfer to
   recover from an OSD failure, but does not impose that much load on monitor
   nodes as they need to calculate locations for less (but larger) data chunks.
  </para>

  <para>
   Find more information on the optimal number of PGs for your cluster using
   the <link xlink:href="http://ceph.com/pgcalc/">online calculator</link>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.tuneups.mix_ssd">
  <title>Can I Use SSDs and Hard Disks on the Same Cluster?</title>

  <para>
   Solid-state drives (SSD) are generally faster than hard disks. If you mix
   the two types of disks for the same write operation, the data writing to the
   SSD disk will be slowed down by the hard disk performance. Thus, you should
   <emphasis>never mix SSDs and hard disks</emphasis> for data writing
   following <emphasis>the same rule</emphasis> (see
   <xref linkend="datamgm.rules" role="internalbook"/> for more information on rules for storing
   data).
  </para>

  <para>
   There are generally 2 cases where using SSD and hard disk on the same
   cluster makes sense:
  </para>

  <orderedlist spacing="normal">
   <listitem>
    <para>
     Use each disk type for writing data following different rules. Then you
     need to have a separate rule for the SSD disk, and another rule for the
     hard disk.
    </para>
   </listitem>
   <listitem>
    <para>
     Use each disk type for a specific purpose. For example the SSD disk for
     journal, and the hard disk for storing data.
    </para>
   </listitem>
  </orderedlist>
 </sect1>
 <sect1 xml:id="storage.bp.tuneups.ssd_tradeoffs">
  <title>What are the Trade-offs of Using a Journal on SSD?</title>

  <para>
   Using SSDs for OSD journal(s) is better for performance as the journal is
   usually the bottleneck of hard disk-only OSDs. SSDs are often used to share
   journals of several OSDs.
  </para>

  <para>
   Following is a list of potential disadvantages of using SSDs for OSD
   journal:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     SSD disks are more expensive than hard disks. But as one OSD journal
     requires up to 6GB of disk space only, the price may not be so crucial.
    </para>
   </listitem>
   <listitem>
    <para>
     SSD disk consumes storage slots which can be otherwise used by a large
     hard disk to extend the cluster capacity.
    </para>
   </listitem>
   <listitem>
    <para>
     SSD disks have reduced write cycles compared to hard disks, but modern
     technologies are beginning to eliminate the problem.
    </para>
   </listitem>
   <listitem>
    <para>
     If you share more journals on the same SSD disk, you risk losing all the
     related OSDs after the SSD disk fails. This will require a lot of data to
     be moved to rebalance the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Hotplugging disks becomes more complex as the data mapping is not 1:1 the
     failed OSD and the journal disk.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
  <chapter xml:base="admin_bestpractices_integration.xml" version="5.0" xml:id="storage.bp.integration">
 <title>Integration</title>
 <sect1 xml:id="storage.bp.integration.kvm">
  <title>Storing KVM Disks in Ceph Cluster</title>

  <para>
   You can create a disk image for KVM-driven virtual machine, store it in a
   Ceph pool, optionally convert the content of an existing image to it, and
   then run the virtual machine with <command>qemu-kvm</command> making use of
   the disk image stored in the cluster. For more detailed information, see
   <xref linkend="cha.ceph.kvm" role="internalbook"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.integration.libvirt">
  <title>Storing <systemitem class="library">libvirt</systemitem> Disks in Ceph Cluster</title>

  <para>
   Similar to KVM (see <xref linkend="storage.bp.integration.kvm" role="internalbook"/>), you can
   use Ceph to store virtual machines driven by <systemitem class="library">libvirt</systemitem>. The advantage is
   that you can run any <systemitem class="library">libvirt</systemitem>-supported virtualization solution, such as
   KVM, Xen, or LXC. For more information, see
   <xref linkend="cha.ceph.libvirt" role="internalbook"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.integration.xen">
  <title>Storing Xen Disks in Ceph Cluster</title>

  <para>
   One way to use Ceph for storing Xen disks is to make use of <systemitem class="library">libvirt</systemitem> as
   described in <xref linkend="cha.ceph.libvirt" role="internalbook"/>.
  </para>

  <para>
   Another option is to make Xen talk to the <systemitem>rbd</systemitem>
   block device driver directly:
  </para>

  <procedure>
   <step>
    <para>
     If you have no disk image prepared for Xen, create a new one:
    </para>
<screen>rbd create myimage --size 8000 --pool mypool</screen>
   </step>
   <step>
    <para>
     List images in the pool <literal>mypool</literal> and check if your new
     image is there:
    </para>
<screen>rbd list mypool</screen>
   </step>
   <step>
    <para>
     Create a new block device by mapping the <literal>myimage</literal> image
     to the <systemitem>rbd</systemitem> kernel module:
    </para>
<screen>sudo rbd map --pool mypool myimage</screen>
    <tip>
     <title>User Name and Authentication</title>
     <para>
      To specify a user name, use <option>--id
      <replaceable>user-name</replaceable></option>. Moreover, if you use
      <systemitem>cephx</systemitem> authentication, you must also specify a
      secret. It may come from a keyring or a file containing the secret:
     </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyring
 /path/to/keyring</screen>
     <para>
      or
     </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
    </tip>
   </step>
   <step>
    <para>
     List all mapped devices:
    </para>
<screen>rbd showmapped
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</screen>
   </step>
   <step>
    <para>
     Now you can configure Xen to use this device as a disk for running a
     virtual machine. You can for example add the following line to the
     <command>xl</command>-style domain configuration file:
    </para>
<screen>disk = [ '/dev/rbd0,,sda', '/dev/cdrom,,sdc,cdrom' ]</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="storage.bp.integration.mount_rbd">
  <title>Mounting and Unmounting an RBD Image</title>

  <para>
   Images stored inside a Ceph cluster pool can be mapped to a block device.
   You can then format such device, mount it to be able to exchange files, and
   unmount it when done.
  </para>

  <procedure>
   <step>
    <para>
     Make sure your Ceph cluster includes a pool with the disk image you want
     to mount. Assume the pool is called <literal>mypool</literal> and the
     image is <literal>myimage</literal>.
    </para>
<screen>rbd list mypool</screen>
   </step>
   <step>
    <para>
     Map the image to a new block device.
    </para>
<screen>sudo rbd map --pool mypool myimage</screen>
    <tip>
     <title>User Name and Authentication</title>
     <para>
      To specify a user name, use <option>--id
      <replaceable>user-name</replaceable></option>. Moreover, if you use
      <systemitem>cephx</systemitem> authentication, you must also specify a
      secret. It may come from a keyring or a file containing the secret:
     </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyring
 /path/to/keyring</screen>
     <para>
      or
     </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
    </tip>
   </step>
   <step>
    <para>
     List all mapped devices:
    </para>
<screen>rbd showmapped
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</screen>
    <para>
     The device we want to work on is <filename>/dev/rbd0</filename>.
    </para>
   </step>
   <step>
    <para>
     Make an XFS file system on the <filename>/dev/rbd0</filename> device.
    </para>
<screen>sudo mkfs.xfs /dev/rbd0
 log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
 log stripe unit adjusted to 32KiB
 meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
          =                       sectsz=512   attr=2, projid32bit=1
          =                       crc=0        finobt=0
 data     =                       bsize=4096   blocks=2097152, imaxpct=25
          =                       sunit=1024   swidth=1024 blks
 naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
 log      =internal log           bsize=4096   blocks=2560, version=2
          =                       sectsz=512   sunit=8 blks, lazy-count=1
 realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
   </step>
   <step>
    <para>
     Mount the device and check it is correctly mounted. Replace
     <filename>/mnt</filename> with your mount point.
    </para>
<screen>sudo mount /dev/rbd0 /mnt
 mount | grep rbd0
 /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
    <para>
     Now you can move data from/to the device as if it was a local directory.
    </para>
    <tip>
     <title>Increasing the Size of RBD Device</title>
     <para>
      If you find that the size of the RBD device is no longer enough, you can
      easily increase it.
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        Increase the size of the RBD image, for example up to 10GB.
       </para>
<screen>rbd resize --size 10000  mypool/myimage
 Resizing image: 100% complete...done.</screen>
      </listitem>
      <listitem>
       <para>
        Grow the file system to fill up the new size of the device.
       </para>
<screen>sudo xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
      </listitem>
     </orderedlist>
    </tip>
   </step>
   <step>
    <para>
     After you finish accessing the device, you can unmount it.
    </para>
<screen>sudo unmount /mnt</screen>
   </step>
  </procedure>
 </sect1>
</chapter>
  <chapter xml:base="admin_bestpractices_maint.xml" version="5.0" xml:id="storage.bp.cluster_mntc">
 <title>Cluster Maintenance and Troubleshooting</title>
 <para/>
 
 <sect1 xml:id="storage.bp.cluster_mntc.calamari_addpool">
  <title>Creating and Deleting Pools from Calamari</title>

  <para>
   Apart from using the command line to create or delete pools (see
   <xref linkend="storage.bp.cluster_mntc.add_pool" role="internalbook"/> and
   <xref linkend="storage.bp.cluster_mntc.del_pool" role="internalbook"/>), you can do the same
   from within Calamari in a more comfortable user interface.
  </para>

  <para>
   To create a new pool using Calamari, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Log in to a running instance of Calamari.
    </para>
   </step>
   <step>
    <para>
     Go to
     <menuchoice><guimenu>Manage</guimenu><guimenu>Pools</guimenu></menuchoice>.
     You can see a list of the cluster's existing pools.
    </para>
   </step>
   <step>
    <para>
     Click <inlinemediaobject>
     <imageobject>
      <imagedata fileref="calamari_adpool_plus.png" width="25px"/>
     </imageobject>
     </inlinemediaobject> in the right top.
    </para>
   </step>
   <step>
    <para>
     Enter a name for the new pool, and either change the number of replicas,
     number of placement groups, and the CRUSH ruleset, or leave them at
     default values.
    </para>
   </step>
   <step>
    <para>
     Click <inlinemediaobject>
     <imageobject>
      <imagedata fileref="calamari_adpool_plus.png" width="25px"/>
     </imageobject>
     </inlinemediaobject> to confirm, then <guimenu>Cancel</guimenu> the
     warning dialog.
    </para>
   </step>
   <step>
    <para>
     Now you can see the new pool in the list of all existing pools. You can
     verify the existence of the new pool on the command line with
    </para>
<screen>ceph osd lspools</screen>
   </step>
  </procedure>

  <para>
   To delete an existing pool using Calamari, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Log in to a running instance of Calamari.
    </para>
   </step>
   <step>
    <para>
     Go to
     <menuchoice><guimenu>Manage</guimenu><guimenu>Pools</guimenu></menuchoice>.
     You can see a list of the cluster's existing pools.
    </para>
   </step>
   <step>
    <para>
     From the list of pools, choose the one to delete and click the related
     <inlinemediaobject>
     <imageobject>
      <imagedata fileref="calamari_rmpool_thrash.png" width="25px"/>
     </imageobject>
     </inlinemediaobject>
    </para>
   </step>
   <step>
    <para>
     Confirm the deletion and <guimenu>Cancel</guimenu> the warning dialog.
    </para>
   </step>
   <step>
    <para>
     You can verify the deletion of the pool on the command line with
    </para>
<screen>ceph osd lspools</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.mng_keyrings">
  <title>Managing Keyring Files</title>

  <para>
   When Ceph runs with authentication and authorization enabled (enabled by
   default), you must specify a user name and a keyring containing the secret
   key of the specified user. If you do not specify a user name, Ceph will use
   <literal>client.admin</literal> as the default user name. If you do not
   specify a keyring, Ceph will look for a keyring via the
   <option>keyring</option> setting in the Ceph configuration. For example,
   if you execute the <command>ceph health</command> command without specifying
   a user or keyring:
  </para>

<screen>ceph health</screen>

  <para>
   Ceph interprets the command like this:
  </para>

<screen>ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health</screen>

  <para>
   <command>ceph-authtool</command> is a utility to create, view, and modify a
   Ceph keyring file. A keyring file stores one or more Ceph authentication
   keys and possibly an associated capability specification. Each key is
   associated with an entity name, of the form {client,mon,mds,osd}.name.
  </para>

  <para>
   To create a new <filename>keyring</filename> file in the current directory
   containing a key for <literal>client.example1</literal>:
  </para>

<screen>ceph-authtool -C -n client.example1 --gen-key keyring</screen>

  <para>
   To add a new key for <literal>client.example2</literal>, omit the
   <option>-C</option> option:
  </para>

<screen>ceph-authtool -n client.example2 --gen-key keyring</screen>

  <para>
   The <filename>keyring</filename> now has two entries:
  </para>

<screen>ceph-authtool -l keyring
 [client.example1]
     key = AQCQ04NV8NE3JBAAHurrwc2BTVkMGybL1DYtng==
 [client.example2]
     key = AQBv2INVWMqFIBAAf/4/H3zxzAsPBTH4jsN80w==</screen>

  <para>
   For more information on <filename>ceph-authtool</filename>, see its manual
   page <filename>man 8 ceph-authtool</filename>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.create_client_keys">
  <title>Creating Client Keys</title>

  <para>
   User management functionality provides Ceph cluster administrators with
   the ability to create, update and delete users directly in the cluster
   environment.
  </para>

  <tip>
   <para>
    When you create or delete users in the Ceph cluster, you may need to
    distribute keys to clients so that they can be added to keyrings.
   </para>
  </tip>

  <para>
   Adding a user creates a user name (TYPE.ID), a secret key and possibly
   capabilities included in the command you use to create the user. A user’s
   key enables the user to authenticate with the cluster. The user’s
   capabilities authorize the user to read, write, or execute on monitors,
   OSDs, or metadata servers.
  </para>

  <para>
   Authentication key creation usually follows cluster user creation. There are
   several ways to add a user. The most convenient seems to be using
  </para>

<screen>ceph auth get-or-create</screen>

  <para>
   It returns a keyfile format with the user name [in brackets] and the key. If
   the user already exists, this command simply returns the user name and key
   in the keyfile format. You may use the<option>-o
   <replaceable>filename</replaceable></option> option to save the output to a
   file.
  </para>

<screen>ceph auth get-or-create client.example1
 [client.example1]
    key = AQDs+odVODCGGxAAvmSnsNx3XYHJ7Ri6sZFfhw==</screen>

  <para>
   You can verify that the client key was added to the cluster keyring:
  </para>

<screen>ceph auth list
    [...]
 client.example1
    key: AQDs+odVODCGGxAAvmSnsNx3XYHJ7Ri6sZFfhw==</screen>

  <para>
   When creating client users, you may create a user with no capabilities. A
   user with no capabilities is useless beyond mere authentication, because the
   client cannot retrieve the cluster map from the monitor. However, you can
   create a user with no capabilities if you want to defer adding capabilities
   later using the <command>ceph auth caps</command> command.
  </para>

  <tip>
   <para>
    After you add a key to the cluster keyring, go to the relevant client(s)
    and copy the keyring from the cluster host to the client(s).
   </para>
  </tip>

  <para>
   Find more details in the related upstream documentation, see
   <link xlink:href="http://ceph.com/docs/master/rados/operations/user-management/">User
   Management</link>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.revoke_client_keys">
  <title>Revoking Client Keys</title>

  <para>
   If you need to remove an already generated client key from the keyring file,
   use the <command>ceph auth del</command> command. To remove the key for user
   <literal>client.example1</literal> that we added in
   <xref linkend="storage.bp.cluster_mntc.create_client_keys" role="internalbook"/>:
  </para>

<screen>ceph auth del client.example1</screen>

  <para>
   and check the deletion with <command>ceph auth list</command>.
  </para>

  <tip>
   <para>
    After you add a key to the cluster keyring, go to the relevant client(s)
    and copy the keyring from the cluster host to the client(s).
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.unbalanced">
  <title>Checking for Unbalanced Data Writing</title>

  <para>
   When data is written to OSDs evenly, the cluster is considered balanced.
   Each OSD within a cluster is assigned its <emphasis>weight</emphasis>. The
   weight is a relative number and tells Ceph how much of the data should be
   written to the related OSD. The higher the weight, the more data will be
   written. If an OSD has zero weight, no data will be written to it. If the
   weight of an OSD is relatively high compared to other OSDs, a large portion
   of the data will be written there, which makes the cluster unbalanced.
  </para>

  <para>
   Unbalanced clusters have poor performance, and in the case that an OSD with
   a high weight suddenly crashes, a lot of data needs to be moved to other
   OSDs, which slows down the cluster as well.
  </para>

  <para>
   To avoid this, you should regularly check OSDs for the amount of data
   writing. If the amount is between 30% and 50% of the capacity of a group of
   OSDs specified by a given rule set, you need to reweight the OSDs. Check for
   individual disks and find out which of them fill up faster than the others
   (or are generally slower), and lower their weight. The same is valid for
   OSDs where not enough data is written—you can increase their weight to
   have Ceph write more data to them. In the following example, you will find
   out the weight of an OSD with ID 13, and reweight it from 3 to 3.05:
  </para>

<screen>$ ceph osd tree | grep osd.13
 13  3                   osd.13  up  1

 $ ceph osd crush reweight osd.13 3.05
 reweighted item id 13 name 'osd.13' to 3.05 in crush map

 $ ceph osd tree | grep osd.13
 13  3.05                osd.13  up  1</screen>

  <para/>

  <tip>
   <title>OSD Reweight by Utilization</title>
   <para>
    The <command>ceph osd reweight-by-utilization</command>
    <replaceable>threshold</replaceable> command automates the process of
    reducing the weight of OSDs which are heavily overused. By default it will
    adjust the weights downward on OSDs which reached 120% of the average
    usage, but if you include threshold it will use that percentage instead.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="Cluster_Time_Setting">
  	<title>Time Synchronization of Nodes</title>
  	<para>
  	Ceph requires precise time synchronization between particular nodes. You should set up a node with your own NTP server. Even though you can point all ntpd instances to a remote public time server, we do not recommend it with Ceph.  With such a configuration, each node in the cluster has its own NTP daemon that communicate continually over the Internet with a set of three or four time servers, all of which are quite some hops away. This solution introduces a large degree of latency variability that makes it difficult or impossible to keep the clock drift under 0.05 seconds (which is what the Ceph monitors require).
  	</para>
  	<para>
  	Thus use a single machine as the NTP server for the whole cluster. Your NTP server ntpd instance may then point to the remore (public) NTP server or it can have its own time source. The ntpd instances on all nodes are then pointed to this local server. Such a solution has several advantages like—eliminating unnecessary network traffic and clock skews, decreasing load on the public NTP servers. For details how to set up the NTP server refer to <link xlink:href="https://www.suse.com/documentation/sled11/book_sle_admin/data/cha_netz_xntp.html">SUSE Linux Enterprise Server Administration Guide</link>.
  	</para>
  	<para>
  	Then to change the time on your cluster, do the following:
  	</para>
  	<important>
  		<title>Setting Time</title>
  		<para>
  		You may face a situation when you need to set the time back, e.g. if the time changes from the summer to the standard time. We do not recommend to move the time backward for a longer period than the cluster is down. Moving the time forward does not cause any trouble.
  		</para>
  	</important>
  	<procedure>
  		<title>Time Synchronization on the Cluster</title>
  		<step>
  			<para>
  			Stop all clients accessing the Ceph cluster, especially those using iSCSI.
  			</para>
  		</step>
  		<step>
  			<para>
  			Shut down your Ceph cluster. On each node run:
  			</para>
  			<screen>rcceph stop</screen>
  			<note>
  				<para>
  				If you use Ceph and SUSE OpenStack Cloud, stop also the SUSE OpenStack Cloud. 
  				</para>
  			</note>
  		</step>
  		<step>
  			<para>
  			Verify that your NTP server is set up correctly—all ntpd daemons det their time from a source or sources in the local network.
  			</para>
  		</step>
  		<step>
  			<para>
  			Set the correct time on your NTP server.
  			</para>
  		</step>
  		<step>
  			<para>
  			Verify that NTP is running and working properly, on all nodes run:
  			</para>
  			<screen>status ntpd.service</screen>
  			<para>or</para>
  			<screen>ntpq -p</screen>
  		</step>
  		<step>
  			<para>
  			Start all monitoring nodes and verify that there is no clock skew:
  			</para>
  			<screen>systemctl start <replaceable>target</replaceable></screen>
  		</step>
  		<step>
  			<para>
  			Start all OSD nodes.
  			</para>
  		</step>
  		<step>
  			<para>
  			Start other Ceph services.
  			</para>
  		</step>
  		<step>
  			<para>
  			Start the SUSE OpenStack Cloud if you have it.
  			</para>
  		</step>
  	</procedure>
  	
  </sect1>

 
 <sect1 xml:id="storage.bp.cluster_mntc.sw_upg">
  <title>Upgrading Software</title>

  <para>
   Both SUSE Linux Enterprise Server and SUSE Enterprise Storage products are provided with regular package updates.
   To apply new updates to the whole cluster, you need to run
  </para>

<screen>sudo zypper dup</screen>

  <para>
   on all cluster nodes. Remember to upgrade all the monitor nodes first, and
   then all the OSD nodes one by one.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.add_pgnum">
  <title>Increasing the Number of Placement Groups</title>

  <para>
   When creating a new pool, you specify the number of placement groups for the
   pool (see <xref linkend="ceph.pools.operate.add_pool" role="internalbook"/>). After adding more
   OSDs to the cluster, you usually need to increase the number of placement
   groups as well for performance and data durability reasons. For each
   placement group, OSD and monitor nodes need memory, network and CPU at all
   times and even more during recovery. From which follows that minimizing the
   number of placement groups saves significant amounts of resources.
  </para>

  <warning>
   <title>Too High Value of <option>pg_num</option></title>
   <para>
    When changing the <option>pg_num</option> value for a pool, it may happen
    that the new number of placement groups exceeds the allowed limit. For
    example
   </para>
<screen>ceph osd pool set rbd pg_num 4096
 Error E2BIG: specified pg_num 3500 is too large (creating 4096 new PGs \
 on ~64 OSDs exceeds per-OSD max of 32)</screen>
   <para>
    The limit prevents extreme placement group splitting, and is derived from
    the <option>mon_osd_max_split_count</option> value.
   </para>
  </warning>

  <para>
   To determine the right new number of placement groups for a resized cluster
   is a complex task. One approach is to continuously grow the number of
   placement groups up to the state when the cluster performance is optimal. To
   determine the new incremented number of placement groups, you need to get
   the value of the <option>mon_osd_max_split_count</option> parameter, and add
   it to the current number of placement groups. To give you a basic idea, take
   a look at the following script:
  </para>

<screen>max_inc=`ceph daemon mon.a config get mon_osd_max_split_count 2&gt;&amp;1 \
  | tr -d '\n ' | sed 's/.*"\([[:digit:]]\+\)".*/\1/'`
 pg_num=`ceph osd pool get rbd pg_num | cut -f2 -d: | tr -d ' '`
 echo "current pg_num value: $pg_num, max increment: $max_inc"
 next_pg_num="$(($pg_num+$max_inc))"
 echo "allowed increment of pg_num: $next_pg_num"</screen>

  <para>
   After finding out the next number of placement groups, increase it with
  </para>

<screen>ceph osd pool set <replaceable>pool_name</replaceable> pg_num <replaceable>next_pg_num</replaceable></screen>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.add_pool">
  <title>Adding a Pool</title>

  <para>
   After you first deploy a cluster, Ceph uses the default pools to store
   data. You can later create a new pool with
  </para>

<screen>ceph osd pool create</screen>

  <para>
   For more information on cluster pool creation, see
   <xref linkend="ceph.pools.operate.add_pool" role="internalbook"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.del_pool">
  <title>Deleting a Pool</title>

  <para>
   By deleting a pool, you permanently destroy all data stored in that pool.
   You can delete a previously created pool with
  </para>

<screen>ceph osd pool delete</screen>

  <para>
   For more information on cluster pool deletion, see
   <xref linkend="ceph.pools.operate.del_pool" role="internalbook"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph.troubleshooting">
 	<title>Troubleshooting</title>
 	<para>
 	This section describes several issues that you may face when you operate a Ceph cluster.
 	</para>
 	
 	<sect2 xml:id="storage.bp.cluster_mntc.rados_striping">
  <title>Sending Large Objects with <command>rados</command> Fails with Full OSD</title>

  <para>
   <command>rados</command> is a command line utility to manage RADOS object
   storage. For more information, see <command>man 8 rados</command>.
  </para>

  <para>
   If you send a large object to a Ceph cluster with the
   <command>rados</command> utility, such as
  </para>

<screen>rados -p mypool put myobject /file/to/send</screen>

  <para>
   it can fill up all the related OSD space and cause serious trouble to the
   cluster performance. RADOS has a 'striper' API that enables applications to
   stripe large objects over multiple OSDs. If you turn the striping feature on
   with the <option>--striper</option> option, you can prevent the OSD from
   filling up.
  </para>

<screen>rados --striper -p mypool put myobject /file/to/send</screen>
 </sect2>
 
 <sect2 xml:id="ceph.xfs.corruption">
 	<title>Corrupted XFS Filesystem</title>
 	<para>
 	In rare circumstances like kernel bug or broken/misconfigured hardware, the underlying file system (XFS) in which an OSD stores its data might be damaged and unmountable.
 	</para>
 	<para>
 	If you are sure there is no problem with your hardware and the system is configured properly, raise a bug against the XFS subsytem of the SUSE Linux Enterprise Server kernel and mark the particular OSD as down:
 	</para>
 	<screen>ceph osd down <replaceable>OSD identification</replaceable></screen>
 	<warning>
 	<title>Do Not Format or Otherwise Modify the Damaged Device</title>
 	<para>
 	Even though using <command>xfs_repair</command> to fix the problem in the filesystem may seem reasonable, do not use it as the command modifies the file system. The OSD may start but its functioning may be influenced. 
 	</para>
 	</warning>
 	<para>
 	Now zap the underlying disk and recreate the OSD by running:
 	</para>
 	<screen>ceph-disk prepare --zap $OSD_DISK_DEVICE $OSD_JOURNAL_DEVICE"</screen>
 	<para>for example:</para>
 	<screen>ceph-disk prepare --zap /dev/sdb /dev/sdd2</screen>
 </sect2>
 </sect1>
</chapter>
  <chapter xml:base="admin_bestpractices_perform.xml" version="5.0" xml:id="storage.bp.performance">
 <title>Performance Diagnosis</title>
 <para/>
 <sect1 xml:id="storage.bp.performance.slowosd">
  <title>Finding Slow OSDs</title>

  <para>
   When tuning the cluster performance, it is very important to identify slow
   storage/OSDs within the cluster. The reason is that if the data is written
   to the slow(est) disk, the complete write operation slows down as it always
   waits until it is finished on all the related disks.
  </para>

  <para>
   It is not trivial to locate the storage bottleneck. You need to examine each
   and every OSD to find out the ones slowing down the write process. To do a
   benchmark on a single OSD, run:
  </para>

<screen role="ceph_tell_osd_bench"><command>ceph tell</command> <replaceable>osd_id</replaceable> bench</screen>

  <para>
   For example:
  </para>

<screen><prompt>cephadm &gt; </prompt>ceph tell osd.0 bench
 { "bytes_written": 1073741824,
   "blocksize": 4194304,
   "bytes_per_sec": "19377779.000000"}</screen>

  <para>
   Then you need to run this command on each OSD and compare the
   <literal>bytes_per_sec</literal> value to get the slow(est) OSDs.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.performance.net_issues">
  <title>Is My Network Causing Issues?</title>

  <para>
   There are more reasons why the cluster performance may become weak. One of
   them can be network problems. In such case, you may notice the cluster
   reaching quorum, OSD and monitor nodes going offline, data transfers taking
   a long time, or a lot of reconnect attempts.
  </para>

  <para>
   To check whether cluster performance is degraded by network problems,
   inspect the Ceph log files under the <filename>/var/log/ceph</filename>
   directory.
  </para>

  <para>
   To fix network issues on the cluster, focus on the following points:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Basic network diagnostics. Try to ping between cluster nodes and pay
     attention to data loss and response times.
    </para>
   </listitem>
   <listitem>
    <para>
     Network performance benchmark. Use tools such as Netperf to measure the
     performance of your network.
    </para>
   </listitem>
   <listitem>
    <para>
     Check firewall settings on cluster nodes. Make sure they do not block
     ports/protocols required by Ceph operation. See
     <xref linkend="storage.bp.net.firewall" role="internalbook"/> for more information on firewall
     settings.
    </para>
   </listitem>
   <listitem>
    <para>
     Check the networking hardware, such as network cards, cables, or switches,
     for proper operation.
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>Separate Network</title>
   <para>
    To ensure fast and safe network communication between cluster nodes, set up
    a separate network used exclusively by the cluster OSD and monitor nodes.
   </para>
  </tip>
 </sect1>
</chapter>
  <chapter xml:base="admin_bestpractices_srvmaint.xml" version="5.0" xml:id="storage.bp.srv_maint">
 <title>Server Maintenance</title>
 <para/>
 <sect1 xml:id="storage.bp.srv_maint.add_server">
  <title>Adding a Server to a Cluster</title>

  <tip>
   <para>
    When adding an OSD to an existing cluster, be aware that the cluster will
    be rebalancing for some time afterward. To minimize the rebalancing
    periods, it is best to add all the OSDs you intend to add at the same time.
   </para>
  </tip>

  <para>
   If you are adding an OSD to a cluster, follow
   <xref linkend="storage.bp.inst.add_osd_cephdeploy" role="internalbook"/>.
  </para>

  <para>
   If you are adding a monitor to a cluster, follow
   <xref linkend="storage.bp.inst.add_rm_monitor" role="internalbook"/>.
  </para>

  <important>
   <para>
    After adding a monitor, make sure that
    <filename>/etc/ceph/ceph.conf</filename> files on each server point to the
    new monitor as well so that it works after the next reboot.
   </para>
  </important>

  <tip>
   <para>
    Adding an OSD and monitor on the same server is recommended only for small
    size clusters. Although the monitor can share disk with the operating
    system (preferably an SSD disk for performance reasons), it should
    <emphasis>never</emphasis> share disk with an OSD.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="storage.bp.srv_maint.rm_server">
  <title>Removing a Server from a Cluster</title>

  <para>
   When removing an OSD from an existing cluster, make sure there are enough
   OSDs left in the cluster so that the replication rules can be followed. Also
   be aware that the cluster will be rebalancing for some time after removing
   the OSD.
  </para>

  <para>
   If you are removing an OSD from a cluster, follow
   <xref linkend="storage.bp.disk.del" role="internalbook"/>.
  </para>

  <para>
   If you are removing a monitor from a cluster, follow
   <xref linkend="storage.bp.inst.add_rm_monitor.rmmon" role="internalbook"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.srv_maint.fds_inc">
  <title>Increasing File Descriptors</title>

  <para>
   For OSD daemons, the read/write operations are critical to keep the Ceph
   cluster balanced. They often need to have many files open for reading and
   writing at the same time. On the OS level, the maximum number of
   simultaneously open files is called 'maximum number of file descriptors'.
  </para>

  <para>
   To prevent OSDs from running out of file descriptors, you can override the
   OS default value and specify the number in
   <filename>/etc/ceph/ceph.conf</filename>, for example:
  </para>

<screen>max_open_files = 131072</screen>

  <para>
   After you change <option>max_open_files</option>, you need to restart the
   OSD service on the relevant Ceph node.
  </para>
 </sect1>
</chapter>
  <chapter xml:base="admin_bestpractices_net.xml" version="5.0" xml:id="storage.bp.net">
 <title>Networking</title>
 <para/>
 <sect1 xml:id="storage.bp.net.ntp">
  <title>Setting NTP to a Ceph Cluster</title>

  <para>
   In a cluster environment, it is necessary to keep all cluster nodes' time
   synchronized. NTP—Network Time Protocol—is a network service
   commonly used for this purpose. NTP is well integrated in SUSE products,
   including SUSE Enterprise Storage. There are two ways to configure NTP—either using
   YaST, or setting it up manually. Find both methods described—and
   more information on NTP in general—in
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html">SUSE Linux Enterprise Server
   Administration Guide</link>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.net.firewall">
  <title>Firewall Settings for Ceph</title>

  <para>
   We recommend protecting the network cluster communication with SUSE
   Firewall. You can edit its configuration by selecting
   <menuchoice><guimenu>YaST</guimenu><guimenu>Security and
   Users</guimenu><guimenu>Firewall</guimenu><guimenu>Allowed
   Services</guimenu></menuchoice>.
  </para>

  <para>
   For Calamari, enable the "HTTP Server", "Carbon" and "SaltStack" services
   (ports 80, 2003, 2004, 4505 and 4506).
  </para>

  <para>
   For Ceph monitor nodes, enable the "Ceph MON" service (port 6789).
  </para>

  <para>
   For Ceph OSD (or MDS) nodes, enable the "Ceph OSD/MDS" service (ports
   6800-7300).
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.net.private">
  <title>Adding a Private Network to a Running Cluster</title>

  <para>
   If you do not specify a cluster network during Ceph deployment, it assumes
   a single public network environment. While Ceph operates fine with a
   public network, its performance and security improves when you set a second
   private cluster network.
  </para>

  <para>
   A general recommendation for a Ceph cluster is to have two networks: a
   public (front-side) and cluster (back-side) one. To support two networks,
   each Ceph node needs to have at least two network cards.
  </para>

  <para>
   You need to apply the following changes to each Ceph node. It is
   comfortable for a small cluster, but can be very time demanding if you have
   a cluster consisting of hundreds or thousands of nodes.
  </para>

  <procedure>
   <step>
    <para>
     Stop Ceph related services on each cluster node.
    </para>
    <para>
     Replace <literal>10.0.0.0/24</literal> with the IP address and netmask of
     the cluster network. You can specify more comma-delimited subnets. If you
     need to specifically assign static IP addresses or override
     <option>cluster network</option> settings, you can do so with the optional
     <option>cluster addr</option>.
    </para>
   </step>
   <step>
    <para>
     Check that the private cluster network works as expected on the OS level.
    </para>
   </step>
   <step>
    <para>
     Start Ceph related services on each cluster node.
    </para>
<screen>sudo rcceph start</screen>
   </step>
  </procedure>
 </sect1>
</chapter>
 </part>
 <glossary xml:base="admin_glossary.xml" version="5.0" xml:id="gloss.storage.glossary">

 <title>Glossary</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <glossdiv xml:id="gloss.storage.general">
  <title>General</title>
  <glossentry xml:id="gloss.storage.adminode"><glossterm>Admin node</glossterm>
   <glossdef>
    <para>
     The node from which you run the <command>ceph-deploy</command> utility to
     deploy Ceph on OSD nodes.
    </para>
   </glossdef>
  </glossentry>
  <glossentry xml:id="gloss.storage.bucket"><glossterm>Bucket</glossterm>
   <glossdef>
    <para>
     A point which aggregates other nodes into a hierarchy of physical
     locations.
    </para>
   </glossdef>
  </glossentry>
  <glossentry xml:id="gloss.storage.crush"><glossterm>CRUSH, CRUSH Map</glossterm>
   <glossdef>
    <para>
     An algorithm that determines how to store and retrieve data by computing
     data storage locations. CRUSH requires a map of the cluster to
     pseudo-randomly store and retrieve data in OSDs with a uniform
     distribution of data across the cluster.
    </para>
   </glossdef>
  </glossentry>
  <glossentry xml:id="gloss.storage.mon"><glossterm>Monitor node, MON</glossterm>
   <glossdef>
    <para>
     A cluster node that maintains maps of cluster state, including the monitor
     map, or the OSD map.
    </para>
   </glossdef>
  </glossentry>
  <glossentry xml:id="gloss.storage.osd"><glossterm>OSD node</glossterm>
   <glossdef>
    <para>
     A cluster node that stores data, handles data replication, recovery,
     backfilling, rebalancing, and provides some monitoring information to
     Ceph monitors by checking other Ceph OSD daemons.
    </para>
   </glossdef>
  </glossentry>
  <glossentry xml:id="gloss.storage.node"><glossterm>Node</glossterm>
   <glossdef>
    <para>
     Any single machine or server in a Ceph cluster.
    </para>
   </glossdef>
  </glossentry>
  <glossentry><glossterm>Pool</glossterm>
   <glossdef>
    <para>
     Logical partitions for storing objects such as disk images.
    </para>
   </glossdef>
  </glossentry>
  <glossentry><glossterm>Rule Set</glossterm>
   <glossdef>
    <para>
     Rules to determine data placement for a pool.
    </para>
   </glossdef>
  </glossentry>
 </glossdiv>

 <glossdiv xml:id="gloss.storage.ceph">
  <title>Ceph Specific Terms</title>
  <glossentry><glossterm>Calamari</glossterm>
   <glossdef>
    <para>
     A management and monitoring system for Ceph storage cluster. It provides
     a Web user interface that makes Ceph cluster monitoring simple.
    </para>
   </glossdef>
  </glossentry>
  <glossentry xml:id="gloss.storage.ceph_storage_cluster"><glossterm>Ceph Storage Cluster</glossterm>
   <glossdef>
    <para>
     The core set of storage software which stores the user’s data. Such a
     set consists of Ceph monitors and OSDs.
    </para>
    <para>
     AKA <quote>Ceph Object Store</quote>.
    </para>
   </glossdef>
  </glossentry>
  <glossentry xml:id="gloss.storage.rgw"><glossterm><phrase>RADOS Gateway</phrase></glossterm>
   <glossdef>
    <para>
     The S3/Swift gateway component for Ceph Object Store.
    </para>
   </glossdef>
  </glossentry>
 </glossdiv>
</glossary>
 <appendix xml:base="admin_sls.xml" version="5.0" xml:id="app.storage.sls">
 <title>Salt State (SLS) File Example</title>
 <para>
  This example shows a cluster configuration split into several SLS files. You
  can customize them to build up a cluster with Salt. Note that you need to
  do local customization to the SLS files, such as supplying suitable disk
  device names, host names and IP addresses valid for your network environment.
  Lines beginning with '#' are comments.
 </para>
 <para>
  The structure is following:
 </para>
<screen>
├── ses
│   ├── ceph
│   │   ├── ceph.conf
│   │   └── init.sls
│   ├── common
│   │   ├── admin_key.sls
│   │   ├── mds_key.sls
│   │   ├── mon_key.sls
│   │   ├── osd_key.sls
│   │   └── rgw_key.sls
│   ├── mds
│   │   └── init.sls
│   ├── mon
│   │   └── init.sls
│   ├── osd
│   │   └── init.sls
│   └── rgw
│       └── init.sls
└── top.sls
 </screen>
 <example>
  <title><filename>top.sls</filename></title>
  <para>
   The configuration toplevel file <filename>top.sls</filename> includes other
   SLS files from the subdirectories of the <filename>ses</filename> directory,
   depending on which component is used.
  </para>
<screen>base:
  '*':
    - ses.ceph
  '*mon*':
    - ses.mon
  '*osd*':
    - ses.osd
  '*mds*':
    - ses.mds  
  '*rgw*':
    - ses.rgw</screen>
 </example>
 <example>
  <title><filename>ses/ceph/init.sls</filename></title>
<screen># We need to install ceph and its configuration library
packages:
  pkg.installed:
    - names:
      - ceph
      - python-ceph-cfg

# We need a ceph configuration file before we start.
# Note:
# - The file name is dependent on the cluster name:
#    /etc/ceph/${CLUSTER_NAME}.conf
/etc/ceph/ceph.conf:
  file:
    - managed
    - source:
# Where to get the source file will have to be customized to your environment.
    - salt://ses/ceph/ceph.conf
    - user: root
    - group: root
    - mode: 644
    - makedirs: True
    - require:
      - pkg: packages</screen>
 </example>
 <example>
  <title><filename>ses/ceph/ceph.conf</filename></title>
<screen>[global]
fsid = eaac9695-4265-4ca8-ac2a-f3a479c559b1
mon_initial_members = osd-mon-node0, mon-osd-mds-node1, mon-osd-node2
mon_host = 192.168.100.168,192.168.100.223,192.168.100.130
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
filestore_xattr_use_omap = true</screen>
 </example>
 <para>
  Create the admin key and the keys for specific service types with
 </para>
<screen>salt '*' ceph.keyring_create type=&lt;keyring_type&gt;</screen>
 <para>
  before saving them. Customize the 'secret' value for your site using the
  values from the previous create command. All keys must be saved before the
  monitors are created as this has a side effect of creating keys not managed
  by Salt.
 </para>
 <example>
  <title><filename>ses/common/admin_key.sls</filename></title>
<screen>keyring_admin_save:
  module.run:
   - name: ceph.keyring_save
   - kwargs: {
     'keyring_type' : 'admin',
     'secret' : 'AQBR8KhWgKw6FhAAoXvTT6MdBE+bV+zPKzIo6w=='
     }
   - require:
   - sls: ses.ceph</screen>
 </example>
 <example>
  <title><filename>ses/common/mds_key.sls</filename></title>
<screen>keyring_mds_save:
  module.run:
    - name: ceph.keyring_save
    - kwargs: {
        'keyring_type' : 'mds',
        'secret' : 'AQBR8KhWgKw6FhAAoXvTT6MdBE+bV+zPKzIo6w=='
        }
    - require:
    - sls: ses.ceph</screen>
 </example>
 <example>
  <title><filename>ses/common/mon_key.sls</filename></title>
<screen>keyring_mon_save:
  module.run:
    - name: ceph.keyring_save
    - kwargs: {
        'keyring_type' : 'mon',
        'secret' : 'AQBR8KhWgKw6FhAAoXvTT6MdBE+bV+zPKzIo6w=='
        }
    - require:
    - sls: ses.ceph</screen>
 </example>
 <example>
  <title><filename>ses/common/osd_key.sls</filename></title>
<screen>keyring_osd_save:
  module.run:
    - name: ceph.keyring_save
    - kwargs: {
        'keyring_type' : 'osd',
        'secret' : 'AQBR8KhWgKw6FhAAoXvTT6MdBE+bV+zPKzIo6w=='
        }
    - require:
    - sls: ses.ceph</screen>
 </example>
 <example>
  <title><filename>ses/common/rgw_key.sls</filename></title>
<screen>keyring_rgw_save:
  module.run:
    - name: ceph.keyring_save
    - kwargs: {
        'keyring_type' : 'rgw',
        'secret' : 'AQBR8KhWgKw6FhAAoXvTT6MdBE+bV+zPKzIo6w=='
        }
    - require:
    - sls: ses.ceph</screen>
 </example>
 <example>
  <title><filename>ses/mds/init.sls</filename></title>
<screen>include:
  - ses.ceph
  - ses.common.mds_key

keyring_mds_auth_add:
  module.run:
    - name: ceph.keyring_mds_auth_add
    - require:
      - module: keyring_mds_save
      - ceph: cluster_status

mds_create:
  module.run:
    - name: ceph.mds_create
    - kwargs: {
        name: mds.{{ grains['machine_id'] }},
        port: 1000,
        addr:{{ grains['fqdn_ip4'] }}
        }
    - require:
    - module: keyring_mds_auth_add</screen>
 </example>
 <example>
  <title><filename>ses/mon/init.sls</filename></title>
<screen>include:
  - ses.ceph
  - ses.common.admin_key
  - ses.common.mon_key

mon_create:
  module.run:
    - name: ceph.mon_create
    - require:
      - module: keyring_admin_save
      - module: keyring_mon_save

cluster_status:
  ceph.quorum:
    - require:
    - module: mon_create</screen>
 </example>
 <example>
  <title><filename>ses/osd/init.sls</filename></title>
<screen>include:
  - ses.ceph
  - ses.common.osd_key

keyring_osd_auth_add:
  module.run:
    - name: ceph.keyring_osd_auth_add
    - require:
      - module: keyring_osd_save
      - ceph: cluster_status

# Prepare disks for OSD use

prepare_vdb:
  module.run:
    - name: ceph.osd_prepare
    - kwargs: {
        osd_dev: /dev/vdb
      }
    - require:
      - module: keyring_osd_auth_add

# Activate OSD's on prepared disks

activate_vdb:
  module.run:
    - name: ceph.osd_activate
				- kwargs: {
        osd_dev: /dev/vdb
								}</screen>
 </example>
 <example>
  <title><filename>ses/rgw/init.sls</filename></title>
<screen>include:
  - ses.ceph
  - ses.common.rgw_key</screen>
 </example>
</appendix>
 <appendix xml:base="admin_manual-install.xml" version="5.0" xml:id="app.storage.manual_inst">
 <title>Example Procedure of Manual Ceph Installation</title>
 <para>
  The following procedure shows the commands that you need to install Ceph
  storage cluster manually.
 </para>
 <procedure>
  <step>
   <para>
    Generate the key secrets for the Ceph services you plan to run. You can
    use the following command to generate it:
   </para>
<screen>python -c "import os ; import struct ; import time; import base64 ; \
 key = os.urandom(16) ; header = struct.pack('&lt;hiih',1,int(time.time()),0,len(key)) ; \
 print base64.b64encode(header + key)"</screen>
  </step>
  <step>
   <para>
    Add the keys to the related keyrings. First for
    <systemitem class="username">client.admin</systemitem>, then for monitors,
    and then other related services, such as OSD, <phrase>RADOS Gateway</phrase>, or MDS:
   </para>
<screen>ceph-authtool -n client.admin \
 --create-keyring /etc/ceph/ceph.client.admin.keyring \
 --cap mds 'allow *' --cap mon 'allow *' --cap osd 'allow *'
ceph-authtool -n mon. \
 --create-keyring /var/lib/ceph/bootstrap-mon/ceph-osceph-03.keyring \
 --set-uid=0 --cap mon 'allow *'
ceph-authtool -n client.bootstrap-osd \
 --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring \
 --cap mon 'allow profile bootstrap-osd'
ceph-authtool -n client.bootstrap-rgw \
 --create-keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring \
 --cap mon 'allow profile bootstrap-rgw'
ceph-authtool -n client.bootstrap-mds \
 --create-keyring /var/lib/ceph/bootstrap-mds/ceph.keyring \
 --cap mon 'allow profile bootstrap-mds'</screen>
  </step>
  <step>
   <para>
    Create a monmap—a database of all monitors in a cluster:
   </para>
<screen>monmaptool --create --fsid eaac9695-4265-4ca8-ac2a-f3a479c559b1 \
 /tmp/tmpuuhxm3/monmap
monmaptool --add osceph-02 192.168.43.60 /tmp/tmpuuhxm3/monmap
monmaptool --add osceph-03 192.168.43.96 /tmp/tmpuuhxm3/monmap
monmaptool --add osceph-04 192.168.43.80 /tmp/tmpuuhxm3/monmap</screen>
  </step>
  <step>
   <para>
    Create a new keyring and import keys from the admin and monitors' keyrings
    there. Then use them to start the monitors:
   </para>
<screen>ceph-authtool --create-keyring /tmp/tmpuuhxm3/keyring \
 --import-keyring /var/lib/ceph/bootstrap-mon/ceph-osceph-03.keyring
ceph-authtool /tmp/tmpuuhxm3/keyring \
 --import-keyring /etc/ceph/ceph.client.admin.keyring
sudo -u ceph ceph-mon --mkfs -i osceph-03 \
 --monmap /tmp/tmpuuhxm3/monmap --keyring /tmp/tmpuuhxm3/keyring
systemctl restart ceph-mon@osceph-03</screen>
  </step>
  <step>
   <para>
    Check the monitors state in <systemitem class="daemon">systemd</systemitem>:
   </para>
<screen>systemctl show --property ActiveState ceph-mon@osceph-03</screen>
  </step>
  <step>
   <para>
    Check if Ceph is running and reports the monitor status:
   </para>
<screen>ceph --cluster=ceph \
 --admin-daemon /var/run/ceph/ceph-mon.osceph-03.asok mon_status</screen>
  </step>
  <step>
   <para>
    Check the specific services' status using the existing keys:
   </para>
<screen>ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin -f json-pretty status
[...]
ceph --connect-timeout 5 \
 --keyring /var/lib/ceph/bootstrap-mon/ceph-osceph-03.keyring \
 --name mon. -f json-pretty status</screen>
  </step>
  <step>
   <para>
    Import keyring from existing Ceph services and check the status:
   </para>
<screen>ceph auth import -i /var/lib/ceph/bootstrap-osd/ceph.keyring
ceph auth import -i /var/lib/ceph/bootstrap-rgw/ceph.keyring
ceph auth import -i /var/lib/ceph/bootstrap-mds/ceph.keyring
ceph --cluster=ceph \
 --admin-daemon /var/run/ceph/ceph-mon.osceph-03.asok mon_status
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin -f json-pretty status</screen>
  </step>
  <step>
   <para>
    Prepare disks/partitions for OSDs, using the XFS file system:
   </para>
<screen>ceph-disk -v prepare --fs-type xfs --data-dev --cluster ceph \
 --cluster-uuid eaac9695-4265-4ca8-ac2a-f3a479c559b1 /dev/vdb
ceph-disk -v prepare --fs-type xfs --data-dev --cluster ceph \
 --cluster-uuid eaac9695-4265-4ca8-ac2a-f3a479c559b1 /dev/vdc
[...]</screen>
  </step>
  <step>
   <para>
    Activate the partitions:
   </para>
<screen>ceph-disk -v activate --mark-init systemd --mount /dev/vdb1
ceph-disk -v activate --mark-init systemd --mount /dev/vdc1</screen>
  </step>
  <step>
   <para>
    For SUSE Enterprise Storage version 2.1 and earlier, create the default pools:
   </para>
<screen>ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .users.swift 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .intent-log 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .rgw.gc 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .users.uid 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .rgw.control 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .users 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .usage 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .log 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .rgw 16 16</screen>
  </step>
  <step>
   <para>
    Create the <phrase>RADOS Gateway</phrase> instance key from the bootstrap key:
   </para>
<screen>ceph --connect-timeout 5 --cluster ceph --name client.bootstrap-rgw \
 --keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring auth get-or-create \
 client.rgw.0dc1e13033d2467eace46270f0048b39 osd 'allow rwx' mon 'allow rw' \
 -o /var/lib/ceph/radosgw/ceph-rgw.<replaceable>rgw_name</replaceable>/keyring
  </screen>
  </step>
  <step>
   <para>
    Enable and start <phrase>RADOS Gateway</phrase>:
   </para>
<screen>systemctl enable ceph-radosgw@rgw.<replaceable>rgw_name</replaceable>
systemctl start ceph-radosgw@rgw.<replaceable>rgw_name</replaceable></screen>
  </step>
  <step>
   <para>
    Optionally, create the MDS instance key from the bootstrap key, then enable
    and start it:
   </para>
<screen>ceph --connect-timeout 5 --cluster ceph --name client.bootstrap-mds \
 --keyring /var/lib/ceph/bootstrap-mds/ceph.keyring auth get-or-create \
 mds.mds.<replaceable>rgw_name</replaceable> osd 'allow rwx' mds allow mon \
 'allow profile mds' \
 -o /var/lib/ceph/mds/ceph-mds.<replaceable>rgw_name</replaceable>/keyring
systemctl enable ceph-mds@mds.<replaceable>rgw_name</replaceable>
systemctl start ceph-mds@mds.<replaceable>rgw_name</replaceable></screen>
  </step>
 </procedure>
</appendix>
 <appendix xml:base="admin_docupdates.xml" version="5.0" xml:id="ap.adm.docupdate">
 <title>Documentation Updates</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation/>
   <dm:languages/>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter lists content changes for this document since the initial
  release of SUSE Enterprise Storage 1.
 </para>
 <para>
  The document was updated on the following dates:
 </para>

 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <xref linkend="sec.adm.docupdates.4maint1" xrefstyle="SectTitleOnPage" role="internalbook"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="sec.adm.docupdates.4" xrefstyle="SectTitleOnPage" role="internalbook"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="sec.adm.docupdates.3" xrefstyle="SectTitleOnPage" role="internalbook"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="sec.adm.docupdates.2" xrefstyle="SectTitleOnPage" role="internalbook"/>
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="sec.adm.docupdates.1" xrefstyle="SectTitleOnPage" role="internalbook"/>
   </para>
  </listitem>
 </itemizedlist>

 <sect1 xml:id="sec.adm.docupdates.4maint1">
  <title>February, 2017 (Release of SUSE Enterprise Storage 4 Maintenance Update 1)</title>

  <variablelist>
   <varlistentry>
    <term>General Updates</term>
    <listitem>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>

  <variablelist>
   <varlistentry>
    <term>Bugfixes</term>
    <listitem>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Added the admin node to the upgrade workflow in <xref linkend="ceph.upgrade.general" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=1012155"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Rewrote <xref linkend="ceph.operating.services" role="internalbook"/> to avoid globbing
        services
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=1009500"/>).
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.adm.docupdates.4">
  <title>December, 2016 (Release of SUSE Enterprise Storage 4)</title>

  <variablelist>
   <varlistentry>
    <term>General Updates</term>
    <listitem>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Restructured the whole document introducing DocBook 'parts' to group
        related chapters.
       </para>
      </listitem>
      <listitem>
       <para>
        Introduced <xref linkend="ceph.oa" role="internalbook"/> (Fate #321085).
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>

    <term><xref linkend="cha.ceph.upgrade" role="internalbook"/>
    </term>
    <listitem>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Replaced the old upgrade procedure with
        <xref linkend="ceph.upgrade.to4" role="internalbook"/>.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>

  <variablelist>
   <varlistentry>
    <term>Bugfixes</term>
    <listitem>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Extended <xref linkend="ceph.install.crowbar" role="internalbook"/> to include more
        detailed information based on the SUSE OpenStack Cloud documentation
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=1003541"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Increased the memory requirement for an OSD in
        <xref linkend="sysreq.osd" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=982496"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Improved the each node preparation section
        <xref linkend="ceph.install.ceph-deploy.eachnode" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=1005752"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Improved <xref linkend="ceph.install.saltstack" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=993499"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Improved <xref linkend="ceph.install.calamari" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=1003529"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Improved <xref linkend="storage.bp.recover.clockskew" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=999856"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Improved <xref linkend="ceph.cephfs.mdf.add" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=992769"/>).
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.adm.docupdates.3">
  <title>June, 2016 (Release of SUSE Enterprise Storage 3)</title>

  <variablelist>
   <varlistentry>
    <term>General Updates</term>
    <listitem>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Added <xref linkend="app.storage.manual_inst" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="cha.storage.cephx" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="app.storage.sls" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="ceph.rgw.fed" role="internalbook"/> (Fate #320602).
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="ceph.install.saltstack" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="cha.ceph.cephfs" role="internalbook"/> (Fate #318586).
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>

    <term><xref linkend="cha.ceph.install" role="internalbook"/>
    </term>
    <listitem>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Improved network recommendation tip in
        <xref linkend="ceph.install.ceph-deploy.network" role="internalbook"/>.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>

    <term><xref linkend="cha.ceph.iscsi" role="internalbook"/>
    </term>
    <listitem>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Added <xref linkend="ceph.iscsi.rbd.optional" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="ceph.iscsi.connect.linux.multipath" role="internalbook"/>.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>

  <variablelist>
   <varlistentry>
    <term>Bugfixes</term>
    <listitem>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Improved the procedure to set up hot-storage and cold-storage in
        <xref linkend="ses.tiered.storage" role="internalbook"/> and added
        <xref linkend="cache.tier.configure" role="internalbook"/>.
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=982607"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added a command to install Ceph on the MDS server in
        <xref linkend="ceph.cephfs.mdf.add" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=993820"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added a tip referring to more information about using existing
        partitions for OSDs in
        <xref linkend="ceph.install.ceph-deploy.cephdeploy" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=992019"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Reordered snapshots-related sections and created
        <xref linkend="cha.ceph.snapshots" role="internalbook"/> with
        <xref linkend="cha.ceph.snapshots.rbd" role="internalbook"/> and
        <xref linkend="cha.ceph.snapshots.pool" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=982707"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Mixing installation methods is not supported in
        <xref linkend="ceph.install.crowbar" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=988038"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Format 1 is no longer the default (in favor of the format 2) when
        creating RBD volumes in <xref linkend="ceph.iscsi.install" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=987992"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added note about increasing the ruleset number in
        <xref linkend="datamgm.rules" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=997051"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Specified which clients are able to migrate to optimal tunables
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=982995"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Split <xref linkend="ceph.iscsi.rbd.optional" role="internalbook"/> into
        <xref linkend="ceph.iscsi.rbd.advanced" role="internalbook"/> and added configuration
        options description
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=986037"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="op.mixed_ssd_hdd" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=982375"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Updated minimal recommendations in <xref linkend="sysreq.osd" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=981642"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Fixed support information on snapshot cloning in
        <xref linkend="ceph.snapshoti.layering" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=982713"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Improved 'bucket' explanation in <xref linkend="datamgm.buckets" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=985047"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Clarified non-mixing workload phrase in <xref linkend="sysreq.mon" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=982497"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Updated RAM requirement for OSDs in <xref linkend="sysreq.osd" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=982496"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Fixed <literal>hit_set_count</literal> default value in
        <xref linkend="ceph.pools.values" role="internalbook"/> and added note with external link
        in <xref linkend="ses.tiered.storage" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=982284"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Fixed and improved <command>ceph-deploy</command> command line in
        <xref linkend="ceph.install.ceph-deploy.cephdeploy" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=981617"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Updated several places to match the current Ceph release in
        <xref linkend="op.crush.addosd" role="internalbook"/>, <xref linkend="datamgm.rules" role="internalbook"/>, and
        <xref linkend="datamgm.buckets" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=982563"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        In <xref linkend="ceph.pools.values" role="internalbook"/>, added (explanation) of the
        following poll parameters: <literal>hashpspool</literal>,
        <literal>expected_num_objects</literal>,
        <literal>cache_target_dirty_high_ratio</literal>,
        <literal>hit_set_grade_decay_rate</literal>,
        <literal>hit_set_grade_search_last_n</literal>,
        <literal>fast_read</literal>, <literal>scrub_min_interval</literal>,
        <literal>scrub_max_interval</literal>,
        <literal>deep_scrub_interval</literal>, <literal>nodelete</literal>,
        <literal>nopgchange</literal>, <literal>nosizechange</literal>,
        <literal>noscrub</literal>, <literal>nodeep-scrub</literal>.
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=982512"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="bp.osd_on_exisitng_partitions" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=970104"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added software pattern selection screens in
        <xref linkend="ceph.install.ceph-deploy.eachnode" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=981951"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Removed RAID recommendations for OSD disks placement in
        <xref linkend="sysreq.osd" role="internalbook"/> and <xref linkend="datamgm.devices" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=981611"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Updated the default set of CRUSH map's buckets in
        <xref linkend="datamgm.buckets" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=981756"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Removed 'data' and 'metadata' pools, no longer the default
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=981758"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Fixed trademarked 3rd party products names and replaced with entities
        in <xref linkend="cha.ceph.iscsi" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=983018"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Updated <phrase>RADOS Gateway</phrase> service name to
        <systemitem>ceph-radosgw@radosgw.<replaceable>gateway_name</replaceable></systemitem>
        across affected sections
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=980594"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="sec.ceph.tiered.caution" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=968290"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Recommended to use <command>sudo</command> with the
        <command>ceph</command> command in
        <xref linkend="ceph.install.ceph-deploy.cephdeploy" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=978075"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Changed the default <literal>min_size</literal> value in
        <xref linkend="datamgm.rules" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=977556"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Fixed the <literal>master:dns_name_of_salt_master</literal> option in
        <xref linkend="ceph.install.saltstack" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=977187"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Extended and improved the OSD removal procedure in
        <xref linkend="storage.bp.disk.del" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=974624"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Prefixed <phrase>RADOS Gateway</phrase> hosts with <literal>rgw.</literal> in
        <xref linkend="ceph.rgw.cephdeploy.install" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=974472"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Changed the subcommand position in the <command>ceph-deploy calamari
        connect</command> command in <xref linkend="ceph.install.calamari" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=969836"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added information about perpetually stuck PGs in
        <xref linkend="storage.bp.recover.stuckinactive" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=968067"/>).
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.adm.docupdates.2">
  <title>January, 2016 (Release of SUSE Enterprise Storage 2.1)</title>

  <variablelist>
   <varlistentry>
    <term>General Updates</term>
    <listitem>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Removed Btrfs as it is not supported as of SUSE Enterprise Storage 2.
       </para>
      </listitem>
      <listitem>
       <para>
        Moved <xref linkend="ceph.tier.erasure" role="internalbook"/> from
        <xref linkend="cha.ceph.tiered" role="internalbook"/> to <xref linkend="cha.ceph.erasure" role="internalbook"/>
        so that the information provided follows the right order.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="cha.ceph.iscsi" role="internalbook"/>.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>

    <term><xref linkend="cha.ceph.install" role="internalbook"/>
    </term>
    <listitem>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Added <xref linkend="ceph.install.crowbar" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added a short description of the <option>--master</option> option in
        the <command>ceph-deploy calamari connect</command> command in
        <xref linkend="ceph.install.calamari" role="internalbook"/>.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>

    <term><xref linkend="cha.ceph.operating" role="internalbook"/>
    </term>
    <listitem>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Removed the <emphasis>Checking MDS Status</emphasis> section in
        <xref linkend="ceph.monitor" role="internalbook"/> as MDS is not covered yet.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>

    <term><xref linkend="cha.ceph.kvm" role="internalbook"/>
    </term>
    <listitem>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Added <xref linkend="ceph.kvm.install" role="internalbook"/>.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>

  <variablelist>
   <varlistentry>
    <term>Bugfixes</term>
    <listitem>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Added <command>systemctl stop cthulhu.service</command> when clearing a
        stale cluster
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=967849"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Fixed a typo in <xref linkend="ceph.rgw.manual" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=967937"/>)
       </para>
      </listitem>
      <listitem>
       <para>
        Fixed a typo in the <command>ceph-deploy rgw</command> command syntax
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=962976"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Restructured the whole <xref linkend="cha.ceph.gw" role="internalbook"/>, added
        <xref linkend="ceph.rgw.operating" role="internalbook"/>,
        <xref linkend="ceph.rgw.access" role="internalbook"/>,
        <xref linkend="storage.bp.account.s3quota" role="internalbook"/>, and
        <xref linkend="storage.bp.account.user_pwd" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=946873"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added links to the extension installation in
        <xref linkend="ceph.install.ceph-deploy.eachnode" role="internalbook"/> and
        <xref linkend="ceph.install.crowbar.admin_server" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=962085"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Renamed 'Monitoring a cluster' to 'Determining cluster state' in
        <xref linkend="ceph.monitor" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=958302"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.recover.toomanypgs" role="internalbook"/>.
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=948375"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Advised the customers to manually prevent Apache from listening on the
        default port 80 if they prefer a different port number
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=942703"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Removed FastCGI occurrences and file references in
        <xref linkend="cha.ceph.gw" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=946877"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added <command>ceph-deploy</command> way of installing/migrating <phrase>RADOS Gateway</phrase>
        instances
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=946771"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Moved the step about checking the firewall status to
        <xref linkend="ceph.install.ceph-deploy.eachnode" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=946775"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Inserted information about the need for correct network setup for each
        node in the procedure in
        <xref linkend="ceph.install.ceph-deploy.eachnode" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=946778"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Extended the information about 'zapping' the previously used disk while
        entirely erasing its content in
        <xref linkend="ceph.install.ceph-deploy.cephdeploy" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=946765"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Moved the tip on the non-default cluster name to the end of
        <xref linkend="ceph.install.ceph-deploy.cephdeploy" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=946773"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added tip on SSH alias in
        <xref linkend="ceph.install.ceph-deploy.eachnode" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=946776"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Removed the last complex <command>zypper rm</command> command from
        Ceph cleaning stage in
        <xref linkend="ceph.install.ceph-deploy.purge" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=946762"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Suggest to install the <systemitem>romana</systemitem> package instead
        of <systemitem>calamari-clients</systemitem> in
        <xref linkend="ceph.install.calamari" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=944473"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Swapped the order of chapters <xref linkend="cha.ceph.upgrade" role="internalbook"/> and
        <xref linkend="cha.ceph.install" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=946772"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Fixed Apache support information
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=946769"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Either erased or made it optional to use <command>ceph-deploy osd
        activate</command> in
        <xref linkend="storage.bp.inst.cephdeploy_usage" role="internalbook"/>,
        <xref linkend="ceph.install.ceph-deploy.cephdeploy" role="internalbook"/>, and
        <xref linkend="ses.tiered.storage" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=946768"/>).
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.adm.docupdates.1">
  <title>October, 2015 (Release of SUSE Enterprise Storage 2)</title>

  <variablelist>
   <varlistentry>
    <term>General</term>
    <listitem>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Added <xref linkend="cha.ceph.upgrade" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="cha.ceph.sysreq" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="cha.ceph.libvirt" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="cha.ceph.kvm" role="internalbook"/>.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>

  <variablelist>
   <varlistentry>
    <term><xref linkend="cha.ceph.install" role="internalbook"/>
    </term>
    <listitem>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Extended the tip on cleaning the previous Calamari installation in
        <xref linkend="ceph.install.calamari" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Fixed the blocking firewall in the installation procedure in
        <xref linkend="ceph.install.ceph-deploy.cephdeploy" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=936064"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Shifted the cluster health check in the installation procedure in
        <xref linkend="ceph.install.ceph-deploy.cephdeploy" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=936067"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added a tip on disabling requiretty in
        <xref linkend="ceph.install.ceph-deploy.eachnode" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=936017"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Fixed the pool names in
        <xref linkend="ceph.install.ceph-deploy.purge" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=936023"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added <systemitem>ceph</systemitem> to be installed along with
        <systemitem>ceph-deploy</systemitem> in
        <xref linkend="ceph.install.ceph-deploy.cephdeploy" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=936020"/>).
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><xref linkend="cha.ceph.gw" role="internalbook"/>
    </term>
    <listitem>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Fixed the <command>systemctl radosgw</command> command in
        <xref linkend="ses.rgw.config" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=940483"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Replaced Apache with the embedded Civetweb, mainly in
        <xref linkend="ceph.rgw.manual" role="internalbook"/>.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><xref linkend="cha.storage.bestpractice" role="internalbook"/>
    </term>
    <listitem>
     <para/>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Added <xref linkend="storage.bp.cluster_mntc" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.hwreq.replicas" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.cluster_mntc.calamari_addpool" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.inst.rgw_client" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.monitoring.calamari_usage_graphs" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added a tip on preventing for full OSDs in
        <xref linkend="storage.bp.monitoring.fullosd" role="internalbook"/>
        (<link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=930756"/>).
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.report_bug" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.cluster_mntc.mng_keyrings" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.cluster_mntc.create_client_keys" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.inst.cephdeploy_usage" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.net.ntp" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.tuneups.pg_num" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.hwreq.replicas" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.cluster_mntc.unbalanced" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.cluster_mntc.sw_upg" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="ses.bp.mindisk" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.tuneups.ssd_tradeoffs" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.cluster_mntc.add_pgnum" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="ses.bp.share_ssd_journal" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.inst.rgw" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.monitoring.fullosd" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.performance.net_issues" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.account.s3rm" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added<xref linkend="storage.bp.account.s3add" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.tuneups.mix_ssd" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="ses.bp.ram" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.account.swiftrm" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.account.swiftadd" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.integration.mount_rbd" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.net.firewall" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.cluster_mntc.del_pool" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.cluster_mntc.add_pool" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.srv_maint.fds_inc" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.net.private" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.integration.xen" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.srv_maint.rm_server" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.srv_maint.add_server" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.inst.add_rm_monitor" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="ses.bp.numofdisks" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.inst.add_osd_cephdeploy" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.inst.add_osd_cephdisk" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.performance.slowosd" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="ses.bp.diskshare" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.recover.clockskew" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.monitoring.journalfails" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.monitoring.diskfails" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.recover.osddown" role="internalbook"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        Added <xref linkend="storage.bp.recover.stalecalamari" role="internalbook"/>.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
</appendix>
</book>
